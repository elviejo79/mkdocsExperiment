{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Alinex Development Guide The book works as a ground documentation containing information and help in the area of web development. This includes everything necessary from the environment, server and client programming including a lot of helpful tools and technologies. Also if not really web components but two systems interchanging information through a web interface which is often used today this document will help. It is a big area with a lot of different competing fast and always progressing technologies. That makes it difficult to keep up to date with everything. Therefore this book can't be complete or up to date at any time. But it will evolve and you may look again some time. This is a book explaining all the major parts and development background around the Alinex named coding projects. But it is not completely specific to Alinex and more a book to learn and use the different IT technologies with best practice from the Alinex modules. The epics included are the ones I decided to look at and if something is missing maybe I had not seen this or decided to not try it out till now. Some descriptions are made while learning so this is also a reference for myself. So it is also a knowledge base and collection of my personal progress in learning the different technologies and often contains short overviews and only the essential facts of the original documentation. The decisions from this book are not real hard facts but a definition for me to follow as possible. It may change as the world around changes. And you don't have to follow this standards but it helps for all which are working deeply with the Alinex modules and participating to it to understand the parts behind. Some of the Alinex modules use an older standard (like the CoffeeScript based ones) but will eventually be upgraded to newer versions. Keep in mind that the book explains a lot of technologies around web development but it is neither meant as a complete guide to teach the technologies more like a best practice and how to bring it all together. Structure of Book The top level chapters are: Home With this introduction about the book, the Alinex Namespace and myself. Including a short blog about what is going on and on what I work. Environment All the tools around, which you may need to develop and work on the code. It describes programs and technologies often used to do the development work. And it contains all which is needed within the whole development cycle including CI/CD. Technologies Programming languages, tools and technologies which are used in the client server area. Only the ones with a big appearance here are moved out to own chapters below. Solutions This chapter will show ready to use solutions for programming or as application. This may contain tools from the Alinex namespace or others. JavaScript Programming in JavaScript and languages which translates into JavaScript like CoffeeScript, TypeScript and Flow. Usage may be on client but also on the server based on Node.JS. Rust As a system programming language which features stability Rust may be used on the server side in the client server programming. Accessibility The book is made responsive meaning it's rendering will vary and always be optimizes for the device and window screen you use. You may access it with the smartphone as good as within your PC Browser. Specially on your smartphone you can not only access it in your browser but also make it look more like an app by calling \"Add to Home Screen\" from the browser menu. This technology comes from the PWA but as the book is static no service-worker or active elements are included. Requirements As far as possible Alinex tries to not be restricted by a specific system or service but to come to an end it focuses on specific systems and services. At first this documentation is based on Debian Linux examples but it will work on any other Linux system or mac with some minor changes. You may also work with windows by replacing the few OS specific calls with appropriate ones. All the environment tools used here may also be replaced by other ones for their work. And at last the most important thing the end results are mostly not OS specific and will run on any system. Development basics My key concepts are \" Configuration over Implementation \" and \" Keep it Simple \". All the different modules in the Alinex namespace are loose coupled but optimal integrated. At first they solve my own problems but I will enlarge their functionality and often my problems are the problems of others, too. I believe in open source and all the tools which are mentioned here are free for open source projects. But you mostly can also use them for private use. Conclusion This is not only written to teach others but as said before mainly to keep the knowledge as a reference for myself. Also it is not statically written but a type of living book which evolve and grow in time hopefully as fast as technology goes on. So read it once, but come back again to see the newest changes. This book is also available as PDF or ePub, so if you need a static copy (not always fully up to date) download: alinex-book.pdf or alinex-book.epub . {!docs/abbreviations.txt!}","title":"Introduction"},{"location":"#alinex-development-guide","text":"The book works as a ground documentation containing information and help in the area of web development. This includes everything necessary from the environment, server and client programming including a lot of helpful tools and technologies. Also if not really web components but two systems interchanging information through a web interface which is often used today this document will help. It is a big area with a lot of different competing fast and always progressing technologies. That makes it difficult to keep up to date with everything. Therefore this book can't be complete or up to date at any time. But it will evolve and you may look again some time. This is a book explaining all the major parts and development background around the Alinex named coding projects. But it is not completely specific to Alinex and more a book to learn and use the different IT technologies with best practice from the Alinex modules. The epics included are the ones I decided to look at and if something is missing maybe I had not seen this or decided to not try it out till now. Some descriptions are made while learning so this is also a reference for myself. So it is also a knowledge base and collection of my personal progress in learning the different technologies and often contains short overviews and only the essential facts of the original documentation. The decisions from this book are not real hard facts but a definition for me to follow as possible. It may change as the world around changes. And you don't have to follow this standards but it helps for all which are working deeply with the Alinex modules and participating to it to understand the parts behind. Some of the Alinex modules use an older standard (like the CoffeeScript based ones) but will eventually be upgraded to newer versions. Keep in mind that the book explains a lot of technologies around web development but it is neither meant as a complete guide to teach the technologies more like a best practice and how to bring it all together.","title":"Alinex Development Guide"},{"location":"#structure-of-book","text":"The top level chapters are: Home With this introduction about the book, the Alinex Namespace and myself. Including a short blog about what is going on and on what I work. Environment All the tools around, which you may need to develop and work on the code. It describes programs and technologies often used to do the development work. And it contains all which is needed within the whole development cycle including CI/CD. Technologies Programming languages, tools and technologies which are used in the client server area. Only the ones with a big appearance here are moved out to own chapters below. Solutions This chapter will show ready to use solutions for programming or as application. This may contain tools from the Alinex namespace or others. JavaScript Programming in JavaScript and languages which translates into JavaScript like CoffeeScript, TypeScript and Flow. Usage may be on client but also on the server based on Node.JS. Rust As a system programming language which features stability Rust may be used on the server side in the client server programming.","title":"Structure of Book"},{"location":"#accessibility","text":"The book is made responsive meaning it's rendering will vary and always be optimizes for the device and window screen you use. You may access it with the smartphone as good as within your PC Browser. Specially on your smartphone you can not only access it in your browser but also make it look more like an app by calling \"Add to Home Screen\" from the browser menu. This technology comes from the PWA but as the book is static no service-worker or active elements are included.","title":"Accessibility"},{"location":"#requirements","text":"As far as possible Alinex tries to not be restricted by a specific system or service but to come to an end it focuses on specific systems and services. At first this documentation is based on Debian Linux examples but it will work on any other Linux system or mac with some minor changes. You may also work with windows by replacing the few OS specific calls with appropriate ones. All the environment tools used here may also be replaced by other ones for their work. And at last the most important thing the end results are mostly not OS specific and will run on any system.","title":"Requirements"},{"location":"#development-basics","text":"My key concepts are \" Configuration over Implementation \" and \" Keep it Simple \". All the different modules in the Alinex namespace are loose coupled but optimal integrated. At first they solve my own problems but I will enlarge their functionality and often my problems are the problems of others, too. I believe in open source and all the tools which are mentioned here are free for open source projects. But you mostly can also use them for private use.","title":"Development basics"},{"location":"#conclusion","text":"This is not only written to teach others but as said before mainly to keep the knowledge as a reference for myself. Also it is not statically written but a type of living book which evolve and grow in time hopefully as fast as technology goes on. So read it once, but come back again to see the newest changes. This book is also available as PDF or ePub, so if you need a static copy (not always fully up to date) download: alinex-book.pdf or alinex-book.epub . {!docs/abbreviations.txt!}","title":"Conclusion"},{"location":"LICENSE/","text":"License {: .right } This book is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License . You are free to Share \u2014 copy and redistribute the material in any medium or format Adapt \u2014 remix, transform, and build upon the material The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms Attribution - You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. NonCommercial - You may not use the material for commercial purposes. ShareAlike - If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions - You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. {!docs/abbreviations.txt!}","title":"License"},{"location":"LICENSE/#license","text":"{: .right } This book is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License .","title":"License"},{"location":"LICENSE/#you-are-free-to","text":"Share \u2014 copy and redistribute the material in any medium or format Adapt \u2014 remix, transform, and build upon the material The licensor cannot revoke these freedoms as long as you follow the license terms.","title":"You are free to"},{"location":"LICENSE/#under-the-following-terms","text":"Attribution - You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. NonCommercial - You may not use the material for commercial purposes. ShareAlike - If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions - You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.","title":"Under the following terms"},{"location":"LICENSE/#notices","text":"You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. {!docs/abbreviations.txt!}","title":"Notices"},{"location":"about/","text":"About this book Technology behind To write documentation I had to look for a tool with the following specifications: easily write documentation from within my development editor static website output clean layout and responsive design CI integration possible So before writing something myself I had a look around and at first used GitBook . That was fine and working but I got into trouble with CI because of code bugs in latest versions so I did look again for a better tool and switched to mkdocs . Both of them are very similar in form of writing in markdown and generating static pages. The book you see now is based on mkdocs , a python static site generator with some extension plugins and an optimized theme. Failures / correction Nobody is perfect and everywhere may be spelling errors, wrong assumptions, missing information,bugs or misleading information. If you find such things and want to help me optimize this please send your changes or inform me to become a collaborator. Source of this book About myself I am a developer and IT operator from south Germany with a degree in economic computer science with over 20 years of experience. My technical range goes from architecture, project management, development till operation and server administration. All in the range of web applications with front- and backend. Alexander Schilling {!docs/abbreviations.txt!}","title":"About"},{"location":"about/#about-this-book","text":"","title":"About this book"},{"location":"about/#technology-behind","text":"To write documentation I had to look for a tool with the following specifications: easily write documentation from within my development editor static website output clean layout and responsive design CI integration possible So before writing something myself I had a look around and at first used GitBook . That was fine and working but I got into trouble with CI because of code bugs in latest versions so I did look again for a better tool and switched to mkdocs . Both of them are very similar in form of writing in markdown and generating static pages. The book you see now is based on mkdocs , a python static site generator with some extension plugins and an optimized theme.","title":"Technology behind"},{"location":"about/#failures-correction","text":"Nobody is perfect and everywhere may be spelling errors, wrong assumptions, missing information,bugs or misleading information. If you find such things and want to help me optimize this please send your changes or inform me to become a collaborator. Source of this book","title":"Failures / correction"},{"location":"about/#about-myself","text":"I am a developer and IT operator from south Germany with a degree in economic computer science with over 20 years of experience. My technical range goes from architecture, project management, development till operation and server administration. All in the range of web applications with front- and backend. Alexander Schilling {!docs/abbreviations.txt!}","title":"About myself"},{"location":"alinex/","text":"What is Alinex The name is an artificial short name for ALEX and IN -ternet systems. This is used as a working title for all my developments since some time. It is more type of a learning project, which sometimes brings out something useful. It is not a really directed development process. So at the moment I have no strict timelines or roadmaps all are more some planing which may change as my personal thinking demands. The development area is completely in the field of client/server and web programming. So a lot of code works on the backend but I also made some frontends needed to use them comfortably. But although it has no business plan behind some productive systems come out of it like flexible and powerful tools for IT operation tasks. Not all of them are free and open but most of the modules are. Who am I {: .icon .right} My name is Alexander Schilling from the south of Germany. I studied business informatics with applied science and from that point grew with the Internet and web technologies. I am working as IT operator in a medium sized German company and my development there is purely for internal optimizations and making my work easier. In my free time I enjoy family life, activities but also find some time to keep ahead with IT technologies and development. Also I can say that I'm bound to Linux and Unix like systems since my studies. Find more info about myself and my projects on my personal site . Why I created it As I got more into the concept of NodeJS I found out that this is the ideal solution for the Alinex platform. I used this project to check new technologies out and hopefully bring it more on the way than my PHP server part till then. I worked further on creating a modular system with some of the great modules available. Hardest thing is to find the best module for each job out of the great repository. With this project I went to new land (for me) in the web technologies and tried lots of the current propagated technologies and patterns out. But that was only the start for the Alinex projects. Language Decision As always the world is neither black nor white but something between and everybody sees it his way. You may decide other than I and the JavaScript & Co languages are neither always the best but I love the rapid prototyping and package management of it. If the performance would be higher rated as the fast prototyping I will prefer Rust at the moment. I checked the language out and found it appealing as a core language. For fast prototyping I further use JavaScript and in Linux Administration also often Bash. But all this depends on personal knowledge, preference and the area there the final product should be used. The documentation here also depends on the time at which I started writing it. Open Source Why do I things like Alinex as open source? Open source software isn't about doing free work. It's about giving back to the community. I live in a world of open source, most code on my notebook, workstation and servers are free and open source and they work. Linux isn't written by only unpaid people who just have a hobby. Linux is written mostly by companies like Novel, Red Hat, IBM, and even Oracle. Other people are helping me write my code in exchange for me sharing my code and knowledge. It's a win-win situation. History After my study in the mid of 1998 I used a lot of Perl to do my webmaster job and we created different small to medium sized tools and also a full featured CMS with it to do this work. After that I worked some years as developer and architect in a university attached company making a web application based on Java technology. While I was working with Java I started making modules in PHP in my leisure time to check out what is possible. This was the start of a long running project which has not really a clear goal but to learn and make good basic modules. Over some years I got on with it but sometimes did some side peaks into other languages like Python or Ruby. As I got a peek at NodeJS (it was in version 0.10 at this time) I got a deeper look and started my first tests with it. I found it appealing and together with CoffeeScript (looks like python code ;-) I switched to more go with this language for the future. A lot of my ideas from the PHP modules were converted and I build just a good module base to also use it in different projects at work. In this time I has changed again and was more working in Administration and Development of IT operations helper systems. Since I do more operations tasks at my work job and the systems and servers growing on and on but the operations stuff kept short I saw this as the right position for myself. Not as normal operator but more as a DevOps person developing the operation tasks by automate and simplify the multiple complex tasks. The whole time I try to be up to date and checkout new technologies and tools as far as they look suited for me. I'm not only into scripting and also got deeper looks into Go and Rust but I'm clearly a Linux guy. {!docs/abbreviations.txt!}","title":"What is Alinex"},{"location":"alinex/#what-is-alinex","text":"The name is an artificial short name for ALEX and IN -ternet systems. This is used as a working title for all my developments since some time. It is more type of a learning project, which sometimes brings out something useful. It is not a really directed development process. So at the moment I have no strict timelines or roadmaps all are more some planing which may change as my personal thinking demands. The development area is completely in the field of client/server and web programming. So a lot of code works on the backend but I also made some frontends needed to use them comfortably. But although it has no business plan behind some productive systems come out of it like flexible and powerful tools for IT operation tasks. Not all of them are free and open but most of the modules are.","title":"What is Alinex"},{"location":"alinex/#who-am-i","text":"{: .icon .right} My name is Alexander Schilling from the south of Germany. I studied business informatics with applied science and from that point grew with the Internet and web technologies. I am working as IT operator in a medium sized German company and my development there is purely for internal optimizations and making my work easier. In my free time I enjoy family life, activities but also find some time to keep ahead with IT technologies and development. Also I can say that I'm bound to Linux and Unix like systems since my studies. Find more info about myself and my projects on my personal site .","title":"Who am I"},{"location":"alinex/#why-i-created-it","text":"As I got more into the concept of NodeJS I found out that this is the ideal solution for the Alinex platform. I used this project to check new technologies out and hopefully bring it more on the way than my PHP server part till then. I worked further on creating a modular system with some of the great modules available. Hardest thing is to find the best module for each job out of the great repository. With this project I went to new land (for me) in the web technologies and tried lots of the current propagated technologies and patterns out. But that was only the start for the Alinex projects.","title":"Why I created it"},{"location":"alinex/#language-decision","text":"As always the world is neither black nor white but something between and everybody sees it his way. You may decide other than I and the JavaScript & Co languages are neither always the best but I love the rapid prototyping and package management of it. If the performance would be higher rated as the fast prototyping I will prefer Rust at the moment. I checked the language out and found it appealing as a core language. For fast prototyping I further use JavaScript and in Linux Administration also often Bash. But all this depends on personal knowledge, preference and the area there the final product should be used. The documentation here also depends on the time at which I started writing it.","title":"Language Decision"},{"location":"alinex/#open-source","text":"Why do I things like Alinex as open source? Open source software isn't about doing free work. It's about giving back to the community. I live in a world of open source, most code on my notebook, workstation and servers are free and open source and they work. Linux isn't written by only unpaid people who just have a hobby. Linux is written mostly by companies like Novel, Red Hat, IBM, and even Oracle. Other people are helping me write my code in exchange for me sharing my code and knowledge. It's a win-win situation.","title":"Open Source"},{"location":"alinex/#history","text":"After my study in the mid of 1998 I used a lot of Perl to do my webmaster job and we created different small to medium sized tools and also a full featured CMS with it to do this work. After that I worked some years as developer and architect in a university attached company making a web application based on Java technology. While I was working with Java I started making modules in PHP in my leisure time to check out what is possible. This was the start of a long running project which has not really a clear goal but to learn and make good basic modules. Over some years I got on with it but sometimes did some side peaks into other languages like Python or Ruby. As I got a peek at NodeJS (it was in version 0.10 at this time) I got a deeper look and started my first tests with it. I found it appealing and together with CoffeeScript (looks like python code ;-) I switched to more go with this language for the future. A lot of my ideas from the PHP modules were converted and I build just a good module base to also use it in different projects at work. In this time I has changed again and was more working in Administration and Development of IT operations helper systems. Since I do more operations tasks at my work job and the systems and servers growing on and on but the operations stuff kept short I saw this as the right position for myself. Not as normal operator but more as a DevOps person developing the operation tasks by automate and simplify the multiple complex tasks. The whole time I try to be up to date and checkout new technologies and tools as far as they look suited for me. I'm not only into scripting and also got deeper looks into Go and Rust but I'm clearly a Linux guy. {!docs/abbreviations.txt!}","title":"History"},{"location":"policy/","text":"Privacy statement This privacy statement explains to you the type, scope and purpose of the processing of personal data (hereinafter referred to as \"data\") within our online offer and the associated websites, functions and contents (hereinafter jointly referred to as \"online offer\"). With regard to the terms used, such as \"processing\" or \"person responsible\", we refer to the definitions in Art. 4 of the General Data Protection Regulation (GDPR). !!! abstract First of all, no you do not need to expose any personal data to access pages under alinex.gitlab.io. **Personal data are neither collected** nor even asked for. Furthermore, these pages neither embed ads, nor trackers, nor social media plugins, and they are served without the use of cookies. And they are completely server over secure connections using HTTPS. Responsible person Alexander Schilling M\u00fchlstra\u00dfe 13 72805 Lichtenstein Germany info@alinex.de Categories of affected persons Visitors and users of the online offer (In the following, I also refer to these affected persons as \"users\"). Purpose of processing Provision of the online offer, its functions and contents. Security measures. Range measurement/marketing. Terms used \"Personal data\" means any information relating to an identified or identifiable natural person (hereinafter referred to as \"data subject\"); an identifiable natural person is one who can be identified, directly or indirectly, in particular by assignment to an identifier such as a name, an identification number, location data, an online identifier (e.g. cookie) or to one or more special features that express the physical, physiological, genetic, psychological, economic, cultural or social identity of that natural person. \"processing\" means any operation carried out with or without the aid of automated procedures or any such series of operations in connection with personal data. The term goes a long way and covers practically every handling of data. \"Responsible\" means the natural or legal person, authority, institution or other body that alone or together with others decides on the purposes and means of processing personal data. \"Processor\" means a natural or legal person, public authority, agency or other body that processes personal data on behalf of the controller.' Relevant legal bases In accordance with Art. 13 GDPR, we inform you of the legal basis of our data processing. If the legal basis is not mentioned in the data protection declaration, the following applies: The legal basis for obtaining consents is Art. 6 para. 1 lit. a and Art. 7 GDPR, the legal basis for processing for the performance of our services and performance of contractual measures as well as for answering inquiries is Art. 6 para. 1 lit. b GDPR, the legal basis for processing to fulfill our legal obligations is Art. 6 para. 1 lit. c GDPR, and the legal basis for processing to protect our legitimate interests is Art. 6 para. 1 lit. f GDPR. In the event that the vital interests of the data subject or another natural person require the processing of personal data, Art. 6 para. 1 lit. d GDPR serves as the legal basis. Updates of the policy statement We ask you to inform yourself regularly about the content of our privacy statement. We will adapt the privacy statement as soon as changes in the data processing carried out by us make this necessary. We will inform you as soon as the changes require your cooperation (e.g. consent) or other individual notification. Collaboration with processors and third parties If we disclose data to other persons and companies (contract processors or third parties) within the scope of our processing, transmit it to them or otherwise grant them access to the data, this shall only take place on the basis of a legal permission (e.g. if a transmission of the data to third parties, such as payment service providers, in accordance with Art. 6 Para. 1 lit. b GDPR for contract fulfillment is necessary), if you have consented, if a legal obligation provides for this or on the basis of our legitimate interests (e.g. when using agents, web hosts, etc.). If we commission third parties with the processing of data on the basis of a so-called \"order processing contract\", this is done on the basis of Art. 28 GDPR. Integration of services and contents of third parties Based on our legitimate interests (i.e. interest in the analysis, optimization and economic operation of our online offer within the meaning of Art. 6 (1) lit. GDPR), we make use of content or services offered by third-party providers in order to provide their content and services Services, such as Include videos or fonts (collectively referred to as \"content\"). This always presupposes that the third-party providers of this content perceive the IP address of the users, since they could not send the content to their browser without the IP address. The IP address is therefore required for the presentation of this content. We endeavor to use only content whose respective providers use the IP address solely for the delivery of the content. Third parties may also use so-called pixel tags (invisible graphics, also referred to as \"web beacons\") for statistical or marketing purposes. The \"pixel tags\" can be used to evaluate information such as visitor traffic on the pages of this website. The pseudonymous information may also be stored in cookies on the user's device and may include, but is not limited to, technical information about the browser and operating system, referring web pages, visit time, and other information regarding the use of our online offer. Hosting / Data storage The hosting services we use serve to provide the following services: Infrastructure and platform services, computing capacity, storage space and database services, security services and technical maintenance services that we use for the purpose of operating this online offering. We or our hosting provider process inventory data, contact data, content data, contract data, usage data, meta- and communication data of customers, interested parties and visitors of this online offer on the basis of our legitimate interests in an efficient and secure provision of this online offer according to Art. 6 Para. 1 lit. f GDPR in conjunction with. Art. 28 GDPR (conclusion of order processing contract). The alinex.gitlab.io site is hosted by GitLab 268 Bush Street #350. San Francisco,CA 94104. Therefore their policy will be in effect for this, too. Collection of access data and log files We, or our hosting provider, collect the following data on the basis of our legitimate interests within the meaning of Art. 6 para. 1 lit. f. GDPR data on each access to the server on which this service is located (so-called server log files). Access data includes the name of the accessed website, file, date and time of access, transferred data volume, notification of successful access, browser type and version, the user's operating system, referrer URL (the previously visited page), IP address and the requesting provider. Log file information is stored for a few days for troubleshooting and for security reasons (e.g. to detect misuse or fraud) and then deleted. Data whose further storage is required for evidentiary purposes are excluded from deletion until the respective incident has been finally clarified. {!docs/abbreviations.txt!}","title":"Policy"},{"location":"policy/#privacy-statement","text":"This privacy statement explains to you the type, scope and purpose of the processing of personal data (hereinafter referred to as \"data\") within our online offer and the associated websites, functions and contents (hereinafter jointly referred to as \"online offer\"). With regard to the terms used, such as \"processing\" or \"person responsible\", we refer to the definitions in Art. 4 of the General Data Protection Regulation (GDPR). !!! abstract First of all, no you do not need to expose any personal data to access pages under alinex.gitlab.io. **Personal data are neither collected** nor even asked for. Furthermore, these pages neither embed ads, nor trackers, nor social media plugins, and they are served without the use of cookies. And they are completely server over secure connections using HTTPS.","title":"Privacy statement"},{"location":"policy/#responsible-person","text":"Alexander Schilling M\u00fchlstra\u00dfe 13 72805 Lichtenstein Germany info@alinex.de","title":"Responsible person"},{"location":"policy/#categories-of-affected-persons","text":"Visitors and users of the online offer (In the following, I also refer to these affected persons as \"users\").","title":"Categories of affected persons"},{"location":"policy/#purpose-of-processing","text":"Provision of the online offer, its functions and contents. Security measures. Range measurement/marketing.","title":"Purpose of processing"},{"location":"policy/#terms-used","text":"\"Personal data\" means any information relating to an identified or identifiable natural person (hereinafter referred to as \"data subject\"); an identifiable natural person is one who can be identified, directly or indirectly, in particular by assignment to an identifier such as a name, an identification number, location data, an online identifier (e.g. cookie) or to one or more special features that express the physical, physiological, genetic, psychological, economic, cultural or social identity of that natural person. \"processing\" means any operation carried out with or without the aid of automated procedures or any such series of operations in connection with personal data. The term goes a long way and covers practically every handling of data. \"Responsible\" means the natural or legal person, authority, institution or other body that alone or together with others decides on the purposes and means of processing personal data. \"Processor\" means a natural or legal person, public authority, agency or other body that processes personal data on behalf of the controller.'","title":"Terms used"},{"location":"policy/#relevant-legal-bases","text":"In accordance with Art. 13 GDPR, we inform you of the legal basis of our data processing. If the legal basis is not mentioned in the data protection declaration, the following applies: The legal basis for obtaining consents is Art. 6 para. 1 lit. a and Art. 7 GDPR, the legal basis for processing for the performance of our services and performance of contractual measures as well as for answering inquiries is Art. 6 para. 1 lit. b GDPR, the legal basis for processing to fulfill our legal obligations is Art. 6 para. 1 lit. c GDPR, and the legal basis for processing to protect our legitimate interests is Art. 6 para. 1 lit. f GDPR. In the event that the vital interests of the data subject or another natural person require the processing of personal data, Art. 6 para. 1 lit. d GDPR serves as the legal basis.","title":"Relevant legal bases"},{"location":"policy/#updates-of-the-policy-statement","text":"We ask you to inform yourself regularly about the content of our privacy statement. We will adapt the privacy statement as soon as changes in the data processing carried out by us make this necessary. We will inform you as soon as the changes require your cooperation (e.g. consent) or other individual notification.","title":"Updates of the policy statement"},{"location":"policy/#collaboration-with-processors-and-third-parties","text":"If we disclose data to other persons and companies (contract processors or third parties) within the scope of our processing, transmit it to them or otherwise grant them access to the data, this shall only take place on the basis of a legal permission (e.g. if a transmission of the data to third parties, such as payment service providers, in accordance with Art. 6 Para. 1 lit. b GDPR for contract fulfillment is necessary), if you have consented, if a legal obligation provides for this or on the basis of our legitimate interests (e.g. when using agents, web hosts, etc.). If we commission third parties with the processing of data on the basis of a so-called \"order processing contract\", this is done on the basis of Art. 28 GDPR.","title":"Collaboration with processors and third parties"},{"location":"policy/#integration-of-services-and-contents-of-third-parties","text":"Based on our legitimate interests (i.e. interest in the analysis, optimization and economic operation of our online offer within the meaning of Art. 6 (1) lit. GDPR), we make use of content or services offered by third-party providers in order to provide their content and services Services, such as Include videos or fonts (collectively referred to as \"content\"). This always presupposes that the third-party providers of this content perceive the IP address of the users, since they could not send the content to their browser without the IP address. The IP address is therefore required for the presentation of this content. We endeavor to use only content whose respective providers use the IP address solely for the delivery of the content. Third parties may also use so-called pixel tags (invisible graphics, also referred to as \"web beacons\") for statistical or marketing purposes. The \"pixel tags\" can be used to evaluate information such as visitor traffic on the pages of this website. The pseudonymous information may also be stored in cookies on the user's device and may include, but is not limited to, technical information about the browser and operating system, referring web pages, visit time, and other information regarding the use of our online offer.","title":"Integration of services and contents of third parties"},{"location":"policy/#hosting-data-storage","text":"The hosting services we use serve to provide the following services: Infrastructure and platform services, computing capacity, storage space and database services, security services and technical maintenance services that we use for the purpose of operating this online offering. We or our hosting provider process inventory data, contact data, content data, contract data, usage data, meta- and communication data of customers, interested parties and visitors of this online offer on the basis of our legitimate interests in an efficient and secure provision of this online offer according to Art. 6 Para. 1 lit. f GDPR in conjunction with. Art. 28 GDPR (conclusion of order processing contract). The alinex.gitlab.io site is hosted by GitLab 268 Bush Street #350. San Francisco,CA 94104. Therefore their policy will be in effect for this, too.","title":"Hosting / Data storage"},{"location":"policy/#collection-of-access-data-and-log-files","text":"We, or our hosting provider, collect the following data on the basis of our legitimate interests within the meaning of Art. 6 para. 1 lit. f. GDPR data on each access to the server on which this service is located (so-called server log files). Access data includes the name of the accessed website, file, date and time of access, transferred data volume, notification of successful access, browser type and version, the user's operating system, referrer URL (the previously visited page), IP address and the requesting provider. Log file information is stored for a few days for troubleshooting and for security reasons (e.g. to detect misuse or fraud) and then deleted. Data whose further storage is required for evidentiary purposes are excluded from deletion until the respective incident has been finally clarified. {!docs/abbreviations.txt!}","title":"Collection of access data and log files"},{"location":"blog/2016/","text":"Developer Blog November 2016 KDE: Because I had to reinstall my notebook I decided to go back, after some years, to KDE through Linux Mint KDE edition. It works fine and has more integrated tools. After some configuration it looks and works as I want. August 2016 CodeDoc: After using JBT/Docker a long time I decided to create my own tool which creates documentation out of the code. It is based on markdown within the code files and above each methods which are transformed into HTML pages. A lot of languages are supported. July 2016 CodeDoc: Starting to build an alternative documentation tool to the till now used JBT/Docker . Linux Mint: I installed Linux Mint 18 based on Ubuntu 16.04 on my new notebook using the Cinnamon desktop. That was the logic consequence after using Mint 16 a long time. May 2016 Formatter Module: This module has the ability to transform data structures or tables between different formats. Table Module: To work and transform table based data structures I made a module which has different methods to do manipulations like sort, insert, join.. in an easy way. April 2016 Utility Module: Extend the module with deep clone and extend. Scripter: Base framework to make short CoffeeScript files runnable in an preset environment. Mailman: Command execution based on IMAP mailbox. All commands are setup by easy configurations. DbReport: Automatic reports based on queries to relational databases. The result is mostly send as HTML mail. March 2016 Docker: Switch to newer version of the documentation tool which now uses MarkdownIT and therefore has more abilities to format in the documents. The transform of the custom styles used some time but I got it done. February 2016 Handlebars: The default helpers are limited so I made a collection of extended helpers which make the language more powerful. Development Site: The new site structure of my development site now contains an overview of my modules, some development guides and my personal blog. January 2016 Atom: After using Sublime a long time I now switched to use Atom editor to develop. I had to invest some time to find and checkout the plugins I need to get a complete system supporting script development. Dual Boot: I tried a lot to get dual boot working but after some problems I decided to better format my hard disk and only install Linux on it. {!docs/abbreviations.txt!}","title":"Blog 2016"},{"location":"blog/2016/#developer-blog","text":"","title":"Developer Blog"},{"location":"blog/2016/#november-2016","text":"KDE: Because I had to reinstall my notebook I decided to go back, after some years, to KDE through Linux Mint KDE edition. It works fine and has more integrated tools. After some configuration it looks and works as I want.","title":"November 2016"},{"location":"blog/2016/#august-2016","text":"CodeDoc: After using JBT/Docker a long time I decided to create my own tool which creates documentation out of the code. It is based on markdown within the code files and above each methods which are transformed into HTML pages. A lot of languages are supported.","title":"August 2016"},{"location":"blog/2016/#july-2016","text":"CodeDoc: Starting to build an alternative documentation tool to the till now used JBT/Docker . Linux Mint: I installed Linux Mint 18 based on Ubuntu 16.04 on my new notebook using the Cinnamon desktop. That was the logic consequence after using Mint 16 a long time.","title":"July 2016"},{"location":"blog/2016/#may-2016","text":"Formatter Module: This module has the ability to transform data structures or tables between different formats. Table Module: To work and transform table based data structures I made a module which has different methods to do manipulations like sort, insert, join.. in an easy way.","title":"May 2016"},{"location":"blog/2016/#april-2016","text":"Utility Module: Extend the module with deep clone and extend. Scripter: Base framework to make short CoffeeScript files runnable in an preset environment. Mailman: Command execution based on IMAP mailbox. All commands are setup by easy configurations. DbReport: Automatic reports based on queries to relational databases. The result is mostly send as HTML mail.","title":"April 2016"},{"location":"blog/2016/#march-2016","text":"Docker: Switch to newer version of the documentation tool which now uses MarkdownIT and therefore has more abilities to format in the documents. The transform of the custom styles used some time but I got it done.","title":"March 2016"},{"location":"blog/2016/#february-2016","text":"Handlebars: The default helpers are limited so I made a collection of extended helpers which make the language more powerful. Development Site: The new site structure of my development site now contains an overview of my modules, some development guides and my personal blog.","title":"February 2016"},{"location":"blog/2016/#january-2016","text":"Atom: After using Sublime a long time I now switched to use Atom editor to develop. I had to invest some time to find and checkout the plugins I need to get a complete system supporting script development. Dual Boot: I tried a lot to get dual boot working but after some problems I decided to better format my hard disk and only install Linux on it. {!docs/abbreviations.txt!}","title":"January 2016"},{"location":"blog/2017/","text":"Developer Blog December 2017 React/Vue: I made a short comparison only based on the presentations and code examples of both and had a look over different modules using this. I find Vue more understandable and easy to use but need a component library to not create every element of the UI on my own. November 2017 Angular: Using angular is at first very shiny with full fledged start packs. But after some times I got problems understanding and correctly working with all the magic behind it. So I have to look further. Frontend: After a long time developing backend modules I now want to make a complete application including the frontend part and started checking out all the modern technologies in the frontend frameworks to decide what to use. October 2017 TypeScript : It is growing really fast so I had to check it out. But as always I need to use it, not only test it. Therefore I decided to make the rewrite of an earlier unfinished server module using TypeScript. It was an success but with some problems till everything works. Server Module : Started to continue the server module based on Hapi server. The basic works fine but a lot of modules have to be created... September 2017 Config/Validator : Integration of config methods and conversion to JS + Flow from the old CoffeeScript sources was completed. Flow : Use of Flow to get type safety in JavaScript is sometimes really tricky and hard to completely integrate. May 2017 ES.Next : Learn to use the more modern JavaScript languages like ES6 and ongoing with the use of Babel as transpiler. April 2017 Rust/Go Benchmarking : I did a comparison based on a simple web server serving a static GIF file. As Apache could deliver 12k/sec, Go using net/http did 55k/sec and Rust using Iron won with 77k/sec on my notebook. WebObjects : Application which allows to step through relational database structures based on configured relations. CodeDoc/Report : I got my CodeDoc module rewritten with an extended report module which also can transform documents from and to different modules. But it sometimes got stuck in transforming maybe in the heavy async parts. Debugging there was almost impossible because in step by step debugging it works. February 2017 Rust: Before doing something using Go I now also tried some times to use Rust. As Go is something in between a core language and scripting I think Rust is really great but needs a lot of time to do it correctly. Also the community and module repositories are small at the moment. January 2017 Go: As always I have a look on new emerging languages and I want to check out Go and its features to decide if I want to do some more with it. {!docs/abbreviations.txt!}","title":"Blog 2017"},{"location":"blog/2017/#developer-blog","text":"","title":"Developer Blog"},{"location":"blog/2017/#december-2017","text":"React/Vue: I made a short comparison only based on the presentations and code examples of both and had a look over different modules using this. I find Vue more understandable and easy to use but need a component library to not create every element of the UI on my own.","title":"December 2017"},{"location":"blog/2017/#november-2017","text":"Angular: Using angular is at first very shiny with full fledged start packs. But after some times I got problems understanding and correctly working with all the magic behind it. So I have to look further. Frontend: After a long time developing backend modules I now want to make a complete application including the frontend part and started checking out all the modern technologies in the frontend frameworks to decide what to use.","title":"November 2017"},{"location":"blog/2017/#october-2017","text":"TypeScript : It is growing really fast so I had to check it out. But as always I need to use it, not only test it. Therefore I decided to make the rewrite of an earlier unfinished server module using TypeScript. It was an success but with some problems till everything works. Server Module : Started to continue the server module based on Hapi server. The basic works fine but a lot of modules have to be created...","title":"October 2017"},{"location":"blog/2017/#september-2017","text":"Config/Validator : Integration of config methods and conversion to JS + Flow from the old CoffeeScript sources was completed. Flow : Use of Flow to get type safety in JavaScript is sometimes really tricky and hard to completely integrate.","title":"September 2017"},{"location":"blog/2017/#may-2017","text":"ES.Next : Learn to use the more modern JavaScript languages like ES6 and ongoing with the use of Babel as transpiler.","title":"May 2017"},{"location":"blog/2017/#april-2017","text":"Rust/Go Benchmarking : I did a comparison based on a simple web server serving a static GIF file. As Apache could deliver 12k/sec, Go using net/http did 55k/sec and Rust using Iron won with 77k/sec on my notebook. WebObjects : Application which allows to step through relational database structures based on configured relations. CodeDoc/Report : I got my CodeDoc module rewritten with an extended report module which also can transform documents from and to different modules. But it sometimes got stuck in transforming maybe in the heavy async parts. Debugging there was almost impossible because in step by step debugging it works.","title":"April 2017"},{"location":"blog/2017/#february-2017","text":"Rust: Before doing something using Go I now also tried some times to use Rust. As Go is something in between a core language and scripting I think Rust is really great but needs a lot of time to do it correctly. Also the community and module repositories are small at the moment.","title":"February 2017"},{"location":"blog/2017/#january-2017","text":"Go: As always I have a look on new emerging languages and I want to check out Go and its features to decide if I want to do some more with it. {!docs/abbreviations.txt!}","title":"January 2017"},{"location":"blog/2018/","text":"Developer Blog A look back to the year I was amazed about VueJS and got a working example running based on an ExpressJS server. In the second half the Rust language got my attention again, but put aside by working on a basic bash script library for operation tasks. I also made the change to GitLab and at the end the MkDocs tool. All in all I learned a lot, made good practical solutions and think I'm also on the right way into the future. December 2018 Bash-lib: Finally I had the time to present my bash-lib with full documentation also using MkDocs . For all who want do use it download files are available, too. MkDocs: As conclusion to the problems and discontinuing of free gitbook use I decided to switch to MkDocs which also is markdown based with a python static and extendable site builder. Also with this step a complete restructuring was done. See more at MkDocs Async Control in Bash: I made an extension to my bash libraries which allows to run a collection of processes combined parallel and in series with the ability to continue on the not finished task on a restart after abort. November 2018 SSH Control: A port of a previous multi-ssh tool (closed source) to the new bash lib. This is used for server management and deployment. Admin-Utils: Also build upon the bash library specific commands to help fixing some gaps (closed source). October 2018 Bash Library: Creation of a bash library with coloring, logging with file rotation, locking to serialize multiple processes and system analyzation methods. GitLab + Mattermost: Self hosting and administration of this systems. September 2018 Database Testing: Tryout of ORM and direct accessing PostgreSQL databases in Rust. August 2018 Rust by Example: With the knowledge of the Rust book I could create my web server and run it. But I am missing the experience and routine and so the code feels a bit uncontrollable to me. So I decided to step back and do some more simpler examples with different language parts, first. GitLab: Installation and use of self hosted GitLab with Mattermost. Objectix: My first project using Rust is a web server. The Goal should be to use it to gather object information like data, correctness checks and hints. Depending on the client the REST calls should return well formatted HTML or JSON. But first I have to get the Rust base under control ;-) July 2018 Rust: After I did a tryout and some tests while ago, I now want to really use Rust. Therefore my first steps are to dive into the language by first reading about the concepts again, making some simple examples and planing the first project. Authorization: Now it is also respected on the client side. The information comes out of the already existing authenticated user setting. Both user and roles may be also disabled. Server stability: By extending the logging into before and after logs with data the process can be better shown and a problem in the default populate hook was found and fixed by own populate handler. June 2018 Documentation Project: Updated this document which combines the external GitBook document with the GitHub page contents. Moving to GitLab: After checking out the free cloud based and standalone GitLab service I decided to migrate my active projects and documentation and go on with it. Authorization: Dialogs to define the abilities within the Portal and to later establish full server and client authorization. May 2018 GDPR: Because of the General Data Protection Regulation (GDPR) I had also to show a compatible privacy policy which was included into the application in the both supported languages German and English. Basic chat: To try the realtime channel connection between clients and server I added a test which basically works but lacks on some styling issues. Multiple users are able to chat between each other in one common chat room. Internationalization: added for the complete Portal Client . April 2018 Repair: A new CLI application with interactive dialogs should help me analyze problem in data structures. In contrast to the WebObjects program I now make a script for each object containing search, data, references and checks. March 2018 User administration: The management of user accounts in the Portal Client was completed including create, update and remove of accounts. Authentication: Finalize the authentication from client over the server against the mongo db users. Homepage: To further test Vue2 within the quasar framework I decided to make my personal web site (only some static information) using also the same technology stack. VS Code: Try out the alternative to Atom editor. I had some performance problems using Atom on my notebook. So I checked out the similar tool which is also build on electron & Co but at the moment better as IDE. February 2018 Client-Server-Connection: To make a stable and easy to use connection between the client and server part I had to try out different ways to integrate the Feathers client into Vue2. The result got me a module allowing direct service calls or using an VueEx store as state management system. January 2018 REST Server: The initial server is build with feathers and express supporting HTTP and socket.io connections. For the first working version I made a message service, JWT based authentication and simple validation working. That was the start of Alinex Portal . Server Planing: For the new client application I need a REST server with authentication. This should be done using NodeJs, too. So I had to look for the right technologies and decided to use: ExpressJS as web server, Feathers for REST and realtime API and its integrated Authentication . Quasar Framework: After some days of searching I decided based on my previous testings in some of the available web frameworks to give the newer Quasar Framework a chance and see how far I can go with it. First I had to read through the following documentations in about 2 days and tryout what I found there: Vue.js , vue-router , Quasar Framework , Vuelidate , Axios , JWT , Stylus . Framework Idea: I made a decision to make a modern administration panel usable in Browser, App and maybe as a Desktop Application, too. A REST service should later feed the panel. KDE Neon: After I work since more than a year at home successfully with KDE I now decided to do the same on my companies notebook. But not all distributions worked fine. I got lucky with KDE Neon. {!docs/abbreviations.txt!}","title":"Blog 2018"},{"location":"blog/2018/#developer-blog","text":"A look back to the year I was amazed about VueJS and got a working example running based on an ExpressJS server. In the second half the Rust language got my attention again, but put aside by working on a basic bash script library for operation tasks. I also made the change to GitLab and at the end the MkDocs tool. All in all I learned a lot, made good practical solutions and think I'm also on the right way into the future.","title":"Developer Blog"},{"location":"blog/2018/#december-2018","text":"Bash-lib: Finally I had the time to present my bash-lib with full documentation also using MkDocs . For all who want do use it download files are available, too. MkDocs: As conclusion to the problems and discontinuing of free gitbook use I decided to switch to MkDocs which also is markdown based with a python static and extendable site builder. Also with this step a complete restructuring was done. See more at MkDocs Async Control in Bash: I made an extension to my bash libraries which allows to run a collection of processes combined parallel and in series with the ability to continue on the not finished task on a restart after abort.","title":"December 2018"},{"location":"blog/2018/#november-2018","text":"SSH Control: A port of a previous multi-ssh tool (closed source) to the new bash lib. This is used for server management and deployment. Admin-Utils: Also build upon the bash library specific commands to help fixing some gaps (closed source).","title":"November 2018"},{"location":"blog/2018/#october-2018","text":"Bash Library: Creation of a bash library with coloring, logging with file rotation, locking to serialize multiple processes and system analyzation methods. GitLab + Mattermost: Self hosting and administration of this systems.","title":"October 2018"},{"location":"blog/2018/#september-2018","text":"Database Testing: Tryout of ORM and direct accessing PostgreSQL databases in Rust.","title":"September 2018"},{"location":"blog/2018/#august-2018","text":"Rust by Example: With the knowledge of the Rust book I could create my web server and run it. But I am missing the experience and routine and so the code feels a bit uncontrollable to me. So I decided to step back and do some more simpler examples with different language parts, first. GitLab: Installation and use of self hosted GitLab with Mattermost. Objectix: My first project using Rust is a web server. The Goal should be to use it to gather object information like data, correctness checks and hints. Depending on the client the REST calls should return well formatted HTML or JSON. But first I have to get the Rust base under control ;-)","title":"August 2018"},{"location":"blog/2018/#july-2018","text":"Rust: After I did a tryout and some tests while ago, I now want to really use Rust. Therefore my first steps are to dive into the language by first reading about the concepts again, making some simple examples and planing the first project. Authorization: Now it is also respected on the client side. The information comes out of the already existing authenticated user setting. Both user and roles may be also disabled. Server stability: By extending the logging into before and after logs with data the process can be better shown and a problem in the default populate hook was found and fixed by own populate handler.","title":"July 2018"},{"location":"blog/2018/#june-2018","text":"Documentation Project: Updated this document which combines the external GitBook document with the GitHub page contents. Moving to GitLab: After checking out the free cloud based and standalone GitLab service I decided to migrate my active projects and documentation and go on with it. Authorization: Dialogs to define the abilities within the Portal and to later establish full server and client authorization.","title":"June 2018"},{"location":"blog/2018/#may-2018","text":"GDPR: Because of the General Data Protection Regulation (GDPR) I had also to show a compatible privacy policy which was included into the application in the both supported languages German and English. Basic chat: To try the realtime channel connection between clients and server I added a test which basically works but lacks on some styling issues. Multiple users are able to chat between each other in one common chat room. Internationalization: added for the complete Portal Client .","title":"May 2018"},{"location":"blog/2018/#april-2018","text":"Repair: A new CLI application with interactive dialogs should help me analyze problem in data structures. In contrast to the WebObjects program I now make a script for each object containing search, data, references and checks.","title":"April 2018"},{"location":"blog/2018/#march-2018","text":"User administration: The management of user accounts in the Portal Client was completed including create, update and remove of accounts. Authentication: Finalize the authentication from client over the server against the mongo db users. Homepage: To further test Vue2 within the quasar framework I decided to make my personal web site (only some static information) using also the same technology stack. VS Code: Try out the alternative to Atom editor. I had some performance problems using Atom on my notebook. So I checked out the similar tool which is also build on electron & Co but at the moment better as IDE.","title":"March 2018"},{"location":"blog/2018/#february-2018","text":"Client-Server-Connection: To make a stable and easy to use connection between the client and server part I had to try out different ways to integrate the Feathers client into Vue2. The result got me a module allowing direct service calls or using an VueEx store as state management system.","title":"February 2018"},{"location":"blog/2018/#january-2018","text":"REST Server: The initial server is build with feathers and express supporting HTTP and socket.io connections. For the first working version I made a message service, JWT based authentication and simple validation working. That was the start of Alinex Portal . Server Planing: For the new client application I need a REST server with authentication. This should be done using NodeJs, too. So I had to look for the right technologies and decided to use: ExpressJS as web server, Feathers for REST and realtime API and its integrated Authentication . Quasar Framework: After some days of searching I decided based on my previous testings in some of the available web frameworks to give the newer Quasar Framework a chance and see how far I can go with it. First I had to read through the following documentations in about 2 days and tryout what I found there: Vue.js , vue-router , Quasar Framework , Vuelidate , Axios , JWT , Stylus . Framework Idea: I made a decision to make a modern administration panel usable in Browser, App and maybe as a Desktop Application, too. A REST service should later feed the panel. KDE Neon: After I work since more than a year at home successfully with KDE I now decided to do the same on my companies notebook. But not all distributions worked fine. I got lucky with KDE Neon. {!docs/abbreviations.txt!}","title":"January 2018"},{"location":"blog/2019/","text":"Developer Blog Looking back this was a year of change. I made a big step to a new base for my ALinex code using TypeScript. The modules are not only rewritten but also extended and optimized. Hopefully in the next year I can finish the Framework and go the way over the already started server to a client framework. December 2019 Service Test Tool: Now the DataStore and Validator are used productively as test tool to check different web services for their result. It is more fine grained and more detailed as the simpler monitor check in the monitoring systems and can be made easy configurable. See the documentation for that. November 2019 Confluence as Operations Manual: In a project to move over 200 web services on around 100 virtual machines with multiple database clusters and file storages I created an operations manual using confluence. The base is a domain model in which each host, cluster, service, database, share and process has it's own name. Under this names documentation pages are made which are interlinked with each other, partly updated from a host analysis program using confluence API and also linked to Monitoring, Provider, Jenkins, Code Repository and more. Update data tools: It was quite here in my blog, but only there. I didn't make a break, I also worked further on using, hardening and integrating my data structure tools: Data - now also got a diff tool DataStore - new shell protocol and more options with meta data sollection Validator - better schema import and regexp schema October 2019 REST Server: The old portal server was rewritten as Alinex Server using Typescript and Feathers 4. Now also with better debugging and Validator used for configuration. September 2019 Feathers.js/Quasar I am happy that the new version of Feathers V4 was released now, as my plan was to use this in the second half of the year. Also the quasar frontend framework reached Version 1.0 within the last months. So now I am able to switch to the new Versions and also change my code to TypeScript. This will go into a new major release of the server and admin portal hopefully around the end of the year. Feathers now supports TypeScript and has a its authentication base completely rebuild. New in quasar is the ability to also build SSR applications. Development with this may start within September. August 2019 Export/import Validation Schema: The Validator is nearly complete now. Also schema export/import was added to make it possible to define schema instead of coding it. The new version was released really working great now. The last two parts will follow the next month: new schema type: regexp \\ This will check for a valid regular expression, maybe also given as regexp string. validation of schema in importTree \\ This makes it possible to detect errors in the schema definition and also be more human friendly by using the sanitizing possibilities of the validator also in the schema. Data handling: More and more the package suite was growing the last months: Data - as a manipulation of data structures DataStore - adding load and store abilities also with multi source support Validator - adding Schema Validation to all of this So it looks like I got all together, what is needed to work with data structures. But there should it really be used? My first intention was to have a configuration loader. But you also need it to gather data and work on meta data structures or enhance security by validating all external data. This leads to a use case I first didn't saw but now is used a lot, to make service tests which validate the response. As you see this all, it is good but a real application needs more. In the second half of the year I may slowly come back to improve the portal system with a big rework which brings the older code to TypeScript. Hopefully I will have a framework working at the start of next year. July 2019 Consolidation: I stepped back to overlook the further planning for the next version of DataStore which will bring multiple location load and Validator using this in the new CLI version for configuration loading. So I got to the point also rethinking my module structure. This will lead to the following: a new data helper module to which I will move the access and edit methods from DataStore adding a deep merge function also extract the async methods from core into it's own module then use these new modules in DataStore and enable the multi file load and at last update the Validator, too. All these changes will take some time and should be done over the summer. This all leads to my big goal to make a base for a new web framework. Hopefully the next major version of the admin portal will be developed in autumn. Bash-Lib: I finalized a new version which mostly added more detection method on hardware and software detection like hardware detection, geo location, package analyzation, user and crontab analyzation. But also sudo detection and usage support and now with automatic testing using shunit2. June 2019 Async Helper: While developing I find myself often writing nearly the same helper functions like retry or forEach on promises. So I decided to lookout for the perfect module to use but didn't find the perfect fit for me. So I added such helper to the alinex-core library which are simple and easy to use but helps a lot. Schema types: As predicted I am going further on in rewriting also a lot of the special types for the validator, so now it also supports DateTime, Port, IP, Domain and Email. But some more are missing. Schema Builder: A routine task if using the validator is to write schema definitions. Therefore the Builder is now created which contains all the schema types and includes some easier configuration methods which let create a schema in one or only a few lines. Validator as Monitoring: Originally this module was planned as configuration helper, but as it is now I see it's use also on each data transport. So I use the alinex-validator at work productively to test web services, if they are working correctly. Therefore I build short mocha test cases in which a schema is validated against the web service URL. This case is documented as usage example. May 2019 Validator productive: The first schema types are converted, ready and running: any, string, array and object. Also the new references using the DataStore are implemented. So I am short before releasing the 3.0 Version of alinex-validator but it will be not complete and if you need some special types you may wait for 3.1 or 3.2 which should also be released in the next two months. Validator with i18n: The rebuild has begun, first parts are working again having a lot of work ahead. But I also included locales support for error message and describe methods from the base up. Data Store: It is now productive and complete in its feature set. As I implemented all the formats, compression and protocols I had the Idea to also use it for communication with REST services contained within the validator. So while step over to the validator, using the data store, I added a filter rule making the datastore the ideal component for data and reference resolution in the validator. April 2019 Data Store: To make the new validator I created the alinex-datastore module which will read, access and write data structures in different formats and locations. It is a big enhancement to the earlier format module. TypeScript: As I am going to rewrite my codebase one by one I first have to modernize my standards and document them within the alinex-core module and it's documentation site. Validator: I plan to use my old validator in the server and therefore will update it. This should be a partly rewrite with more features using modern JavaScript. But that's only the planing, the realization will come within the next months. March 2019 Admin Portal Update: The portal server will be updated and brought to current standards. This means to update the documentation technology and content and also update server and client to the newest versions. Planing: My plan for the year is to make a big progress on a client server application. As I tried a lot more with a Rust based server I come to the conclusion that it costs me too much time and at first I will go on with the JavaScript only solution. February 2019 System Analyzation: Within the bash-lib I got a big step further in analyzation systems. It now works on multiple Linux distributions but with the focus on Debian based distributions. Also in addition to hardware and OS analyzation it can now read cron entries, ssh users and major software packages. Rust CLI: Ground up work on the basically working server by further investigating and optimizing command line parsing using clap . January 2019 This Book: First of all I took some time to further optimize this book: CSS, Links, Typo, a short chapter about Freeplane , solutions chapter restructured and a bit more information about myself. Actix: Going on to work on the already created Rust Server which is based on Actix-web . First I will further more understand the framework which i decided to build the server on. I will create an abstract of it. {!docs/abbreviations.txt!}","title":"Blog 2019"},{"location":"blog/2019/#developer-blog","text":"Looking back this was a year of change. I made a big step to a new base for my ALinex code using TypeScript. The modules are not only rewritten but also extended and optimized. Hopefully in the next year I can finish the Framework and go the way over the already started server to a client framework.","title":"Developer Blog"},{"location":"blog/2019/#december-2019","text":"Service Test Tool: Now the DataStore and Validator are used productively as test tool to check different web services for their result. It is more fine grained and more detailed as the simpler monitor check in the monitoring systems and can be made easy configurable. See the documentation for that.","title":"December 2019"},{"location":"blog/2019/#november-2019","text":"Confluence as Operations Manual: In a project to move over 200 web services on around 100 virtual machines with multiple database clusters and file storages I created an operations manual using confluence. The base is a domain model in which each host, cluster, service, database, share and process has it's own name. Under this names documentation pages are made which are interlinked with each other, partly updated from a host analysis program using confluence API and also linked to Monitoring, Provider, Jenkins, Code Repository and more. Update data tools: It was quite here in my blog, but only there. I didn't make a break, I also worked further on using, hardening and integrating my data structure tools: Data - now also got a diff tool DataStore - new shell protocol and more options with meta data sollection Validator - better schema import and regexp schema","title":"November 2019"},{"location":"blog/2019/#october-2019","text":"REST Server: The old portal server was rewritten as Alinex Server using Typescript and Feathers 4. Now also with better debugging and Validator used for configuration.","title":"October 2019"},{"location":"blog/2019/#september-2019","text":"Feathers.js/Quasar I am happy that the new version of Feathers V4 was released now, as my plan was to use this in the second half of the year. Also the quasar frontend framework reached Version 1.0 within the last months. So now I am able to switch to the new Versions and also change my code to TypeScript. This will go into a new major release of the server and admin portal hopefully around the end of the year. Feathers now supports TypeScript and has a its authentication base completely rebuild. New in quasar is the ability to also build SSR applications. Development with this may start within September.","title":"September 2019"},{"location":"blog/2019/#august-2019","text":"Export/import Validation Schema: The Validator is nearly complete now. Also schema export/import was added to make it possible to define schema instead of coding it. The new version was released really working great now. The last two parts will follow the next month: new schema type: regexp \\ This will check for a valid regular expression, maybe also given as regexp string. validation of schema in importTree \\ This makes it possible to detect errors in the schema definition and also be more human friendly by using the sanitizing possibilities of the validator also in the schema. Data handling: More and more the package suite was growing the last months: Data - as a manipulation of data structures DataStore - adding load and store abilities also with multi source support Validator - adding Schema Validation to all of this So it looks like I got all together, what is needed to work with data structures. But there should it really be used? My first intention was to have a configuration loader. But you also need it to gather data and work on meta data structures or enhance security by validating all external data. This leads to a use case I first didn't saw but now is used a lot, to make service tests which validate the response. As you see this all, it is good but a real application needs more. In the second half of the year I may slowly come back to improve the portal system with a big rework which brings the older code to TypeScript. Hopefully I will have a framework working at the start of next year.","title":"August 2019"},{"location":"blog/2019/#july-2019","text":"Consolidation: I stepped back to overlook the further planning for the next version of DataStore which will bring multiple location load and Validator using this in the new CLI version for configuration loading. So I got to the point also rethinking my module structure. This will lead to the following: a new data helper module to which I will move the access and edit methods from DataStore adding a deep merge function also extract the async methods from core into it's own module then use these new modules in DataStore and enable the multi file load and at last update the Validator, too. All these changes will take some time and should be done over the summer. This all leads to my big goal to make a base for a new web framework. Hopefully the next major version of the admin portal will be developed in autumn. Bash-Lib: I finalized a new version which mostly added more detection method on hardware and software detection like hardware detection, geo location, package analyzation, user and crontab analyzation. But also sudo detection and usage support and now with automatic testing using shunit2.","title":"July 2019"},{"location":"blog/2019/#june-2019","text":"Async Helper: While developing I find myself often writing nearly the same helper functions like retry or forEach on promises. So I decided to lookout for the perfect module to use but didn't find the perfect fit for me. So I added such helper to the alinex-core library which are simple and easy to use but helps a lot. Schema types: As predicted I am going further on in rewriting also a lot of the special types for the validator, so now it also supports DateTime, Port, IP, Domain and Email. But some more are missing. Schema Builder: A routine task if using the validator is to write schema definitions. Therefore the Builder is now created which contains all the schema types and includes some easier configuration methods which let create a schema in one or only a few lines. Validator as Monitoring: Originally this module was planned as configuration helper, but as it is now I see it's use also on each data transport. So I use the alinex-validator at work productively to test web services, if they are working correctly. Therefore I build short mocha test cases in which a schema is validated against the web service URL. This case is documented as usage example.","title":"June 2019"},{"location":"blog/2019/#may-2019","text":"Validator productive: The first schema types are converted, ready and running: any, string, array and object. Also the new references using the DataStore are implemented. So I am short before releasing the 3.0 Version of alinex-validator but it will be not complete and if you need some special types you may wait for 3.1 or 3.2 which should also be released in the next two months. Validator with i18n: The rebuild has begun, first parts are working again having a lot of work ahead. But I also included locales support for error message and describe methods from the base up. Data Store: It is now productive and complete in its feature set. As I implemented all the formats, compression and protocols I had the Idea to also use it for communication with REST services contained within the validator. So while step over to the validator, using the data store, I added a filter rule making the datastore the ideal component for data and reference resolution in the validator.","title":"May 2019"},{"location":"blog/2019/#april-2019","text":"Data Store: To make the new validator I created the alinex-datastore module which will read, access and write data structures in different formats and locations. It is a big enhancement to the earlier format module. TypeScript: As I am going to rewrite my codebase one by one I first have to modernize my standards and document them within the alinex-core module and it's documentation site. Validator: I plan to use my old validator in the server and therefore will update it. This should be a partly rewrite with more features using modern JavaScript. But that's only the planing, the realization will come within the next months.","title":"April 2019"},{"location":"blog/2019/#march-2019","text":"Admin Portal Update: The portal server will be updated and brought to current standards. This means to update the documentation technology and content and also update server and client to the newest versions. Planing: My plan for the year is to make a big progress on a client server application. As I tried a lot more with a Rust based server I come to the conclusion that it costs me too much time and at first I will go on with the JavaScript only solution.","title":"March 2019"},{"location":"blog/2019/#february-2019","text":"System Analyzation: Within the bash-lib I got a big step further in analyzation systems. It now works on multiple Linux distributions but with the focus on Debian based distributions. Also in addition to hardware and OS analyzation it can now read cron entries, ssh users and major software packages. Rust CLI: Ground up work on the basically working server by further investigating and optimizing command line parsing using clap .","title":"February 2019"},{"location":"blog/2019/#january-2019","text":"This Book: First of all I took some time to further optimize this book: CSS, Links, Typo, a short chapter about Freeplane , solutions chapter restructured and a bit more information about myself. Actix: Going on to work on the already created Rust Server which is based on Actix-web . First I will further more understand the framework which i decided to build the server on. I will create an abstract of it. {!docs/abbreviations.txt!}","title":"January 2019"},{"location":"blog/2020/","text":"Developer Blog A new year and some new ideas to go further in development. July 2020 Multi Format REST Response : Based on the DataStore the ALinex Server got a REST response system in which the format can be specified using accept header with a lot of possibilities from json, csv till excel files. Upgrade GUI : Upgraded to the next Quasar and Quasar app version. June 2020 Alinex Server Productive : It is used at my work as backoffice system for some simple tasks in the moment. This shows that the extension of the base framework for individual use basically works and shows me the problems and optimizations. But the GUI is further under heavy development and may be usable till the end of this month. All in all this are early stages and I don't suggest to use it productively at the moment. May 2020 Modular Server and GUI : Making the REST Server modular, to be extended by specific business logic and extension modules. This should allow to develop a core system and use these in specialized projects by adding to it or overwriting some parts. Mkdocs Material 5 : Transition for all modules to the next major version of mkdocs material. March 2020 TS GUI Client : I started to try to build a client using quasar with TypeScript. This is a first start of an universal frontend for Alinex Server . But this was not running smoothly, I had often trouble because of incomplete definitions and also had to translate each piece of code while packing together. So I decided to go back and use vanilla JS. Rest Server Basics : After the server is now running using TypeScript this month is a time for consolidation. The main part is to better understand the control flow and make it visual using logging. Static books : Beside the HTML a PDF was available since some time but now I had to update it's creation and added also an ePub Version. As always the base HTML Site is the best. February 2020 Going to the frontend direction : This month I started to go my way further to the frontend. The earlier developed portal and portal client which was only a initial try should now be rebuild. My plan is to first complete the server with authentication and access right management this month. Then to build a multi device application on top in march... Validator simplification : Within the code maintenance phase I decided to make the definition of deep structures in validator simpler. To make the definition of complex structures using multiple objects and arrays within each other a new item definition has been implemented which replaces the previous keys or items setting. validator January 2020 Code maintenance : Good code has to be kept up to date. That means as my first thing this year will be to go over all actively used modules and update them to current standards. In this step I will try to add some smaller additions to give each also a little push forward. {!docs/abbreviations.txt!}","title":"Blog 2020"},{"location":"blog/2020/#developer-blog","text":"A new year and some new ideas to go further in development.","title":"Developer Blog"},{"location":"blog/2020/#july-2020","text":"Multi Format REST Response : Based on the DataStore the ALinex Server got a REST response system in which the format can be specified using accept header with a lot of possibilities from json, csv till excel files. Upgrade GUI : Upgraded to the next Quasar and Quasar app version.","title":"July 2020"},{"location":"blog/2020/#june-2020","text":"Alinex Server Productive : It is used at my work as backoffice system for some simple tasks in the moment. This shows that the extension of the base framework for individual use basically works and shows me the problems and optimizations. But the GUI is further under heavy development and may be usable till the end of this month. All in all this are early stages and I don't suggest to use it productively at the moment.","title":"June 2020"},{"location":"blog/2020/#may-2020","text":"Modular Server and GUI : Making the REST Server modular, to be extended by specific business logic and extension modules. This should allow to develop a core system and use these in specialized projects by adding to it or overwriting some parts. Mkdocs Material 5 : Transition for all modules to the next major version of mkdocs material.","title":"May 2020"},{"location":"blog/2020/#march-2020","text":"TS GUI Client : I started to try to build a client using quasar with TypeScript. This is a first start of an universal frontend for Alinex Server . But this was not running smoothly, I had often trouble because of incomplete definitions and also had to translate each piece of code while packing together. So I decided to go back and use vanilla JS. Rest Server Basics : After the server is now running using TypeScript this month is a time for consolidation. The main part is to better understand the control flow and make it visual using logging. Static books : Beside the HTML a PDF was available since some time but now I had to update it's creation and added also an ePub Version. As always the base HTML Site is the best.","title":"March 2020"},{"location":"blog/2020/#february-2020","text":"Going to the frontend direction : This month I started to go my way further to the frontend. The earlier developed portal and portal client which was only a initial try should now be rebuild. My plan is to first complete the server with authentication and access right management this month. Then to build a multi device application on top in march... Validator simplification : Within the code maintenance phase I decided to make the definition of deep structures in validator simpler. To make the definition of complex structures using multiple objects and arrays within each other a new item definition has been implemented which replaces the previous keys or items setting. validator","title":"February 2020"},{"location":"blog/2020/#january-2020","text":"Code maintenance : Good code has to be kept up to date. That means as my first thing this year will be to go over all actively used modules and update them to current standards. In this step I will try to add some smaller additions to give each also a little push forward. {!docs/abbreviations.txt!}","title":"January 2020"},{"location":"concepts/app/","text":"Application Architectures There are different concepts, so I start giving a brief historic overview before going into details of modern concepts. Afterwards the different modern concepts are described. Desktop Applications An application that runs stand-alone in a desktop or laptop computer. It runs on a PC operating system (Windows, Mac, Linux, etc.) as a binary distribution compiled for it. It should have a graphical user interface. Updates have to be installed locally. Here a lot of programming languages may be used like C, Java... Client Server Application If a desktop application is connecting to a server as centralized processing and data storage it is considered a client server application. The network connection mostly to servers in the local network is needed to work. Web Application \"Web-based application\" are client server applications which require the Web browser to run in. They will store and process on the server and only the presentation layer is on the client within the browser. The browser will load the page with data included, render it and wait for user interaction. User actions can lead to a new page request, response, rendering Here languages like PHP, Python, Java on the server are used delivering HTML, CSS, JavaScript to the browser for the UI. Hera a lot of different concepts exist to further structure the architecture like multi tier, MVC... Single Page Application A SPA is a website whose current page is updated dynamically rather than being entirely downloaded from a server. In other words, all the necessary application code for the web application like HTML, CSS and JavaScript needs to be loaded only once. As the user navigates through the web application, all contents and elements that need updating are fetched and re-rendered without requiring the user to reload the browser. This saves the traditional round trip between the browser and the server, thus allows for faster interactions and better user satisfaction. Only the lean data elements are loaded on demand. If the applications are big they tend to a long first loading time but then will present a fast reacting interface. Progressive Web Apps An PWA is still a websites, but it just look and feel like an app, thanks to modern web technology. Unlike SPA, a Progressive Web App (PWA) is more like a set of guidelines and checklists than a specific architecture and gives gives you Speed : Given the same content, PWA loads faster than normal websites thanks to Service Workers technology. It is fast at first load and even faster on second loads onwards since it pre-caches all contents and delivers them when needed. Offline mode : This is also made possible by Service Workers technology. All contents are pre-loaded the first time you visit the PWA and are delivered afterwards using Javascript. This is why it can work offline. After the first load It doesn\u2019t need to request anything more from server to deliver the contents. Add to home screen : This function prompts mobile users to \u201cinstall\u201d the PWA. After the user accepts the prompt, the PWA will be added to their mobile home screen, and it will run like any other installed app. A service worker is one of the key technologies in PWA and it includes access to device-level functionality like camera & microphone, GPS, offline mode, file access, and many more. Sever Side Rendered The most advantageous strategy is to rely on server-side rendering, a mature technique with rich tooling, which is not only perfect to render static content, but is also suitable for SEO and all connected to it. One of the main reasons when you develop a SSR instead of a SPA is for taking care of the SEO. And SEO can be greatly improved by using the Quasar Meta Plugin to manage dynamic HTML markup required by the search engines. {!docs/abbreviations.txt!}","title":"Application"},{"location":"concepts/app/#application-architectures","text":"There are different concepts, so I start giving a brief historic overview before going into details of modern concepts. Afterwards the different modern concepts are described.","title":"Application Architectures"},{"location":"concepts/app/#desktop-applications","text":"An application that runs stand-alone in a desktop or laptop computer. It runs on a PC operating system (Windows, Mac, Linux, etc.) as a binary distribution compiled for it. It should have a graphical user interface. Updates have to be installed locally. Here a lot of programming languages may be used like C, Java...","title":"Desktop Applications"},{"location":"concepts/app/#client-server-application","text":"If a desktop application is connecting to a server as centralized processing and data storage it is considered a client server application. The network connection mostly to servers in the local network is needed to work.","title":"Client Server Application"},{"location":"concepts/app/#web-application","text":"\"Web-based application\" are client server applications which require the Web browser to run in. They will store and process on the server and only the presentation layer is on the client within the browser. The browser will load the page with data included, render it and wait for user interaction. User actions can lead to a new page request, response, rendering Here languages like PHP, Python, Java on the server are used delivering HTML, CSS, JavaScript to the browser for the UI. Hera a lot of different concepts exist to further structure the architecture like multi tier, MVC...","title":"Web Application"},{"location":"concepts/app/#single-page-application","text":"A SPA is a website whose current page is updated dynamically rather than being entirely downloaded from a server. In other words, all the necessary application code for the web application like HTML, CSS and JavaScript needs to be loaded only once. As the user navigates through the web application, all contents and elements that need updating are fetched and re-rendered without requiring the user to reload the browser. This saves the traditional round trip between the browser and the server, thus allows for faster interactions and better user satisfaction. Only the lean data elements are loaded on demand. If the applications are big they tend to a long first loading time but then will present a fast reacting interface.","title":"Single Page Application"},{"location":"concepts/app/#progressive-web-apps","text":"An PWA is still a websites, but it just look and feel like an app, thanks to modern web technology. Unlike SPA, a Progressive Web App (PWA) is more like a set of guidelines and checklists than a specific architecture and gives gives you Speed : Given the same content, PWA loads faster than normal websites thanks to Service Workers technology. It is fast at first load and even faster on second loads onwards since it pre-caches all contents and delivers them when needed. Offline mode : This is also made possible by Service Workers technology. All contents are pre-loaded the first time you visit the PWA and are delivered afterwards using Javascript. This is why it can work offline. After the first load It doesn\u2019t need to request anything more from server to deliver the contents. Add to home screen : This function prompts mobile users to \u201cinstall\u201d the PWA. After the user accepts the prompt, the PWA will be added to their mobile home screen, and it will run like any other installed app. A service worker is one of the key technologies in PWA and it includes access to device-level functionality like camera & microphone, GPS, offline mode, file access, and many more.","title":"Progressive Web Apps"},{"location":"concepts/app/#sever-side-rendered","text":"The most advantageous strategy is to rely on server-side rendering, a mature technique with rich tooling, which is not only perfect to render static content, but is also suitable for SEO and all connected to it. One of the main reasons when you develop a SSR instead of a SPA is for taking care of the SEO. And SEO can be greatly improved by using the Quasar Meta Plugin to manage dynamic HTML markup required by the search engines. {!docs/abbreviations.txt!}","title":"Sever Side Rendered"},{"location":"concepts/exitcodes/","text":"Exit Codes The exit codes are a major part of applications especially if called form other commands. They help to easily detect if everything worked fine or anything made problems. Exit codes are returned from processes to their caller as an unsigned small integer with a value between 0..255. The definitions may vary. Other numbers can be used, but these are treated modulo 256, so exit -10 is equivalent to exit 246, and exit 257 is equivalent to exit 1. Status Code for Success In general a zero exit status indicates that a command succeeded , a non-zero exit status indicates failure. Unix Bash The exit codes are arranged alongside the UNIX default: Code Description 0 OK - no error 1 General error which should not occur 2 Misuse of shell builtins 124 command times out 125 if a command itself fails 126 Command invoked cannot execute 127 \"command not found\" 128 Invalid argument to exit 129 SIGHUP (Signal 1) 130 SIGINT like through ++ctrl+c++ (Signal 2) 131 SIGQUIT (Signal 3) 132 SIGILL (Signal 4) 133 SIGTRAP (Signal 5) 134 SIGABRT or SIGIOT (Signal 6) 135 SIGBUS (Signal 7) 136 SIGFPE (Signal 8) 137 SIGKILL (Signal 9) 143 SIGTERM (Signal 15) 255 Exit status out of range Not all signals are listed here, they are mapped to 128 + signal number . C/C++ Linux Codes The exit codes in C/C++ which are in the kernel can be also found in: /usr/include/asm-generic/errno-base.h /usr/include/asm-generic/errno.h cat /usr/include/asm-generic/errno-base.h /usr/include/asm-generic/errno.h | grep define | sed -r 's/.*\\t([0-9]+)\\t\\/\\* (.*) \\*\\//| \\1 | \\2 |/g' Code Description 0 OK - no error 1 Operation not permitted 2 No such file or directory 3 No such process 4 Interrupted system call 5 I/O error 6 No such device or address 7 Argument list too long 8 Exec format error 9 Bad file number 10 No child processes 11 Try again 12 Out of memory 13 Permission denied 14 Bad address 15 Block device required 16 Device or resource busy 17 File exists 18 Cross-device link 19 No such device 20 Not a directory 21 Is a directory 22 Invalid argument 23 File table overflow 24 Too many open files 25 Not a typewriter 26 Text file busy 27 File too large 28 No space left on device 29 Illegal seek 30 Read-only file system 31 Too many links 32 Broken pipe 33 Math argument out of domain of function 34 Math result not presentable 35 Resource deadlock would occur 36 File name too long 37 No record locks available 38 Function not implemented 39 Directory not empty 40 Too many symbolic links encountered 41 Operation would block 42 No message of desired type 43 Identifier removed 44 Channel number out of range 45 Level 2 not synchronized 46 Level 3 halted 47 Level 3 reset 48 Link number out of range 49 Protocol driver not attached 50 No CSI structure available 51 Level 2 halted 52 Invalid exchange 53 Invalid request descriptor 54 Exchange full 55 No anode 56 Invalid request code 57 Invalid slot 58 EDEADLOCK EDEADLK 59 Bad font file format 60 Device not a stream 61 No data available 62 Timer expired 63 Out of streams resources 64 Machine is not on the network 65 Package not installed 66 Object is remote 67 Link has been severed 68 Advertise error 69 RFS mount error 70 Communication error on send 71 Protocol error 72 Multi hop attempted 73 RFS specific error 74 Not a data message 75 Value too large for defined data type 76 Name not unique on network 77 File descriptor in bad state 78 Remote address changed 79 Can not access a needed shared library 80 Accessing a corrupted shared library 81 .lib section in a.out corrupted 82 Attempting to link in too many shared libraries 83 Cannot exec a shared library directly 84 Illegal byte sequence 85 Interrupted system call should be restarted 86 Streams pipe error 87 Too many users 88 Socket operation on non-socket 89 Destination address required 90 Message too long 91 Protocol wrong type for socket 92 Protocol not available 93 Protocol not supported 94 Socket type not supported 95 Operation not supported on transport endpoint 96 Protocol family not supported 97 Address family not supported by protocol 98 Address already in use 99 Cannot assign requested address 100 Network is down 101 Network is unreachable 102 Network dropped connection because of reset 103 Software caused connection abort 104 Connection reset by peer 105 No buffer space available 106 Transport endpoint is already connected 107 Transport endpoint is not connected 108 Cannot send after transport endpoint shutdown 109 Too many references: cannot splice 110 Connection timed out 111 Connection refused 112 Host is down 113 No route to host 114 Operation already in progress 115 Operation now in progress 116 Stale file handle 117 Structure needs cleaning 118 Not a XENIX named type file 119 No XENIX semaphores available 120 Is a named type file 121 Remote I/O error 122 Quota exceeded 123 No medium found 124 Wrong medium type 125 Operation Canceled 126 Required key not available 127 Key has expired 128 Key has been revoked 129 Key was rejected by service 130 Owner died 131 State not recoverable 132 Operation not possible due to RF-kill 133 Memory page has hardware error Conclusion The definitions defer between each other and are not compatible to each other. But each of them use 0 as success 1 as general error and have some room for your own additional definitions. Combined with bash it is advised to use 16-120 or >160 Combined with C/C++ use >160 {!docs/abbreviations.txt!}","title":"Exit Codes"},{"location":"concepts/exitcodes/#exit-codes","text":"The exit codes are a major part of applications especially if called form other commands. They help to easily detect if everything worked fine or anything made problems. Exit codes are returned from processes to their caller as an unsigned small integer with a value between 0..255. The definitions may vary. Other numbers can be used, but these are treated modulo 256, so exit -10 is equivalent to exit 246, and exit 257 is equivalent to exit 1.","title":"Exit Codes"},{"location":"concepts/exitcodes/#status-code-for-success","text":"In general a zero exit status indicates that a command succeeded , a non-zero exit status indicates failure.","title":"Status Code for Success"},{"location":"concepts/exitcodes/#unix-bash","text":"The exit codes are arranged alongside the UNIX default: Code Description 0 OK - no error 1 General error which should not occur 2 Misuse of shell builtins 124 command times out 125 if a command itself fails 126 Command invoked cannot execute 127 \"command not found\" 128 Invalid argument to exit 129 SIGHUP (Signal 1) 130 SIGINT like through ++ctrl+c++ (Signal 2) 131 SIGQUIT (Signal 3) 132 SIGILL (Signal 4) 133 SIGTRAP (Signal 5) 134 SIGABRT or SIGIOT (Signal 6) 135 SIGBUS (Signal 7) 136 SIGFPE (Signal 8) 137 SIGKILL (Signal 9) 143 SIGTERM (Signal 15) 255 Exit status out of range Not all signals are listed here, they are mapped to 128 + signal number .","title":"Unix Bash"},{"location":"concepts/exitcodes/#cc-linux-codes","text":"The exit codes in C/C++ which are in the kernel can be also found in: /usr/include/asm-generic/errno-base.h /usr/include/asm-generic/errno.h cat /usr/include/asm-generic/errno-base.h /usr/include/asm-generic/errno.h | grep define | sed -r 's/.*\\t([0-9]+)\\t\\/\\* (.*) \\*\\//| \\1 | \\2 |/g' Code Description 0 OK - no error 1 Operation not permitted 2 No such file or directory 3 No such process 4 Interrupted system call 5 I/O error 6 No such device or address 7 Argument list too long 8 Exec format error 9 Bad file number 10 No child processes 11 Try again 12 Out of memory 13 Permission denied 14 Bad address 15 Block device required 16 Device or resource busy 17 File exists 18 Cross-device link 19 No such device 20 Not a directory 21 Is a directory 22 Invalid argument 23 File table overflow 24 Too many open files 25 Not a typewriter 26 Text file busy 27 File too large 28 No space left on device 29 Illegal seek 30 Read-only file system 31 Too many links 32 Broken pipe 33 Math argument out of domain of function 34 Math result not presentable 35 Resource deadlock would occur 36 File name too long 37 No record locks available 38 Function not implemented 39 Directory not empty 40 Too many symbolic links encountered 41 Operation would block 42 No message of desired type 43 Identifier removed 44 Channel number out of range 45 Level 2 not synchronized 46 Level 3 halted 47 Level 3 reset 48 Link number out of range 49 Protocol driver not attached 50 No CSI structure available 51 Level 2 halted 52 Invalid exchange 53 Invalid request descriptor 54 Exchange full 55 No anode 56 Invalid request code 57 Invalid slot 58 EDEADLOCK EDEADLK 59 Bad font file format 60 Device not a stream 61 No data available 62 Timer expired 63 Out of streams resources 64 Machine is not on the network 65 Package not installed 66 Object is remote 67 Link has been severed 68 Advertise error 69 RFS mount error 70 Communication error on send 71 Protocol error 72 Multi hop attempted 73 RFS specific error 74 Not a data message 75 Value too large for defined data type 76 Name not unique on network 77 File descriptor in bad state 78 Remote address changed 79 Can not access a needed shared library 80 Accessing a corrupted shared library 81 .lib section in a.out corrupted 82 Attempting to link in too many shared libraries 83 Cannot exec a shared library directly 84 Illegal byte sequence 85 Interrupted system call should be restarted 86 Streams pipe error 87 Too many users 88 Socket operation on non-socket 89 Destination address required 90 Message too long 91 Protocol wrong type for socket 92 Protocol not available 93 Protocol not supported 94 Socket type not supported 95 Operation not supported on transport endpoint 96 Protocol family not supported 97 Address family not supported by protocol 98 Address already in use 99 Cannot assign requested address 100 Network is down 101 Network is unreachable 102 Network dropped connection because of reset 103 Software caused connection abort 104 Connection reset by peer 105 No buffer space available 106 Transport endpoint is already connected 107 Transport endpoint is not connected 108 Cannot send after transport endpoint shutdown 109 Too many references: cannot splice 110 Connection timed out 111 Connection refused 112 Host is down 113 No route to host 114 Operation already in progress 115 Operation now in progress 116 Stale file handle 117 Structure needs cleaning 118 Not a XENIX named type file 119 No XENIX semaphores available 120 Is a named type file 121 Remote I/O error 122 Quota exceeded 123 No medium found 124 Wrong medium type 125 Operation Canceled 126 Required key not available 127 Key has expired 128 Key has been revoked 129 Key was rejected by service 130 Owner died 131 State not recoverable 132 Operation not possible due to RF-kill 133 Memory page has hardware error","title":"C/C++ Linux Codes"},{"location":"concepts/exitcodes/#conclusion","text":"The definitions defer between each other and are not compatible to each other. But each of them use 0 as success 1 as general error and have some room for your own additional definitions. Combined with bash it is advised to use 16-120 or >160 Combined with C/C++ use >160 {!docs/abbreviations.txt!}","title":"Conclusion"},{"location":"env/","text":"Development Environment To start developing in good quality a lot of things are needed. Nothing good is coded just out of thin air. But what you need is depending on the position in the overall project flow you are in. This is no straight line, it can make sense to go through this stages for each feature or release and to go back some times. Most of the following mentioned tools are free for open source work but there are also powerful enterprise tools from which only Jira is mentioned here. The focus is on open source. Idea Every project starts with the idea, which is developed through brainstorming, discussions and recherche. Here you may need a mind mapping tool like Freeplane mind mapping and communication tools. Alternatively you may generated more structured graphs using a flow chart application like yEd . Issues As a result of the initial idea you split up the overall idea into multiple issues which have to be done. For an update you may already have collected issues from your users. This issues will also show the progress. GitLab or GitHub also contains issue management or use Trello as another simple tool. But there are lots of others like Jira (not free), Bugzilla and more, too. Planing By organizing the issues into milestones and releases the ToDo list will be made. An agile planing with kanban boards like Backlog, ToDo, InWork, Done may help. GitLab or GitHub both also allows to work on issues and organize them. Or use extensions like ZenHub , zube or also Jira (not free) as enterprise system. Coding While you work on the issues you have to write tests, code and documentation. Here a lot of tools are available per language which help you write code faster and keep a good structure. As editor you may use VS Code or Atom which are modern and work for every possible language. They have lots of features making them a small IDE but depending on your programming language you may use any of the big IDEs, too. Useful documentation can be written using markdown and be converted into HTML using MkDocs later. This is the same way this book is written. Additionally you may more technical API out of the code structure and inline doc comments. Commit Once you're happy with the code part or want to give the current state to others, you can commit version control using branches. The big player in version control is Git . You can use it directly or administer it through GitLab or GitHub which both supports a lot more. Test To make the code stable, it's best to make unit and integration tests for as much code as possible. Each language has it's special tools here and you can also use CI to automate it. Review For better quality a code review may be done. Here the language specific API converted into HTML and the free manual formatted by MkDocs are needed. Staging If possible first deploy to a staging area there the end users also can check the new version in a nearly production environment. Production When we have everything working as it should, it's time to deploy to our production environment! Feedback Now it's time to look back and check what stage of our work needs improvement. And finally go back the flow for the next release... {!docs/abbreviations.txt!}","title":"Overview"},{"location":"env/#development-environment","text":"To start developing in good quality a lot of things are needed. Nothing good is coded just out of thin air. But what you need is depending on the position in the overall project flow you are in. This is no straight line, it can make sense to go through this stages for each feature or release and to go back some times. Most of the following mentioned tools are free for open source work but there are also powerful enterprise tools from which only Jira is mentioned here. The focus is on open source.","title":"Development Environment"},{"location":"env/#idea","text":"Every project starts with the idea, which is developed through brainstorming, discussions and recherche. Here you may need a mind mapping tool like Freeplane mind mapping and communication tools. Alternatively you may generated more structured graphs using a flow chart application like yEd .","title":"Idea"},{"location":"env/#issues","text":"As a result of the initial idea you split up the overall idea into multiple issues which have to be done. For an update you may already have collected issues from your users. This issues will also show the progress. GitLab or GitHub also contains issue management or use Trello as another simple tool. But there are lots of others like Jira (not free), Bugzilla and more, too.","title":"Issues"},{"location":"env/#planing","text":"By organizing the issues into milestones and releases the ToDo list will be made. An agile planing with kanban boards like Backlog, ToDo, InWork, Done may help. GitLab or GitHub both also allows to work on issues and organize them. Or use extensions like ZenHub , zube or also Jira (not free) as enterprise system.","title":"Planing"},{"location":"env/#coding","text":"While you work on the issues you have to write tests, code and documentation. Here a lot of tools are available per language which help you write code faster and keep a good structure. As editor you may use VS Code or Atom which are modern and work for every possible language. They have lots of features making them a small IDE but depending on your programming language you may use any of the big IDEs, too. Useful documentation can be written using markdown and be converted into HTML using MkDocs later. This is the same way this book is written. Additionally you may more technical API out of the code structure and inline doc comments.","title":"Coding"},{"location":"env/#commit","text":"Once you're happy with the code part or want to give the current state to others, you can commit version control using branches. The big player in version control is Git . You can use it directly or administer it through GitLab or GitHub which both supports a lot more.","title":"Commit"},{"location":"env/#test","text":"To make the code stable, it's best to make unit and integration tests for as much code as possible. Each language has it's special tools here and you can also use CI to automate it.","title":"Test"},{"location":"env/#review","text":"For better quality a code review may be done. Here the language specific API converted into HTML and the free manual formatted by MkDocs are needed.","title":"Review"},{"location":"env/#staging","text":"If possible first deploy to a staging area there the end users also can check the new version in a nearly production environment.","title":"Staging"},{"location":"env/#production","text":"When we have everything working as it should, it's time to deploy to our production environment!","title":"Production"},{"location":"env/#feedback","text":"Now it's time to look back and check what stage of our work needs improvement. And finally go back the flow for the next release... {!docs/abbreviations.txt!}","title":"Feedback"},{"location":"env/atom/","text":"Atom After using Sublime Text 3 for some time I was impressed about Atom . Atom is mostly based on JavaScript itself to be easy expendable with plugins. Because of that there are a lot of extensions for nearly every language and need. Now atom really grew to a full fledged IDE. Installation That is an easy task, only download and install it directly from their homepage. After that you may install the needed plugins, but only install what you really needs else it may get bloated. To reset everything remove the directory ~/.atom . Plugins General atom-ide-ui - tools for ide character-table - to easy find any UTF character Extended project-manager - helps to switch between different workspaces highlight-selected - show all occurrences of the selected text markdown-pdf - transform markdown into PDF pdf-view - view PDF files inline open-in-browser - to open HTML from the project tree keybinding-cheatsheet - display possible keys tool-bar-atom - customizable toolbar Programming docblockr - helper to write inline documentation todo-show - opens a pane for all to-do's split-diff - shows visual diff between split panes atom-beautify - highlight some included languages pigments - color display on definition platformio-ide-terminal - integrated console atom-ternjs - auto completion linter-json-lint - JSON linter TypeScript ide-typescript - language support linter-tslint - TypeScript linter ES.Next & Co linter-eslint - included linting flow-ide - to lint flow syntax and auto complete autocomplete-modules - for require/import auto completion Also there are language and linter plugins for nearly every language and script you use. Basic Shortcuts At first it is very easy and a lot like sublime. The major shortcut to know for a command search is: ++ctrl+shift+p++ Additionally the following may be helpful: ++ctrl+backslash++ will toggle the file tree view ++alt+backslash++ switch focus to/from tree view ++ctrl+t++ open fuzzy find for files in current project ++ctrl+b++ open fuzzy search to search in open files ++alt+shift+p++ switch projects (using project-manager) Editing Move Cursor ++ctrl+left++ move to beginning of word ++ctrl+right++ move to end of word ++ctrl+g++ go to line (line:column) Edit ++ctrl+up++ / ++ctrl+down++ moves the current line or selected lines up/down Multi cursor ++alt+shift+up++ / ++alt+shift+down++ adds another cursor one line above/below ++ctrl+left-button++ add or remove another cursor ++ctrl+d++ / ++ctrl+u++ ad and remove cursor at the next match of selected word ++esc++ to remove multi cursor and go back to a single one Maybe extend the key mapping with the following: '.platform-linux atom-text-editor': 'shift-alt-L': 'editor:split-selections-into-lines' Code Specific Helpers Show Help ++ctrl+shift+m++ shows a rendered preview (realtime) ++alt+shift+t++ shows the to-do list (using todo-show ) Using Git Modify ++ctrl+shift+h++ show menu of all git commands ++ctrl+shift+a++ adds new files ++ctrl+shift+a++ ++s++ show status of files ++ctrl+shift+a++ ++c++ commit changes (give a message and save ++ctrl+s++) ++ctrl+shift+a++ ++p++ add all changed files and commit + push to remote {!docs/abbreviations.txt!}","title":"Atom"},{"location":"env/atom/#atom","text":"After using Sublime Text 3 for some time I was impressed about Atom . Atom is mostly based on JavaScript itself to be easy expendable with plugins. Because of that there are a lot of extensions for nearly every language and need. Now atom really grew to a full fledged IDE.","title":"Atom"},{"location":"env/atom/#installation","text":"That is an easy task, only download and install it directly from their homepage. After that you may install the needed plugins, but only install what you really needs else it may get bloated. To reset everything remove the directory ~/.atom .","title":"Installation"},{"location":"env/atom/#plugins","text":"","title":"Plugins"},{"location":"env/atom/#general","text":"atom-ide-ui - tools for ide character-table - to easy find any UTF character","title":"General"},{"location":"env/atom/#extended","text":"project-manager - helps to switch between different workspaces highlight-selected - show all occurrences of the selected text markdown-pdf - transform markdown into PDF pdf-view - view PDF files inline open-in-browser - to open HTML from the project tree keybinding-cheatsheet - display possible keys tool-bar-atom - customizable toolbar","title":"Extended"},{"location":"env/atom/#programming","text":"docblockr - helper to write inline documentation todo-show - opens a pane for all to-do's split-diff - shows visual diff between split panes atom-beautify - highlight some included languages pigments - color display on definition platformio-ide-terminal - integrated console atom-ternjs - auto completion linter-json-lint - JSON linter","title":"Programming"},{"location":"env/atom/#typescript","text":"ide-typescript - language support linter-tslint - TypeScript linter","title":"TypeScript"},{"location":"env/atom/#esnext-co","text":"linter-eslint - included linting flow-ide - to lint flow syntax and auto complete autocomplete-modules - for require/import auto completion Also there are language and linter plugins for nearly every language and script you use.","title":"ES.Next &amp; Co"},{"location":"env/atom/#basic-shortcuts","text":"At first it is very easy and a lot like sublime. The major shortcut to know for a command search is: ++ctrl+shift+p++ Additionally the following may be helpful: ++ctrl+backslash++ will toggle the file tree view ++alt+backslash++ switch focus to/from tree view ++ctrl+t++ open fuzzy find for files in current project ++ctrl+b++ open fuzzy search to search in open files ++alt+shift+p++ switch projects (using project-manager)","title":"Basic Shortcuts"},{"location":"env/atom/#editing","text":"Move Cursor ++ctrl+left++ move to beginning of word ++ctrl+right++ move to end of word ++ctrl+g++ go to line (line:column) Edit ++ctrl+up++ / ++ctrl+down++ moves the current line or selected lines up/down Multi cursor ++alt+shift+up++ / ++alt+shift+down++ adds another cursor one line above/below ++ctrl+left-button++ add or remove another cursor ++ctrl+d++ / ++ctrl+u++ ad and remove cursor at the next match of selected word ++esc++ to remove multi cursor and go back to a single one Maybe extend the key mapping with the following: '.platform-linux atom-text-editor': 'shift-alt-L': 'editor:split-selections-into-lines'","title":"Editing"},{"location":"env/atom/#code-specific-helpers","text":"Show Help ++ctrl+shift+m++ shows a rendered preview (realtime) ++alt+shift+t++ shows the to-do list (using todo-show )","title":"Code Specific Helpers"},{"location":"env/atom/#using-git","text":"Modify ++ctrl+shift+h++ show menu of all git commands ++ctrl+shift+a++ adds new files ++ctrl+shift+a++ ++s++ show status of files ++ctrl+shift+a++ ++c++ commit changes (give a message and save ++ctrl+s++) ++ctrl+shift+a++ ++p++ add all changed files and commit + push to remote {!docs/abbreviations.txt!}","title":"Using Git"},{"location":"env/freeplane/","text":"Freeplane Mind Mapping Tool Freeplane is a free and open source software application that allows to create visual appealing structured information. It runs on any operating system that has a current version of Java installed. A MindMap is a visualization of minds by putting words around a topic and connect them by lines. You will read it from the center outwards and get more detailed. It is made visual appealing by the use of colors, different shapes and symbols. But to be productive with it, you need to learn the user interface first. Example {: width=\"100%\" } From the freeplane website Usage You start your MindMap with the topic in the center. To navigate you can use the mouse and menus, but to be effective you should do at least the essential data entering using keyboard shortcuts: Shortcuts File commands: New map - ++ctrl+n++ Open map - ++ctrl+o++ Save map - ++ctrl+s++ Save as - ++ctrl+a++ Print - ++ctrl+p++ Close - ++ctrl+w++ Quit - ++ctrl+q++ Previous map - ++ctrl+left++ Next Map - ++ctrl+right++ Export file to HTML - ++ctrl+e++ Export branch to HTML - ++ctrl+h++ Export branch to new MM file - ++alt+a++ Open first file in history - ++ctrl+shift+w++ New node commands: Add sibling node - ++enter++ Add child node - ++insert++ Add sibling before - ++shift+enter++ Node editing commands: Edit selected node - ++f2++ Edit long node - ++alt+enter++ Join nodes - ++ctrl+j++ Toggle folded - ++space++ Toggle children folded - ++ctrl+space++ Set link by filechooser - ++ctrl+shift+k++ Set link by text entry - ++ctrl+k++ Set image by filechooser - ++alt+k++ Move node up - ++ctrl+up++ Move node down - ++ctrl+down++ Edit commands: Find - ++ctrl+f++ Find next - ++ctrl+g++ Cut - ++ctrl+x++ Copy - ++ctrl+c++ Copy single - ++ctrl+y++ Paste - ++ctrl+v++ Node formatting commands: Italicize - ++ctrl+i++ Bold - ++ctrl+b++ Cloud - ++ctrl+shift+b++ Change node color - ++alt+c++ Blend node color - ++alt+b++ Change node edge color - ++alt+e++ Increase node font size - ++ctrl+l++ decrease node font size - ++ctrl+m++ Increase branch font size - ++ctrl+shift+l++ Decrease branch font size - ++ctrl+shift+m++ Node navigation commands: Go to root - ++esc++ Move up - ++up++ Move down - ++down++ Move left - ++left++ Move right - ++right++ Follow link - ++ctrl+enter++ Zoom out - ++alt+up++ Zoom in - ++alt+down++ Mode commands: MindMap mode - ++alt+1++ Browse mode - ++alt+2++ File mode - ++alt+3++ {!docs/abbreviations.txt!}","title":"Freeplane"},{"location":"env/freeplane/#freeplane-mind-mapping-tool","text":"Freeplane is a free and open source software application that allows to create visual appealing structured information. It runs on any operating system that has a current version of Java installed. A MindMap is a visualization of minds by putting words around a topic and connect them by lines. You will read it from the center outwards and get more detailed. It is made visual appealing by the use of colors, different shapes and symbols. But to be productive with it, you need to learn the user interface first.","title":"Freeplane Mind Mapping Tool"},{"location":"env/freeplane/#example","text":"{: width=\"100%\" } From the freeplane website","title":"Example"},{"location":"env/freeplane/#usage","text":"You start your MindMap with the topic in the center. To navigate you can use the mouse and menus, but to be effective you should do at least the essential data entering using keyboard shortcuts:","title":"Usage"},{"location":"env/freeplane/#shortcuts","text":"File commands: New map - ++ctrl+n++ Open map - ++ctrl+o++ Save map - ++ctrl+s++ Save as - ++ctrl+a++ Print - ++ctrl+p++ Close - ++ctrl+w++ Quit - ++ctrl+q++ Previous map - ++ctrl+left++ Next Map - ++ctrl+right++ Export file to HTML - ++ctrl+e++ Export branch to HTML - ++ctrl+h++ Export branch to new MM file - ++alt+a++ Open first file in history - ++ctrl+shift+w++ New node commands: Add sibling node - ++enter++ Add child node - ++insert++ Add sibling before - ++shift+enter++ Node editing commands: Edit selected node - ++f2++ Edit long node - ++alt+enter++ Join nodes - ++ctrl+j++ Toggle folded - ++space++ Toggle children folded - ++ctrl+space++ Set link by filechooser - ++ctrl+shift+k++ Set link by text entry - ++ctrl+k++ Set image by filechooser - ++alt+k++ Move node up - ++ctrl+up++ Move node down - ++ctrl+down++ Edit commands: Find - ++ctrl+f++ Find next - ++ctrl+g++ Cut - ++ctrl+x++ Copy - ++ctrl+c++ Copy single - ++ctrl+y++ Paste - ++ctrl+v++ Node formatting commands: Italicize - ++ctrl+i++ Bold - ++ctrl+b++ Cloud - ++ctrl+shift+b++ Change node color - ++alt+c++ Blend node color - ++alt+b++ Change node edge color - ++alt+e++ Increase node font size - ++ctrl+l++ decrease node font size - ++ctrl+m++ Increase branch font size - ++ctrl+shift+l++ Decrease branch font size - ++ctrl+shift+m++ Node navigation commands: Go to root - ++esc++ Move up - ++up++ Move down - ++down++ Move left - ++left++ Move right - ++right++ Follow link - ++ctrl+enter++ Zoom out - ++alt+up++ Zoom in - ++alt+down++ Mode commands: MindMap mode - ++alt+1++ Browse mode - ++alt+2++ File mode - ++alt+3++ {!docs/abbreviations.txt!}","title":"Shortcuts"},{"location":"env/git/","text":"Git Version control is the only reasonable way to keep track of changes in code and documentation. It allows to step back to any previous version easily. But the really big advantage comes if multiple developers work together. There are some different version control systems available but after the use of git since some time it was clear to go on with it. Not only because it is good but also because it has a lot of other advantages coupled with its tools. Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. Git is easy to learn and has a tiny footprint with features like cheap local branching, convenient staging areas, and multiple workflows. Git revolutionized version control for when you have more than one developer working on a project. Read more in the Git book Installation At first install the repository software and CLI: sudo apt-get install git-all The setting of the default user will tell Git that you don\u2019t want to type your user name and email every time your computer talks to Git. # use the settings you use on the central origin repository (maybe GitHub) $ git config --global user.name alinex $ git config --global user.email info@alinex.de Allow Password caching To also keep Git from asking for your password every time, you need to turn on the credential helper so that Git will save your password in memory for some time: git config --global credential.helper cache This will keep the password for 15 minutes in memory. But you can also have Git store your credentials permanently using the following: git config --global credential.helper store Note: While this is convenient, Git will store your credentials in clear text in a local file .git-credentials under your \u201chome\u201d directory Basic Concept The most common commands are shown in the following work flow. Here everything is shown using CLI calls but you may use with any Git tool or your editors integration for Git. Git works with: a workspace which is a view on the files in a specific branch at a defined time which you see also as files the staging area is a collection of changes to be committed the local repository which contains all data for the branches on your local machine the remote repository also referred as the origin is the central storage (multiple are possible) Using some git commands you can now transport data between the repositories: Initially to work on an existing project you have to clone it first: git clone <repository url> cd <repo dir> This will create a complete clone of the whole repository in a subdirectory with the file name from the repository URL. Instead you may also fork the repository which makes a copy of the repository on the server. This is useful on GitHub or GitLab if you can only read. The forked project is your own, you can do everything in it and also make a pull request back to the origin with your changes. After changing something you have to add the changes: git add -A <changed path> If one change is done commit it with a short descriptive message (will be asked interactively): git commit Using git commit -a will do both steps in one go. Later if your whole work is done you can push your local changes to the origin server so that other developers will also get them. git pull # first pull the changes of others and merge them (automatically) git push # now send your own changes If you reached a new major, minor or bug fix version or also a special state of the code you should add a tag to get this version easily later: git tag -a v1.4.0 -m 'version 1.4.0 with ....' Here a new version 1.4.0 is tagged. Branching Branching enables you to isolate your work from others. Changes in the primary branch or other branches will not affect your branch, unless you decide to pull the latest changes from those branches. A branch in Git is simply a lightweight movable pointer to the last commit in a row. All branches exist next to each other. Each Git repository (remote or local) contains at least the default master branch. In the following figures the use of branches is visually displayed as swim lanes with circles for each commit. With git checkout -b <name> a new branch will be created. You now work on this branch. The following commands may help further: git branch to show the current branch git checkout <name> to switch to another branch git branch -d <name> to remove a branch If you're done you may merge it with the master: git checkout master git merge <name> # merge the named branch into master Different branches can be merged into any one branch as long as they belong to the same repository. To sync branches between local and remote repository you have to do it for each branch or use the --all switch: git pull --all git push --all This two commands will get all remote changes of all branches to your local repository and then uploads all local changes and branches to the remote repository. The organization of branches may differ on your needs, you may use Git here in different ways. Central branch In this approach the master branch serve as the single point-of-entry for all changes to the project. This may be the right concept to start the work with git or for very small projects with a single developer it should also be sufficient. Later you can always switch to more sophisticated workflows by adding more branches. In this concept you have to check that your code is fully functioning before you transfer your local changes to the remote repository. If you break it, you also break the master on the remote repository and after a pull for all other developers, too. Therefore it is essential to tag the releases to find the last functioning point which can be used for production. Development branch A good first s base is to make two main branches with infinite lifetime: master \u200a-\u200athis branch contains production code. All development code is merged into master if released. develop \u200a-\u200athis branch contains pre-production code. When the features are finished they are merged into develop. No commits should be made in master directly. It's also convenient to tag all merges to the master branch with a version number. Feature branches The base idea is to let the development of each feature or topic take place in it's own branch. Often different teams work on different features and this way they won't disturb each other. The possibility of merge conflicts will be reduced if clear separation is done. With each single feature or topic like a new package, a new function, or some kind of optimization a new feature branch will be created. It can be pushed to the central repository for backup/collaboration. It is branched from the develop branch which now takes the place of an integration branch. Changes on the develop branch should should only be small fixes. The name of each feature branch should be descriptive like newsBlog . They should also be pushed to the remote repository to allow collaboration. And they also end if they are merged back to develop . If the development is unsuccessful for a feature or stopped the branch can be discarded or kept open. Flow workflow This workflow described by Vincent Driessen is especially for projects with release plan. It defines some branch names with specific roles. It is not only a model a specific toolset for git will help you with it. Install git flow tools in Debian using apt-get install git-flow . The base is to use a develop branch as described above with feature branches. But it will also add: release branches are used as preparation for new production releases. Minor bugfixes and meta data changes (version numbers, build dates) are done here. They are named using the new release version like release-1.2.0 . If done it they will merge to master and develop . hotfix branches are like release branches but they are unplanned and be forged from the master . The normal development work can go on while the fix branched is made in parallel. If done it will be merged with master and the current release branch or develop . Both are being deleted after they are merged. Using the additional flow toolset: git flow init -d will create the initial develop branch git flow feature start <your feature> to start on a new feature git flow feature publish <your feature> to push to origin git flow feature pull <your feature> to pull from origin git flow feature finish <name> to end a feature by merging with develop git flow release start <release> [<base>] start a release branch from <base> on develop git flow release finish <release> this will merge to master git flow hotfix start <release> [<base>] git flow hotfix finish <release> Support branches Support branches are essential if you need to maintain multiple major versions at the same time. You could use support branches for supporting minor releases as well. They will be forged from the master and are a dead end, so no need to merge with develop anytime. Name them with the version pattern they will support like support-1.x or support 1.4.x . There may also be hotfix branches which are based on it. If a hotfix only for a support branch is made it only have to be merged back to the support branch. This can be used in addition to any of the other workflows. Conclusion There is no one-size-fits-all workflow, it should be simple and enhance the productivity of your team. And it should fit in with your business requirements and project workflow. If decided it should be documented how to work, to be convenient over your projects and help the developers to concentrate on their real work. To keep a clean history you should use --no-ff on merges. Merges To bring the changes of two branches or repositories together you have to merge them. Mostly you merge a branch back to another branch or the changes between the local and origin repository. git merge <branch> will try to merge the given branch into the current git merge --abort on merge conflicts you may abort the merge git merge --continue go on after conflict is solved Git will decide how the changes from the last common commit will be put together. The fast-forward algorithm will be used when a linear path exists and will put all commits of the given branch behind the current one. This behavior can be suppressed with the --no-ff option. If changes were made in both branches git has to use the 3-way merge . If in both branches changes exist in the same file on the same position it will result in a merge conflict. $ git merge iss53 Auto-merging index.html CONFLICT (content): Merge conflict in index.html Automatic merge failed; fix conflicts and then commit the result. To get an overview of the conflicts you can always call git status . Within the conflicting file you will find one or multiple markers: <<<<<<< HEAD <div id=\"footer\">contact : email.support@the.net</div> ======= <div id=\"footer\"> please contact us at support@the.net </div> >>>>>>> iss53 The first is the version from the current branch, while the second is from the merging branch. You have to solve it by replacing the whole part with the correct content (one, the other or a new one). To solve this in a graphical tool use git mergetool and if fixed add the changes in the normal way using git add and git commit . Worktree In git it is possible to work in multiple branches in parallel, so while working on a new feature you can create a parallel directory with the master branch to make a hotfix and have both in your editor at the same time. # create an parallel worktree for master git worktree add ../my-repo-master master # remove it after done git worktree remove ../my-repor-master You can also open a new branch as worktree: # create an parallel worktree for master git worktree add -b f_branch ../my-repo-feature origin/master Submodules Git allow you to modularize your repository. This is done by the use of submodules which are references to other git repositories. To add a sub repository in a folder of the current repository use: git submodule add https://github.com/<user>/<repo> lib/path If you go into the submodule path you will directly work in the submodule's repository! If this submodule may have changed you should update submodules from the base folder of the parent repository: git submodule update --init --recursive To checkout a project using submodules best way is to use: git clone --recursive <project url> Extract path To create a new repository out of some files, you have to clone the repository to a new folder on your machine and run: git filter-branch --subdirectory-filter <path> -- --all This will erase all data from the repository which is not related to this sub path. Now you have to create the new repository and change the origin to this one: git remote set-url origin https://github.com/<user>/<new repo> git push Now the extracted repository is ready, but you have to replace the code with the submodule now in the original repository: git rm -r <path> git commit -m \"Remove path (preparing for submodule)\" Then add it again as submodule: git submodule add https://github.com/<user>/<new repo> <path> git commit -m \"path submodule\" Don't forget to mention to all collaborators that submodules are used and to make recursive clones and updates. Remove To remove a git submodule the following steps are needed: git submodule deinit -f -- a/submodul rm -rf .git/modules/a/submodule git rm -f a/submodule Special use cases Moving repository First you have to fetch all remote branches: git fetch origin Now check if all branches are local: git branch -a If there are some branches listed as remote which didn\u2019t exist as local ones you have to check them out: git checkout -b <name> origin/<name> Now define a new remote repository: git remote add new-origin <url> Everything set up so you may transfer the repository: git push --all new-origin git push --tags new-origin At last you may delete the old origin and rename it: git remote rm origin git remote rename new-origin origin Pull/push to origin Use the preset names: git pull origin master git push origin master Subversion Integration If you have a subversion server as master you may also use git for your work and sync the changes back to subversion as the master repository. To use this you have to install the extension: apt-get install git-svn To do so you first init your git repository from the subversion master and load the initial data: $ git svn clone -s https://subversion.local/svn/my-repo # -s is for --stdlayout which presumes the svn recommended layout for tags, trunk, and branches # if your repo don't use that remove this option If you don\u2019t want the complete history you may use: $ git svn clone -s -r 40000:HEAD https://subversion.local/svn//my-repo # -r is for the revision to start taking history from To update your repository to HEAD of subversion master run: git svn rebase And to push your commits further to subversion: git svn dcommit Working with forks If you have a fork you may add an additional remote repository: git remote add upstream To sync this you have to: git fetch upstream git checkout master git merge upstream/master Server Setup If you want to use your own central repository you have to setup a git server: Install the required package sudo apt-get install -y git-core Authenticated ssh access Next you create a local user to access git using ssh: sudo adduser git su git cd mkdir .ssh && chmod 700 .ssh touch .ssh/authorized_keys && chmod 600 .ssh/authorized_keys Next you need to add each developer SSH public keys to the authorized_keys file for the git user. cat /tmp/id_rsa.john.pub >> ~/.ssh/authorized_keys Now, you can set up an empty repository for them as described below. The address will be git@gitserver:/var/git/project.git You can easily restrict the git user to only doing Git activities with a limited shell tool called git-shell that comes with Git. cat /etc/shells # see if `git-shell` is already in there. If not... which git-shell # make sure git-shell is installed on your system. sudo vim /etc/shells # and add the path to git-shell from last command sudo chsh git # and enter the path to git-shell, usually: /usr/bin/git-shell Now, the git user can only use the SSH connection to push and pull Git repositories and can\u2019t shell onto the machine. If you try, you\u2019ll see a login rejection like this: $ ssh git@gitserver fatal: Interactive git shell is not enabled. Connection to gitserver closed. HTTP Access This is done using the Apache web server with its possibilities. Add the following to the Apache site configuration: SetEnv GIT_PROJECT_ROOT /var/git SetEnv GIT_HTTP_EXPORT_ALL ScriptAlias /git/ /usr/lib/git-core/git-http-backend/ <Directory \"/usr/lib/git-core*\"> Options ExecCGI Indexes Order allow,deny Allow from all Require all granted </Directory> <LocationMatch \"^/git/\"> AuthType Basic AuthName \"Git Access\" AuthUserFile /etc/apache2/git.passwd AuthGroupFile /etc/apache2/git.groups Require valid-user </LocationMatch> Now you may access the server. Web Interface gitweb apt-get install -y gitweb Create a new server repository First create a bare repository: git --bare init <myrepo> cd <myrepo> git --bare update-server-info cd .. chown -R www-data:www-data <myrepo> Now make a new local repository: git init <path> After you have everything committed add the remote and push the repository: git remote add origin <url> git push --all origin git push --tags origin Add the login credentials in the .netrc file which is used by curl: $ cat ~/.netrc machine <git.yourdomain.com> login reader password reader Migrate from Subversion Best way is to use git-svn-migrate , which can also convert multiple repository at once. {!docs/abbreviations.txt!}","title":"Git"},{"location":"env/git/#git","text":"Version control is the only reasonable way to keep track of changes in code and documentation. It allows to step back to any previous version easily. But the really big advantage comes if multiple developers work together. There are some different version control systems available but after the use of git since some time it was clear to go on with it. Not only because it is good but also because it has a lot of other advantages coupled with its tools. Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. Git is easy to learn and has a tiny footprint with features like cheap local branching, convenient staging areas, and multiple workflows. Git revolutionized version control for when you have more than one developer working on a project. Read more in the Git book","title":"Git"},{"location":"env/git/#installation","text":"At first install the repository software and CLI: sudo apt-get install git-all The setting of the default user will tell Git that you don\u2019t want to type your user name and email every time your computer talks to Git. # use the settings you use on the central origin repository (maybe GitHub) $ git config --global user.name alinex $ git config --global user.email info@alinex.de Allow Password caching To also keep Git from asking for your password every time, you need to turn on the credential helper so that Git will save your password in memory for some time: git config --global credential.helper cache This will keep the password for 15 minutes in memory. But you can also have Git store your credentials permanently using the following: git config --global credential.helper store Note: While this is convenient, Git will store your credentials in clear text in a local file .git-credentials under your \u201chome\u201d directory","title":"Installation"},{"location":"env/git/#basic-concept","text":"The most common commands are shown in the following work flow. Here everything is shown using CLI calls but you may use with any Git tool or your editors integration for Git. Git works with: a workspace which is a view on the files in a specific branch at a defined time which you see also as files the staging area is a collection of changes to be committed the local repository which contains all data for the branches on your local machine the remote repository also referred as the origin is the central storage (multiple are possible) Using some git commands you can now transport data between the repositories: Initially to work on an existing project you have to clone it first: git clone <repository url> cd <repo dir> This will create a complete clone of the whole repository in a subdirectory with the file name from the repository URL. Instead you may also fork the repository which makes a copy of the repository on the server. This is useful on GitHub or GitLab if you can only read. The forked project is your own, you can do everything in it and also make a pull request back to the origin with your changes. After changing something you have to add the changes: git add -A <changed path> If one change is done commit it with a short descriptive message (will be asked interactively): git commit Using git commit -a will do both steps in one go. Later if your whole work is done you can push your local changes to the origin server so that other developers will also get them. git pull # first pull the changes of others and merge them (automatically) git push # now send your own changes If you reached a new major, minor or bug fix version or also a special state of the code you should add a tag to get this version easily later: git tag -a v1.4.0 -m 'version 1.4.0 with ....' Here a new version 1.4.0 is tagged.","title":"Basic Concept"},{"location":"env/git/#branching","text":"Branching enables you to isolate your work from others. Changes in the primary branch or other branches will not affect your branch, unless you decide to pull the latest changes from those branches. A branch in Git is simply a lightweight movable pointer to the last commit in a row. All branches exist next to each other. Each Git repository (remote or local) contains at least the default master branch. In the following figures the use of branches is visually displayed as swim lanes with circles for each commit. With git checkout -b <name> a new branch will be created. You now work on this branch. The following commands may help further: git branch to show the current branch git checkout <name> to switch to another branch git branch -d <name> to remove a branch If you're done you may merge it with the master: git checkout master git merge <name> # merge the named branch into master Different branches can be merged into any one branch as long as they belong to the same repository. To sync branches between local and remote repository you have to do it for each branch or use the --all switch: git pull --all git push --all This two commands will get all remote changes of all branches to your local repository and then uploads all local changes and branches to the remote repository. The organization of branches may differ on your needs, you may use Git here in different ways.","title":"Branching"},{"location":"env/git/#central-branch","text":"In this approach the master branch serve as the single point-of-entry for all changes to the project. This may be the right concept to start the work with git or for very small projects with a single developer it should also be sufficient. Later you can always switch to more sophisticated workflows by adding more branches. In this concept you have to check that your code is fully functioning before you transfer your local changes to the remote repository. If you break it, you also break the master on the remote repository and after a pull for all other developers, too. Therefore it is essential to tag the releases to find the last functioning point which can be used for production.","title":"Central branch"},{"location":"env/git/#development-branch","text":"A good first s base is to make two main branches with infinite lifetime: master \u200a-\u200athis branch contains production code. All development code is merged into master if released. develop \u200a-\u200athis branch contains pre-production code. When the features are finished they are merged into develop. No commits should be made in master directly. It's also convenient to tag all merges to the master branch with a version number.","title":"Development branch"},{"location":"env/git/#feature-branches","text":"The base idea is to let the development of each feature or topic take place in it's own branch. Often different teams work on different features and this way they won't disturb each other. The possibility of merge conflicts will be reduced if clear separation is done. With each single feature or topic like a new package, a new function, or some kind of optimization a new feature branch will be created. It can be pushed to the central repository for backup/collaboration. It is branched from the develop branch which now takes the place of an integration branch. Changes on the develop branch should should only be small fixes. The name of each feature branch should be descriptive like newsBlog . They should also be pushed to the remote repository to allow collaboration. And they also end if they are merged back to develop . If the development is unsuccessful for a feature or stopped the branch can be discarded or kept open.","title":"Feature branches"},{"location":"env/git/#flow-workflow","text":"This workflow described by Vincent Driessen is especially for projects with release plan. It defines some branch names with specific roles. It is not only a model a specific toolset for git will help you with it. Install git flow tools in Debian using apt-get install git-flow . The base is to use a develop branch as described above with feature branches. But it will also add: release branches are used as preparation for new production releases. Minor bugfixes and meta data changes (version numbers, build dates) are done here. They are named using the new release version like release-1.2.0 . If done it they will merge to master and develop . hotfix branches are like release branches but they are unplanned and be forged from the master . The normal development work can go on while the fix branched is made in parallel. If done it will be merged with master and the current release branch or develop . Both are being deleted after they are merged. Using the additional flow toolset: git flow init -d will create the initial develop branch git flow feature start <your feature> to start on a new feature git flow feature publish <your feature> to push to origin git flow feature pull <your feature> to pull from origin git flow feature finish <name> to end a feature by merging with develop git flow release start <release> [<base>] start a release branch from <base> on develop git flow release finish <release> this will merge to master git flow hotfix start <release> [<base>] git flow hotfix finish <release>","title":"Flow workflow"},{"location":"env/git/#support-branches","text":"Support branches are essential if you need to maintain multiple major versions at the same time. You could use support branches for supporting minor releases as well. They will be forged from the master and are a dead end, so no need to merge with develop anytime. Name them with the version pattern they will support like support-1.x or support 1.4.x . There may also be hotfix branches which are based on it. If a hotfix only for a support branch is made it only have to be merged back to the support branch. This can be used in addition to any of the other workflows.","title":"Support branches"},{"location":"env/git/#conclusion","text":"There is no one-size-fits-all workflow, it should be simple and enhance the productivity of your team. And it should fit in with your business requirements and project workflow. If decided it should be documented how to work, to be convenient over your projects and help the developers to concentrate on their real work. To keep a clean history you should use --no-ff on merges.","title":"Conclusion"},{"location":"env/git/#merges","text":"To bring the changes of two branches or repositories together you have to merge them. Mostly you merge a branch back to another branch or the changes between the local and origin repository. git merge <branch> will try to merge the given branch into the current git merge --abort on merge conflicts you may abort the merge git merge --continue go on after conflict is solved Git will decide how the changes from the last common commit will be put together. The fast-forward algorithm will be used when a linear path exists and will put all commits of the given branch behind the current one. This behavior can be suppressed with the --no-ff option. If changes were made in both branches git has to use the 3-way merge . If in both branches changes exist in the same file on the same position it will result in a merge conflict. $ git merge iss53 Auto-merging index.html CONFLICT (content): Merge conflict in index.html Automatic merge failed; fix conflicts and then commit the result. To get an overview of the conflicts you can always call git status . Within the conflicting file you will find one or multiple markers: <<<<<<< HEAD <div id=\"footer\">contact : email.support@the.net</div> ======= <div id=\"footer\"> please contact us at support@the.net </div> >>>>>>> iss53 The first is the version from the current branch, while the second is from the merging branch. You have to solve it by replacing the whole part with the correct content (one, the other or a new one). To solve this in a graphical tool use git mergetool and if fixed add the changes in the normal way using git add and git commit .","title":"Merges"},{"location":"env/git/#worktree","text":"In git it is possible to work in multiple branches in parallel, so while working on a new feature you can create a parallel directory with the master branch to make a hotfix and have both in your editor at the same time. # create an parallel worktree for master git worktree add ../my-repo-master master # remove it after done git worktree remove ../my-repor-master You can also open a new branch as worktree: # create an parallel worktree for master git worktree add -b f_branch ../my-repo-feature origin/master","title":"Worktree"},{"location":"env/git/#submodules","text":"Git allow you to modularize your repository. This is done by the use of submodules which are references to other git repositories. To add a sub repository in a folder of the current repository use: git submodule add https://github.com/<user>/<repo> lib/path If you go into the submodule path you will directly work in the submodule's repository! If this submodule may have changed you should update submodules from the base folder of the parent repository: git submodule update --init --recursive To checkout a project using submodules best way is to use: git clone --recursive <project url>","title":"Submodules"},{"location":"env/git/#extract-path","text":"To create a new repository out of some files, you have to clone the repository to a new folder on your machine and run: git filter-branch --subdirectory-filter <path> -- --all This will erase all data from the repository which is not related to this sub path. Now you have to create the new repository and change the origin to this one: git remote set-url origin https://github.com/<user>/<new repo> git push Now the extracted repository is ready, but you have to replace the code with the submodule now in the original repository: git rm -r <path> git commit -m \"Remove path (preparing for submodule)\" Then add it again as submodule: git submodule add https://github.com/<user>/<new repo> <path> git commit -m \"path submodule\" Don't forget to mention to all collaborators that submodules are used and to make recursive clones and updates.","title":"Extract path"},{"location":"env/git/#remove","text":"To remove a git submodule the following steps are needed: git submodule deinit -f -- a/submodul rm -rf .git/modules/a/submodule git rm -f a/submodule","title":"Remove"},{"location":"env/git/#special-use-cases","text":"","title":"Special use cases"},{"location":"env/git/#moving-repository","text":"First you have to fetch all remote branches: git fetch origin Now check if all branches are local: git branch -a If there are some branches listed as remote which didn\u2019t exist as local ones you have to check them out: git checkout -b <name> origin/<name> Now define a new remote repository: git remote add new-origin <url> Everything set up so you may transfer the repository: git push --all new-origin git push --tags new-origin At last you may delete the old origin and rename it: git remote rm origin git remote rename new-origin origin Pull/push to origin Use the preset names: git pull origin master git push origin master","title":"Moving repository"},{"location":"env/git/#subversion-integration","text":"If you have a subversion server as master you may also use git for your work and sync the changes back to subversion as the master repository. To use this you have to install the extension: apt-get install git-svn To do so you first init your git repository from the subversion master and load the initial data: $ git svn clone -s https://subversion.local/svn/my-repo # -s is for --stdlayout which presumes the svn recommended layout for tags, trunk, and branches # if your repo don't use that remove this option If you don\u2019t want the complete history you may use: $ git svn clone -s -r 40000:HEAD https://subversion.local/svn//my-repo # -r is for the revision to start taking history from To update your repository to HEAD of subversion master run: git svn rebase And to push your commits further to subversion: git svn dcommit","title":"Subversion Integration"},{"location":"env/git/#working-with-forks","text":"If you have a fork you may add an additional remote repository: git remote add upstream To sync this you have to: git fetch upstream git checkout master git merge upstream/master","title":"Working with forks"},{"location":"env/git/#server-setup","text":"If you want to use your own central repository you have to setup a git server:","title":"Server Setup"},{"location":"env/git/#install-the-required-package","text":"sudo apt-get install -y git-core","title":"Install the required package"},{"location":"env/git/#authenticated-ssh-access","text":"Next you create a local user to access git using ssh: sudo adduser git su git cd mkdir .ssh && chmod 700 .ssh touch .ssh/authorized_keys && chmod 600 .ssh/authorized_keys Next you need to add each developer SSH public keys to the authorized_keys file for the git user. cat /tmp/id_rsa.john.pub >> ~/.ssh/authorized_keys Now, you can set up an empty repository for them as described below. The address will be git@gitserver:/var/git/project.git You can easily restrict the git user to only doing Git activities with a limited shell tool called git-shell that comes with Git. cat /etc/shells # see if `git-shell` is already in there. If not... which git-shell # make sure git-shell is installed on your system. sudo vim /etc/shells # and add the path to git-shell from last command sudo chsh git # and enter the path to git-shell, usually: /usr/bin/git-shell Now, the git user can only use the SSH connection to push and pull Git repositories and can\u2019t shell onto the machine. If you try, you\u2019ll see a login rejection like this: $ ssh git@gitserver fatal: Interactive git shell is not enabled. Connection to gitserver closed.","title":"Authenticated ssh access"},{"location":"env/git/#http-access","text":"This is done using the Apache web server with its possibilities. Add the following to the Apache site configuration: SetEnv GIT_PROJECT_ROOT /var/git SetEnv GIT_HTTP_EXPORT_ALL ScriptAlias /git/ /usr/lib/git-core/git-http-backend/ <Directory \"/usr/lib/git-core*\"> Options ExecCGI Indexes Order allow,deny Allow from all Require all granted </Directory> <LocationMatch \"^/git/\"> AuthType Basic AuthName \"Git Access\" AuthUserFile /etc/apache2/git.passwd AuthGroupFile /etc/apache2/git.groups Require valid-user </LocationMatch> Now you may access the server.","title":"HTTP Access"},{"location":"env/git/#web-interface-gitweb","text":"apt-get install -y gitweb","title":"Web Interface gitweb"},{"location":"env/git/#create-a-new-server-repository","text":"First create a bare repository: git --bare init <myrepo> cd <myrepo> git --bare update-server-info cd .. chown -R www-data:www-data <myrepo> Now make a new local repository: git init <path> After you have everything committed add the remote and push the repository: git remote add origin <url> git push --all origin git push --tags origin Add the login credentials in the .netrc file which is used by curl: $ cat ~/.netrc machine <git.yourdomain.com> login reader password reader","title":"Create a new server repository"},{"location":"env/git/#migrate-from-subversion","text":"Best way is to use git-svn-migrate , which can also convert multiple repository at once. {!docs/abbreviations.txt!}","title":"Migrate from Subversion"},{"location":"env/gitbook/","text":"GitBook You can create static pages from Markdown with static site generators like GitBook . GitBook is also an online platform for writing and hosting documentation using open source book format and tool-chain. It has a big user base and can be integrated with GitHub. It is open source and you can also host it yourself or add it to your GitLab pages repository like I did with this book. Setup Local setup Then I set up the repository by placing a book.json in it's root directory. It should at least contain the definition of the root directory used for the book. I need this because I store my documentation beside the code in its own directory. { \"root\": \"./doc\" } Then you need the doc directory with the following files: doc/README.md doc/SUMMARY.de The SUMMARY.de should contain your table of contents as bullet list with optional headings for chapters. This could look like: # Summary - [Introduction](README.md) - [Alinex Project](alinex.md) ### Standards - [Quality](quality.md) - [Documentation](doc.md) - [File Structure](filestructure.md) - [Exit Codes](exitcodes.md) ### Modules - [Alinex](modules.md) - [3rd Party](3rd-party.md) GitBook Cloud Firstly I had to install the GitHub Integrations Plugin within the GitBook options. Also the GitHub had to be allowed to use for GitBook. To have a special cover on the PDF, ePub version of the book is done by providing two images: cover.jpg cover_small.jpg A good cover should respect the following guidelines: Size of 1800x2360 pixels for cover.jpg Size of 200x262 pixels for cover_small.jpg No border Clearly visible book title Any important text should be visible in the small version And to set the layout you may use: { \"pdf\": { \"pageNumbers\": true, \"headerTemplate\": \" \", \"footerTemplate\": \" \" } } GitLab First you have to setup the CI Pipeline to create and upload your pages. For GitBook this is done by placing the following .gitlab-ci.yml file in your projects root: # requiring the environment of NodeJS 8.9.x LTS (carbon) image: node:8.9 # add 'node_modules' to cache for speeding up builds cache: paths: - node_modules/ # Node modules and dependencies before_script: - npm install gitbook-cli -g # install gitbook - gitbook fetch latest # fetch latest stable version - gitbook install # add any requested plugins in book.json #- gitbook fetch pre # fetch latest pre-release version # the 'pages' job will deploy and build your site to the 'public' path pages: stage: deploy script: - gitbook build . public # build to public path - find public -type f -iregex '.*\\.\\(htm\\|html\\|txt\\|text\\|js\\|css\\)$' -execdir gzip -f --keep {} \\; # make compressed files artifacts: paths: - public expire_in: 1 week only: - master # this job will affect only the 'master' branch Push this to your master to add the pipelines. Now you should enable pages under Settings > Pages . So whenever you push new changes to your master the job will run and recreate your static pages. Writing Documentation All the pages are written as single files in the doc folder using Markdown language. It is nearly the same as used on GitHub. Attention: In contrast to the other markdown parsers there should be no space between the code tag and the language. Plugins Layout To improve the layout of the book I use three different plugins: { \"plugins\": [\"toggle-chapters\", \"navigator\", \"downloadpdf\"] } This will open/close chapters like folders, display a page navigation on the right side and a link to download as PDF on the top line. To optimize the navigator output set the following in styles/ebook.css and styles/pdf.css to disable navigator: #anchors-navbar, #goTop { display: none; } And for styles/website.css add this to optimize display and remove for small display: #anchors-navbar { color: darkgray; right: 28px; top: 45px; } #goTop { display: none; } @media (max-width: 660px) { #anchors-navbar { display: none; } } ToDo As already used in GitHub markdown this plugin allows to write ToDo lists with check boxes which may be checked: { \"plugins\": [\"todo\"] } Now you may create a checklist using: [ ] Mercury [x] Venus [x] Earth (Orbit/Moon) [x] Mars [ ] Jupiter [ ] Saturn [ ] Uranus [ ] Neptune [ ] Comet Haley This is done using: - [ ] Mercury - [x] Venus - [x] Earth (Orbit/Moon) - [x] Mars - [ ] Jupiter - [ ] Saturn - [ ] Uranus - [ ] Neptune - [ ] Comet Haley Mermaid Graphs Using Mermaid it is possible to include easy flowcharts without a specific program. It is written as plaintext and converted into chart on display. To make this work the following plugin have to be defined: { \"plugins\": [\"mermaid-gb3\"] } As an example can be: ```mermaid graph TD; A-->B; A-->C; B-->D; C-->D; ``` PlantUML PlantUML is another format to make graphs out of text descriptions like mermaid. The plugin is loaded using: { \"plugins\": [\"puml\"] } And the diagram may be added as: {% plantuml %} Bob->Alice : hello {% endplantuml %} Final Setup book.json { \"root\": \"./doc\", # only gor direct gitbook.com cloud hosting with enabled pdf \"pdf\": { \"pageNumbers\": true, \"headerTemplate\": \" \", \"footerTemplate\": \" \" }, \"plugins\": [ \"todo\", \"mermaid-gb3\", \"puml\", \"navigator\", \"collapsible-chapters\", \"hide-published-with\", \"insert-logo\" ], \"pluginsConfig\": { \"insert-logo\": { \"url\": \"https://alinex.gitlab.io/logo.png\", \"style\": \"background: none;\" }, # also only for pdf on github.com \"downloadpdf\": { \"base\": \"https://www.gitbook.com/download/pdf/book/alinex/nodejs\", \"label\": \"Download PDF\", \"multilingual\": false } } } Further files doc/README.md # Introduction doc/SUMMARY.md # Page index of book doc/cover.jpg # eBook cover doc/cover_small.jpg # small cover doc/styles/ebook.css # user style doc/styles/pdf.css # user style doc/styles/website.css # user style {!docs/abbreviations.txt!}","title":"GitBook"},{"location":"env/gitbook/#gitbook","text":"You can create static pages from Markdown with static site generators like GitBook . GitBook is also an online platform for writing and hosting documentation using open source book format and tool-chain. It has a big user base and can be integrated with GitHub. It is open source and you can also host it yourself or add it to your GitLab pages repository like I did with this book.","title":"GitBook"},{"location":"env/gitbook/#setup","text":"","title":"Setup"},{"location":"env/gitbook/#local-setup","text":"Then I set up the repository by placing a book.json in it's root directory. It should at least contain the definition of the root directory used for the book. I need this because I store my documentation beside the code in its own directory. { \"root\": \"./doc\" } Then you need the doc directory with the following files: doc/README.md doc/SUMMARY.de The SUMMARY.de should contain your table of contents as bullet list with optional headings for chapters. This could look like: # Summary - [Introduction](README.md) - [Alinex Project](alinex.md) ### Standards - [Quality](quality.md) - [Documentation](doc.md) - [File Structure](filestructure.md) - [Exit Codes](exitcodes.md) ### Modules - [Alinex](modules.md) - [3rd Party](3rd-party.md)","title":"Local setup"},{"location":"env/gitbook/#gitbook-cloud","text":"Firstly I had to install the GitHub Integrations Plugin within the GitBook options. Also the GitHub had to be allowed to use for GitBook. To have a special cover on the PDF, ePub version of the book is done by providing two images: cover.jpg cover_small.jpg A good cover should respect the following guidelines: Size of 1800x2360 pixels for cover.jpg Size of 200x262 pixels for cover_small.jpg No border Clearly visible book title Any important text should be visible in the small version And to set the layout you may use: { \"pdf\": { \"pageNumbers\": true, \"headerTemplate\": \" \", \"footerTemplate\": \" \" } }","title":"GitBook Cloud"},{"location":"env/gitbook/#gitlab","text":"First you have to setup the CI Pipeline to create and upload your pages. For GitBook this is done by placing the following .gitlab-ci.yml file in your projects root: # requiring the environment of NodeJS 8.9.x LTS (carbon) image: node:8.9 # add 'node_modules' to cache for speeding up builds cache: paths: - node_modules/ # Node modules and dependencies before_script: - npm install gitbook-cli -g # install gitbook - gitbook fetch latest # fetch latest stable version - gitbook install # add any requested plugins in book.json #- gitbook fetch pre # fetch latest pre-release version # the 'pages' job will deploy and build your site to the 'public' path pages: stage: deploy script: - gitbook build . public # build to public path - find public -type f -iregex '.*\\.\\(htm\\|html\\|txt\\|text\\|js\\|css\\)$' -execdir gzip -f --keep {} \\; # make compressed files artifacts: paths: - public expire_in: 1 week only: - master # this job will affect only the 'master' branch Push this to your master to add the pipelines. Now you should enable pages under Settings > Pages . So whenever you push new changes to your master the job will run and recreate your static pages.","title":"GitLab"},{"location":"env/gitbook/#writing-documentation","text":"All the pages are written as single files in the doc folder using Markdown language. It is nearly the same as used on GitHub. Attention: In contrast to the other markdown parsers there should be no space between the code tag and the language.","title":"Writing Documentation"},{"location":"env/gitbook/#plugins","text":"","title":"Plugins"},{"location":"env/gitbook/#layout","text":"To improve the layout of the book I use three different plugins: { \"plugins\": [\"toggle-chapters\", \"navigator\", \"downloadpdf\"] } This will open/close chapters like folders, display a page navigation on the right side and a link to download as PDF on the top line. To optimize the navigator output set the following in styles/ebook.css and styles/pdf.css to disable navigator: #anchors-navbar, #goTop { display: none; } And for styles/website.css add this to optimize display and remove for small display: #anchors-navbar { color: darkgray; right: 28px; top: 45px; } #goTop { display: none; } @media (max-width: 660px) { #anchors-navbar { display: none; } }","title":"Layout"},{"location":"env/gitbook/#todo","text":"As already used in GitHub markdown this plugin allows to write ToDo lists with check boxes which may be checked: { \"plugins\": [\"todo\"] } Now you may create a checklist using: [ ] Mercury [x] Venus [x] Earth (Orbit/Moon) [x] Mars [ ] Jupiter [ ] Saturn [ ] Uranus [ ] Neptune [ ] Comet Haley This is done using: - [ ] Mercury - [x] Venus - [x] Earth (Orbit/Moon) - [x] Mars - [ ] Jupiter - [ ] Saturn - [ ] Uranus - [ ] Neptune - [ ] Comet Haley","title":"ToDo"},{"location":"env/gitbook/#mermaid-graphs","text":"Using Mermaid it is possible to include easy flowcharts without a specific program. It is written as plaintext and converted into chart on display. To make this work the following plugin have to be defined: { \"plugins\": [\"mermaid-gb3\"] } As an example can be: ```mermaid graph TD; A-->B; A-->C; B-->D; C-->D; ```","title":"Mermaid Graphs"},{"location":"env/gitbook/#plantuml","text":"PlantUML is another format to make graphs out of text descriptions like mermaid. The plugin is loaded using: { \"plugins\": [\"puml\"] } And the diagram may be added as: {% plantuml %} Bob->Alice : hello {% endplantuml %}","title":"PlantUML"},{"location":"env/gitbook/#final-setup","text":"","title":"Final Setup"},{"location":"env/gitbook/#bookjson","text":"{ \"root\": \"./doc\", # only gor direct gitbook.com cloud hosting with enabled pdf \"pdf\": { \"pageNumbers\": true, \"headerTemplate\": \" \", \"footerTemplate\": \" \" }, \"plugins\": [ \"todo\", \"mermaid-gb3\", \"puml\", \"navigator\", \"collapsible-chapters\", \"hide-published-with\", \"insert-logo\" ], \"pluginsConfig\": { \"insert-logo\": { \"url\": \"https://alinex.gitlab.io/logo.png\", \"style\": \"background: none;\" }, # also only for pdf on github.com \"downloadpdf\": { \"base\": \"https://www.gitbook.com/download/pdf/book/alinex/nodejs\", \"label\": \"Download PDF\", \"multilingual\": false } } }","title":"book.json"},{"location":"env/gitbook/#further-files","text":"doc/README.md # Introduction doc/SUMMARY.md # Page index of book doc/cover.jpg # eBook cover doc/cover_small.jpg # small cover doc/styles/ebook.css # user style doc/styles/pdf.css # user style doc/styles/website.css # user style {!docs/abbreviations.txt!}","title":"Further files"},{"location":"env/github/","text":"GitHub GitHub adds a staging platform to share prototypes and open-source projects. GitHub is a free code hosting platform for version control and collaboration with open source. It lets you and others work together on projects from anywhere. On top of the Git repository management it supports further services like Git viewer Integrated CDN Issue reporting and management Wiki pages Static site Easy collaboration If you use GitHub there is no need to setup your own Git server anymore. It\u2019s surprisingly easy to get things set up and most tasks can be done directly by clicking in the web front end. You can also use a badge on your README.md like: [![GitHub watchers](https://img.shields.io/github/watchers/alinex/node-rest.svg?style=social&label=Watch&maxAge=86400)](https://github.com/alinex/node-rest/subscription) [![GitHub stars](https://img.shields.io/github/stars/alinex/node-rest.svg?style=social&label=Star&maxAge=86400)](https://github.com/alinex/node-rest) [![GitHub forks](https://img.shields.io/github/forks/alinex/node-rest.svg?style=social&label=Fork&maxAge=86400)](https://github.com/alinex/node-rest) [![GitHub issues](https://img.shields.io/github/issues/alinex/node-rest.svg?maxAge=86400)](https://github.com/alinex/node-rest/issues) GitHub Setup The use of GitHub itself has already been described so here we go on with some help on how to use GitHub with issues and planning. Create a new project Each repository can have multiple projects. You only need a Title and description to create a new one. After that it should be empty. To work within the project it is best to go to \"Full-screen\" mode (more like full window mode) with the button on the right side. Only in this mode scrolling right/left works. Add columns The project view comes in form of a Kanban board like often used in Agile Development and tools like Jira, Trello... Therefore you have to define your columns. A good starting point will be to name them. Simply: Waiting In Progress Done Agile: Backlog Ready In Progress Test QA Done Cards Within each column you can add cards and drag+drop them between the columns to show the current state. You have two possibilities of cards: Notes This are simple information or tasks which only consists of an title. They are only visible here and can be converted to issues as needed. Issues That are some real tasks with lots of possibilities: More details Possible discussion Pull Requests and more {!docs/abbreviations.txt!}","title":"GitHub"},{"location":"env/github/#github","text":"GitHub adds a staging platform to share prototypes and open-source projects. GitHub is a free code hosting platform for version control and collaboration with open source. It lets you and others work together on projects from anywhere. On top of the Git repository management it supports further services like Git viewer Integrated CDN Issue reporting and management Wiki pages Static site Easy collaboration If you use GitHub there is no need to setup your own Git server anymore. It\u2019s surprisingly easy to get things set up and most tasks can be done directly by clicking in the web front end. You can also use a badge on your README.md like: [![GitHub watchers](https://img.shields.io/github/watchers/alinex/node-rest.svg?style=social&label=Watch&maxAge=86400)](https://github.com/alinex/node-rest/subscription) [![GitHub stars](https://img.shields.io/github/stars/alinex/node-rest.svg?style=social&label=Star&maxAge=86400)](https://github.com/alinex/node-rest) [![GitHub forks](https://img.shields.io/github/forks/alinex/node-rest.svg?style=social&label=Fork&maxAge=86400)](https://github.com/alinex/node-rest) [![GitHub issues](https://img.shields.io/github/issues/alinex/node-rest.svg?maxAge=86400)](https://github.com/alinex/node-rest/issues)","title":"GitHub"},{"location":"env/github/#github-setup","text":"The use of GitHub itself has already been described so here we go on with some help on how to use GitHub with issues and planning.","title":"GitHub Setup"},{"location":"env/github/#create-a-new-project","text":"Each repository can have multiple projects. You only need a Title and description to create a new one. After that it should be empty. To work within the project it is best to go to \"Full-screen\" mode (more like full window mode) with the button on the right side. Only in this mode scrolling right/left works.","title":"Create a new project"},{"location":"env/github/#add-columns","text":"The project view comes in form of a Kanban board like often used in Agile Development and tools like Jira, Trello... Therefore you have to define your columns. A good starting point will be to name them. Simply: Waiting In Progress Done Agile: Backlog Ready In Progress Test QA Done","title":"Add columns"},{"location":"env/github/#cards","text":"Within each column you can add cards and drag+drop them between the columns to show the current state. You have two possibilities of cards:","title":"Cards"},{"location":"env/github/#notes","text":"This are simple information or tasks which only consists of an title. They are only visible here and can be converted to issues as needed.","title":"Notes"},{"location":"env/github/#issues","text":"That are some real tasks with lots of possibilities: More details Possible discussion Pull Requests and more {!docs/abbreviations.txt!}","title":"Issues"},{"location":"env/gitlab/","text":"GitLab As GitHub this is also web based environment around git repositories, Beside providing a centralized, cloud-based location where teams can store, share, publish, test, and collaborate on development projects using git. GitLab adds an web based administration and integration tools. In contrast to GitHub, GitLab is also install-able to be self hosted. Basics GitLab is a web-based repository manager that lets teams collaborate on code, duplicate code to safely create and edit new projects, then merge finished code into existing projects. GitLab is written in the Ruby programming language and includes a Wiki and issue-tracking features. It has different versions: GitLab Community Edition (CE), Enterprise Edition (EE), and a GitLab-hosted version, GitLab.com. It\u2019s got over 1400 contributors and is used by major organizations like Alibaba, NASA, CERN, and more. Its permissions, branch protection, and authentication features are what really make it stand out. Teams can secure projects on a more granular level, and projects are kept even safer while they\u2019re being worked on. Its special features include: It\u2019s free and open-source. Different hosting options: Self-hosted with the Core, Starter, Premium and Ultimate plans, and GitLab hosted SaaS options with the Free, Bronze, Silver and Gold plans. A convenient user interface enables users to access everything from one screen: projects, latest projects, users, latest users, groups, and stats. Settings allow users to control whether a repository is public or private. \u201cSnippet support\u201d lets users share small pieces of code from a project, without sharing the whole project. Protected branches are a new way to keep code safe. They allow users to set higher permissions on a project, so only certain people are able to push, force push, or delete code in a branch. Authentication levels take this security a step further, allowing users to give people access beyond a read/write level. For example, you can give a team member access to issue tracking without having to give them access to the code itself. Improved milestones enable you to set milestones at a group level, not just a developer-specific level. Developers can get insight into the whole team\u2019s scope and view the entire project\u2019s milestones, not just their own. With the \u201cWork in Progress\u201d status, developers can label a project WIP to let collaborators know that the code is unfinished. This prevents it from accidentally getting merged with other code before it\u2019s finished. You can attach files like comments to any communications in GitLab. Kubernetes cluster monitoring with the Ultimate, Silver, and Gold hosting plans Integration with Jira, Confluence, Trello, Jenkins and more. Impressions The best way is to just check it out but at first I will give some graphical impression of how to work with the GitLab Web-view. This is the start page and shows all your projects or projects you are a member of. The code view in GitLab is nearly the same a s known from GitHub. You can browse through the directory structure, show file contents, change branches, tags or use the version history... Your work can be planed and controlled using the integrated issue tracker with labels, assignees, milestones... Really great is also a simple planning board to organize the big list of issues better. Through CI/CD Pipelines you can define automatic tasks with specific triggers. Also a direct look into the pipeline output (necessary in case of errors) is possible and you can see the whole command line output. Access Levels Members of a group or project can have one of the following access levels: Guest to look into and create issues or add comments Reporter to manage and assign issues Developer to make commits and work with branches, merges Maintainer to add team members and manage CI/CD Owner to switch visibility and delete part or whole project In the default the master branch can also only be merged and pushed by the maintainer. But this may be changed under Settings -> Repository. External users can only access projects to which they are explicitly granted access, thus hiding all other internal or private ones from them. Access can be granted by adding the user as member to the project or group. Merge Requests Instead of merging using git on the console or in your UI tool, It can be also done through the graphical website. To do this the last changes have to be pushed to the origin server (GitLab). Open the project you want to merge to in GitLab Select \"New merge request\" on the right side Now select the source branch and target branch Click on the \"Compare branches and continue\" button and fill out the form Click on the \"Submit merge request\" button CI/CD Pipelines By giving a .gitlab-ci.yml configuration within the project this can be setup. The YAML file defines a set of jobs with constraints stating when they should be run. Jobs are picked up by Runners and executed within the environment of the Runner. What is important, is that each job is run independently from each other. Pages You can have a static site included in GitLab. This may be done using MkDocs or GitBook using the CI Pipeline to generate the pages. Summary This is a great tool which really helps in the complete process of software development and brings all the basics in one product. {!docs/abbreviations.txt!}","title":"GitLab"},{"location":"env/gitlab/#gitlab","text":"As GitHub this is also web based environment around git repositories, Beside providing a centralized, cloud-based location where teams can store, share, publish, test, and collaborate on development projects using git. GitLab adds an web based administration and integration tools. In contrast to GitHub, GitLab is also install-able to be self hosted.","title":"GitLab"},{"location":"env/gitlab/#basics","text":"GitLab is a web-based repository manager that lets teams collaborate on code, duplicate code to safely create and edit new projects, then merge finished code into existing projects. GitLab is written in the Ruby programming language and includes a Wiki and issue-tracking features. It has different versions: GitLab Community Edition (CE), Enterprise Edition (EE), and a GitLab-hosted version, GitLab.com. It\u2019s got over 1400 contributors and is used by major organizations like Alibaba, NASA, CERN, and more. Its permissions, branch protection, and authentication features are what really make it stand out. Teams can secure projects on a more granular level, and projects are kept even safer while they\u2019re being worked on. Its special features include: It\u2019s free and open-source. Different hosting options: Self-hosted with the Core, Starter, Premium and Ultimate plans, and GitLab hosted SaaS options with the Free, Bronze, Silver and Gold plans. A convenient user interface enables users to access everything from one screen: projects, latest projects, users, latest users, groups, and stats. Settings allow users to control whether a repository is public or private. \u201cSnippet support\u201d lets users share small pieces of code from a project, without sharing the whole project. Protected branches are a new way to keep code safe. They allow users to set higher permissions on a project, so only certain people are able to push, force push, or delete code in a branch. Authentication levels take this security a step further, allowing users to give people access beyond a read/write level. For example, you can give a team member access to issue tracking without having to give them access to the code itself. Improved milestones enable you to set milestones at a group level, not just a developer-specific level. Developers can get insight into the whole team\u2019s scope and view the entire project\u2019s milestones, not just their own. With the \u201cWork in Progress\u201d status, developers can label a project WIP to let collaborators know that the code is unfinished. This prevents it from accidentally getting merged with other code before it\u2019s finished. You can attach files like comments to any communications in GitLab. Kubernetes cluster monitoring with the Ultimate, Silver, and Gold hosting plans Integration with Jira, Confluence, Trello, Jenkins and more.","title":"Basics"},{"location":"env/gitlab/#impressions","text":"The best way is to just check it out but at first I will give some graphical impression of how to work with the GitLab Web-view. This is the start page and shows all your projects or projects you are a member of. The code view in GitLab is nearly the same a s known from GitHub. You can browse through the directory structure, show file contents, change branches, tags or use the version history... Your work can be planed and controlled using the integrated issue tracker with labels, assignees, milestones... Really great is also a simple planning board to organize the big list of issues better. Through CI/CD Pipelines you can define automatic tasks with specific triggers. Also a direct look into the pipeline output (necessary in case of errors) is possible and you can see the whole command line output.","title":"Impressions"},{"location":"env/gitlab/#access-levels","text":"Members of a group or project can have one of the following access levels: Guest to look into and create issues or add comments Reporter to manage and assign issues Developer to make commits and work with branches, merges Maintainer to add team members and manage CI/CD Owner to switch visibility and delete part or whole project In the default the master branch can also only be merged and pushed by the maintainer. But this may be changed under Settings -> Repository. External users can only access projects to which they are explicitly granted access, thus hiding all other internal or private ones from them. Access can be granted by adding the user as member to the project or group.","title":"Access Levels"},{"location":"env/gitlab/#merge-requests","text":"Instead of merging using git on the console or in your UI tool, It can be also done through the graphical website. To do this the last changes have to be pushed to the origin server (GitLab). Open the project you want to merge to in GitLab Select \"New merge request\" on the right side Now select the source branch and target branch Click on the \"Compare branches and continue\" button and fill out the form Click on the \"Submit merge request\" button","title":"Merge Requests"},{"location":"env/gitlab/#cicd-pipelines","text":"By giving a .gitlab-ci.yml configuration within the project this can be setup. The YAML file defines a set of jobs with constraints stating when they should be run. Jobs are picked up by Runners and executed within the environment of the Runner. What is important, is that each job is run independently from each other.","title":"CI/CD Pipelines"},{"location":"env/gitlab/#pages","text":"You can have a static site included in GitLab. This may be done using MkDocs or GitBook using the CI Pipeline to generate the pages.","title":"Pages"},{"location":"env/gitlab/#summary","text":"This is a great tool which really helps in the complete process of software development and brings all the basics in one product. {!docs/abbreviations.txt!}","title":"Summary"},{"location":"env/mkdocs/","text":"MkDocs As an alternative to GitBook, MkDocs is a fast and simple static site generator with template, plugin and extension support. Documentation source files are written also in Markdown, and configured with a single YAML configuration file. The following description use the material theme but others are possible, too. Material theme has responsive design and fluid layout for all kinds of screens and devices, designed to serve your project documentation in a user-friendly way in 34 languages with optimal readability. Some basic customization like primary and accent color, fonts... could be configured. Also a collection of useful extensions are included here, too. So this is not only a description of the basics but presenting you a fully usable and optimal setup of it. Install On Debian the following steps should be enough to get it locally running: sudo apt install build-essential python3-dev python3-pip python3-setuptools python3-wheel python3-cffi libcairo2 libpango-1.0-0 libpangocairo-1.0-0 libgdk-pixbuf2.0-0 libffi-dev shared-mime-info Take care that you use python 3! $ ll -al $(which python) lrwxrwxrwx 1 root root 18 M\u00e4r 8 10:03 /usr/bin/python -> /usr/bin/python3.6* If this points to python 2.7 change that first. Now you can install the python packages: python -m pip install --upgrade pip python -m pip install mkdocs python -m pip install mkdocs-material python -m pip install pymdown-extensions python -m pip install markdown-blockdiag python -m pip install markdown-include python -m pip install mkdocs-pdf-export-plugin To make it accessible in path, add the following to ~/.bashrc : PATH=$PATH:~/.local/bin And for the epub conversion you need to have calibre installed: curl -sL https://download.calibre-ebook.com/linux-installer.sh | sudo -E bash - Problems mkdocs could not be installed If the above wont install mkdocs try to install some tools first: sudo apt-get install python-setuptools python -m pip install wheel After that retry to install mkdocs and it's extensions. Problem with cairocffi Maybe your cairocffi version is not matching and you get some errors like Requirement.parse('cairocffi>=0.9.0'), {'weasyprint'}) , then you can check your version like: $ python -m pip show cairocffi Name: cairocffi Version: 0.9.0 ... To install a specific version use: python -m pip uninstall cairocffi python -m pip install cairocffi==1.0.1 Preview Server While you are working on the documentation and create new stuff it is often necessary to immediately see how it looks like. This is possible if you start an development server of mkdocs using: mkdocs serve # from within the project home This will start an development server which automatically reloads on changes. Build Documentation To create the documentation in the site sub folder use: mkdocs build And to also create a PDF use: ENABLE_PDF_EXPORT=1 mkdocs build In the setup below the PDF will be stored under site/alinex-book.pdf . Configuration The setup is completely done in a mkdocs.yml file within your project's root directory. First some descriptive information for the site: site_name: Alinex Development Guide site_description: A book to learn modern web technologies. site_author: Alexander Schilling copyright: Copyright &copy; 2016 - 2019 <a href=\"https://alinex.de\">Alexander Schilling</a> While the site_name is used as heading the site_description and site_author goes into the meta data. And the copyright line will be displayed in the footer with optional HTML links as seen above. The navigation may be auto detected or defined using a navigation structure: nav: - Home: - README.md - alinex.md - Languages: - Overview: lang/README.md - Markdown: lang/markdown.m - Handlebars: lang/handlebars.md Chapters can not contain a direct page. A title can be given for each page. If not the title setting at the top of each page is used or the first heading. Now the theme definition, here we use the material theme as a basis: theme: name: material icon: logo: material/book-open-variant favicon: assets/favicon.ico language: en palette: primary: grey accent: orange font: text: Roboto code: Roboto Mono feature: tabs: true The logo can be a name from the material icons (displayed on the top left beside the page heading). The favicon has to be set to an image within the docs folder. If feature/tabs is set the first level of navigation is put at tabs on the top. repo_name: 'alinex/alinex.gitlab.io' repo_url: 'https://gitlab.com/alinex/alinex.gitlab.io' edit_uri: '' {: .border} Like shown in the image the repository will be displayed on the right and if no edit_uri: \"\" is set an icon to edit the page source is added, too. To prevent this in the example config edit_url is set to an empty string. extra: social: - icon: material/gitlab link: https://gitlab.com/alinex - icon: material/github link: https://github.com/alinex - icon: material/home link: https://alinex.de {: .border} The social links use the FontAwesome names as type with a link. They will be displayed at the bottom right corner of the page. extra_css: - assets/extra.css With the extra_css section you may add more stylesheets to the generated HTML which are used to: optimize the theme to be used with attributes Such an CSS file may look like: {!docs/assets/extra.css!} And at last some plugins and extensions for more markdown possibilities like described below: plugins: - search # Enable for PDF export #PDF# - pdf-export: #PDF# verbose: false #PDF# media_type: print #PDF# combined: true #PDF# combined_output_path: alinex-book.pdf markdown_extensions: - extras - toc: permalink: true - pymdownx.caret - pymdownx.tilde - pymdownx.mark - admonition - pymdownx.details - codehilite: guess_lang: false linenums: false - pymdownx.inlinehilite - pymdownx.tabbed - pymdownx.superfences - pymdownx.betterem: smart_enable: all - pymdownx.emoji: emoji_generator: !!python/name:pymdownx.emoji.to_svg - pymdownx.keys - pymdownx.smartsymbols - pymdownx.tasklist: custom_checkbox: true - markdown_blockdiag: format: svg - markdown_include.include See the complete setup of this book . The PDF plugin is marked with special comments to be enabled by a short script if needed only. See below. If VS Code with the Prettier plugin is used, set the tab width to 4 spaces for correct Markdown formatting in MkDocs. PDF and ePub Export The already installed and included pdf-export plugin together with calibre will do both for you. Run the ./mkdocs-pdf.sh script which will: make a copy of mkdocs.yml enable the #PDF# lines build documentation with included PDF convert PDF to ePub cleanup Writing Documentation Pages are written in markdown Format and stored as *.md files within the doc folder. The Markdown implementation is nearly the same as used on GitHub but with some additions: Navigation title To set an alternative title within the navigation define it at YAML at the top: title: Test Page # Test ... Simple Formats Structure the text using headings: # Heading 1 ## Heading 2 ### Heading 3 Blockquotes are done using > signs: > This is a block quote in which only the first line of the paragraph needs > to be indented by `>` but it can also be done on each line. This is a block quote in which only the first line of the paragraph needs to be indented by > but it can also be done on each line. A horizontal line may be used to separate: --- Use some inline text formatting: Format Example Result Italic _Italic_ or *Italic* Italic Bold __Bold__ or **Bold** Bold Superscript H^2^O H^2^O Subscript CH~3~CH~2~OH CH~3~CH~2~OH Insert ^^Insert^^ ^^Insert^^ Delete ~~Delete me~~ ~~Delete me~~ Mark ==mark me== ==mark me== Emoji :smile: :smile: Code `echo \"Hello\"` echo \"Hello\" Code + Highlighting `:::bash echo \"Hello\"` :::bash echo \"Hello\" Keys ++ctrl+alt+h++ ++ctrl+alt+h++ Link [Text](http://my-site.com) Text Image ![Logo](../logo.png) Image (Zoom) ![Logo](../logo.png){.tiny} {.tiny} Lists Unordered list: - water - cola - beer water cola beer Ordered list: 1. select 2. take 3. buy select take buy And finally a task list is also possible: - [x] item 1 - [x] item A - [ ] item B more text - [x] item a - [ ] item b - [x] item c - [x] item C - [ ] item 2 - [ ] item 3 [x] item 1 [x] item A [ ] item B more text [x] item a [ ] item b [x] item c [x] item C [ ] item 2 [ ] item 3 Tables Tables can have alignment: | Left | Center | Right | | :--- | :----: | ----: | | one | two | three | | 1 | 2 | 3 | Left Center Right one two three 1 2 3 Block Messages The Admonition extension configured above will allow to add text blocks. !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. The different types will look like: !!! note Type: `note`, `seealso` !!! abstract Type: `abstract`, `summary`, `tldr` !!! info Type: `info`, `todo` !!! tip Type: `tip`, `hint`, `important` !!! success Type: `success`, `check`, `done` !!! question Type: `question`, `help`, `faq` !!! warning Type: `warning`, `caution`, `attention` !!! failure Type: `failure`, `fail`, `missing` !!! danger Type: `danger`, `error` !!! bug Type: `bug` !!! example Type: `example`, `snippet` !!! quote Type: `quote`, `cite` The title can also be specified in double quotes behind the type or remove the title by setting \"\" for title: !!! note \"Individual\" !!! note \"Individual\" The PyMarkdown.Details extension gives the same but as collapsible boxes: ??? note \"Initially closed\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. ???+ note \"Initially opened\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. ??? note \"Initially closed\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. ???+ note \"Initially opened\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Tabs Using Tabbed groups as tabs can be defined: === \"Bash\" ```bash #!/bin/bash echo \"Hello world!\" ``` === \"Explanation\" This is only a short example of how to make tabs. === \"Bash\" ```bash #!/bin/bash echo \"Hello world!\" ``` === \"Explanation\" This is only a short example of how to make tabs. Code Blocks CodeHilite will help you in display code elements. You have multiple options to specify the language of a code block: ```sql SELECT count(*) FROM my_table; ``` SELECT count(*) FROM my_table; Or by indention and using a shebang: #!/bin/bash grep $1 $2 #!/bin/bash grep $1 $2 Or by indention and three colon start line: :::bash grep $1 $2 :::bash grep $1 $2 Using SuperFences specific lines can be highlighted by passing the line numbers to the hl_lines argument placed right after the language identifier. ```python hl_lines=\"3 4\" \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] Diagrams Using the blockdiag module graphics can be aut generated out of text representations. Simple Diagram: Name the blocks and use arrows between them: blockdiag { // simple graph A -> B -> C -> D; A -> E -> F -> G; } Comments are also possible like shown in line 2. This will result in: blockdiag { // simple graph A -> B -> C -> D; A -> E -> F -> G; } Style: You can style nodes and edges: blockdiag { A [label = \"foo\"]; B [style = dotted]; C [style = dashed]; D [color = pink]; E [color = \"#888888\", textcolor=\"#FFFFFF\"]; // Set labels to edges. (short text only) A -> B [label = \"click\", textcolor=\"red\"]; B -> C [style = dotted]; C -> D [style = dashed]; D -> E [color = \"red\"]; // Set numbered-badge to nodes. F [numbered = 99]; G [label = \"\", background = \"http://blockdiag.com/en/_static/python-logo.gif\"]; H [thick]; // Set arrow direction to edges. A -> F [dir = none]; F -> G [dir = forward]; B -> G [dir = back]; G -> H [dir = both]; C -> H [thick] } blockdiag { A [label = \"foo\"]; B [style = dotted]; C [style = dashed]; D [color = pink]; E [color = \"#888888\", textcolor=\"#FFFFFF\"]; // Set labels to edges. (short text only) A -> B [label = \"click\", textcolor=\"red\"]; B -> C [style = dotted]; C -> D [style = dashed]; D -> E [color = \"red\"]; // Set numbered-badge to nodes. F [numbered = 99]; G [label = \"\", background = \"http://blockdiag.com/en/_static/python-logo.gif\"]; // Set arrow direction to edges. A -> F [dir = none]; F -> G [dir = forward]; B -> G [dir = back]; G -> H [dir = both]; C -> H [thick] } Branches and Direction: blockdiag { // branching edges to multiple children A -> B, C; D -> E <- F -- G <-> H; } blockdiag { // branching edges to multiple children A -> B, C; D -> E <- F -- G <-> H; } Folding: blockdiag { A -> B -> C -> D -> E; // fold edge at C to D (D will be layouted at top level; left side) C -> D [folded]; } blockdiag { A -> B -> C -> D -> E; // fold edge at C to D (D will be layouted at top level; left side) C -> D [folded]; } Shapes: blockdiag { // standard node shapes box [shape = box]; square [shape = square]; roundedbox [shape = roundedbox]; dots [shape = dots]; circle [shape = circle]; ellipse [shape = ellipse]; diamond [shape = diamond]; minidiamond [shape = minidiamond]; note [shape = note]; mail [shape = mail]; cloud [shape = cloud]; actor [shape = actor]; beginpoint [shape = beginpoint]; endpoint [shape = endpoint]; box -> square -> roundedbox -> dots; circle -> ellipse -> diamond -> minidiamond; note -> mail -> cloud -> actor; beginpoint -> endpoint; // node shapes for flowcharts condition [shape = flowchart.condition]; database [shape = flowchart.database]; terminator [shape = flowchart.terminator]; input [shape = flowchart.input]; loopin [shape = flowchart.loopin]; loopout [shape = flowchart.loopout]; condition -> database -> terminator -> input; loopin -> loopout; // Set stacked to nodes. stacked [stacked]; diamond [shape = \"diamond\", stacked]; database [shape = \"flowchart.database\", stacked]; stacked -> diamond -> database; } blockdiag { // standard node shapes box [shape = box]; square [shape = square]; roundedbox [shape = roundedbox]; dots [shape = dots]; circle [shape = circle]; ellipse [shape = ellipse]; diamond [shape = diamond]; minidiamond [shape = minidiamond]; note [shape = note]; mail [shape = mail]; cloud [shape = cloud]; actor [shape = actor]; beginpoint [shape = beginpoint]; endpoint [shape = endpoint]; box -> square -> roundedbox -> dots; circle -> ellipse -> diamond -> minidiamond; note -> mail -> cloud -> actor; beginpoint -> endpoint; // node shapes for flowcharts condition [shape = flowchart.condition]; database [shape = flowchart.database]; terminator [shape = flowchart.terminator]; input [shape = flowchart.input]; loopin [shape = flowchart.loopin]; loopout [shape = flowchart.loopout]; condition -> database -> terminator -> input; loopin -> loopout; // Set stacked to nodes. stacked [stacked]; diamond [shape = \"diamond\", stacked]; database [shape = \"flowchart.database\", stacked]; stacked -> diamond -> database; } Groups: Sorry, could not get this working in mkdocs till now. Classes: blockdiag { // Define class (list of attributes) class emphasis [color = pink, style = dashed]; class redline [color = red, style = dotted]; A -> B -> C; // Set class to node A [class = \"emphasis\"]; // Set class to edge A -> B [class = \"redline\"]; } blockdiag { // Define class (list of attributes) class emphasis [color = pink, style = dashed]; class redline [color = red, style = dotted]; A -> B -> C; // Set class to node A [class = \"emphasis\"]; // Set class to edge A -> B [class = \"redline\"]; } Portrait mode: blockdiag { orientation = portrait A -> B -> C; B -> D; } blockdiag { orientation = portrait A -> B -> C; B -> D; } Abbreviations Abbreviations are defined as an extra paragraph mostly at the end of the document: The HTML specification is maintained by the W3C. _[HTML]: Hyper Text Markup Language _[W3C]: World Wide Web Consortium This will be rendered with a tooltip on each occurrence of this words: The HTML specification is maintained by the W3C. [HTML]: Hyper Text Markup Language [W3C]: World Wide Web Consortium Combined with the later described includes it is also possible to place all abbreviations in a single file and include it at the end of each page. Footnotes The Footnote syntax follows the generally accepted syntax of the Markdown community. You add the footnote using [^1] with the correct number and define the content of this footnote later: Footnotes[^1] have a label[^@#$%] and the footnote's content. [^1]: This is a footnote content. [^@#$%]: A footnote on the label: \"@#\\$%\". This will look like: Footnotes[^1] have a label[^@#$%] and the footnote's content. [^1]: This is a footnote content. [^@#$%]: A footnote on the label: \"@#\\$%\". A footnote label must start with a caret ^ and may contain any inline text (including spaces) between a set of square brackets [] . Only the first caret has any special meaning. A footnote content must start with the label followed by a colon and at least one space. The label used to define the content must exactly match the label used in the body (including capitalization and white space). The content would then follow the label either on the same line or on the next line. The content may contain multiple lines, paragraphs, code blocks, blockquotes and most any other markdown syntax. The additional lines must be indented one level (four spaces or one tab). [^1]: The first paragraph of the definition. Paragraph two of the definition. > A blockquote with > multiple lines. a code block A final paragraph. Include It is possible to include other markdown with a simple statement: { !.gitignore!} No space after first curly brace to work! This statement will be replaced by the contents of the given file. The include extension will work recursively, so any included files within will also be included. This replacement is done prior to any other Markdown processing, so any Markdown syntax that you want can be used within your included files. The file path is relative to the project base (there mkdocs is been executed). Attributes Using attributes it is possible to set the various HTML element's attributes for output. An example attribute list might look like this: {: #someid .someclass somekey='some value' } This shows the possible definitions: A word which starts with # will set the id of an element. A word which starts with . will be added to the list of classes assigned to an element. A key/value pair somekey='some value' will assign that pair to the element. This can be set on block level elements if defined on the last line of the block by itself. This is a paragraph. {: #an_id .a_class } The one exception is headers, as they are only ever allowed on one line. So there you neet to write: ### A hash style header ### {: #hash } If used on inline elements the attributes are defined immediately after the element without any separation: [link](http://example.com){: class=\"foo bar\" title=\"Some title!\" } If classes are used you may define them in an additional CSS file. GitLab CI To get the documentation onto GitLab pages use the following configuration: {!.gitlab-ci.yml!} !!! warning The PDF generation is disabled here at the moment, because it will run in memory problems on gitlab.com. Further reading MkDocs Homepage Markdown Syntax GitHub Markdown {!docs/abbreviations.txt!}","title":"MkDocs"},{"location":"env/mkdocs/#mkdocs","text":"As an alternative to GitBook, MkDocs is a fast and simple static site generator with template, plugin and extension support. Documentation source files are written also in Markdown, and configured with a single YAML configuration file. The following description use the material theme but others are possible, too. Material theme has responsive design and fluid layout for all kinds of screens and devices, designed to serve your project documentation in a user-friendly way in 34 languages with optimal readability. Some basic customization like primary and accent color, fonts... could be configured. Also a collection of useful extensions are included here, too. So this is not only a description of the basics but presenting you a fully usable and optimal setup of it.","title":"MkDocs"},{"location":"env/mkdocs/#install","text":"On Debian the following steps should be enough to get it locally running: sudo apt install build-essential python3-dev python3-pip python3-setuptools python3-wheel python3-cffi libcairo2 libpango-1.0-0 libpangocairo-1.0-0 libgdk-pixbuf2.0-0 libffi-dev shared-mime-info Take care that you use python 3! $ ll -al $(which python) lrwxrwxrwx 1 root root 18 M\u00e4r 8 10:03 /usr/bin/python -> /usr/bin/python3.6* If this points to python 2.7 change that first. Now you can install the python packages: python -m pip install --upgrade pip python -m pip install mkdocs python -m pip install mkdocs-material python -m pip install pymdown-extensions python -m pip install markdown-blockdiag python -m pip install markdown-include python -m pip install mkdocs-pdf-export-plugin To make it accessible in path, add the following to ~/.bashrc : PATH=$PATH:~/.local/bin And for the epub conversion you need to have calibre installed: curl -sL https://download.calibre-ebook.com/linux-installer.sh | sudo -E bash -","title":"Install"},{"location":"env/mkdocs/#problems","text":"","title":"Problems"},{"location":"env/mkdocs/#mkdocs-could-not-be-installed","text":"If the above wont install mkdocs try to install some tools first: sudo apt-get install python-setuptools python -m pip install wheel After that retry to install mkdocs and it's extensions.","title":"mkdocs could not be installed"},{"location":"env/mkdocs/#problem-with-cairocffi","text":"Maybe your cairocffi version is not matching and you get some errors like Requirement.parse('cairocffi>=0.9.0'), {'weasyprint'}) , then you can check your version like: $ python -m pip show cairocffi Name: cairocffi Version: 0.9.0 ... To install a specific version use: python -m pip uninstall cairocffi python -m pip install cairocffi==1.0.1","title":"Problem with cairocffi"},{"location":"env/mkdocs/#preview-server","text":"While you are working on the documentation and create new stuff it is often necessary to immediately see how it looks like. This is possible if you start an development server of mkdocs using: mkdocs serve # from within the project home This will start an development server which automatically reloads on changes.","title":"Preview Server"},{"location":"env/mkdocs/#build-documentation","text":"To create the documentation in the site sub folder use: mkdocs build And to also create a PDF use: ENABLE_PDF_EXPORT=1 mkdocs build In the setup below the PDF will be stored under site/alinex-book.pdf .","title":"Build Documentation"},{"location":"env/mkdocs/#configuration","text":"The setup is completely done in a mkdocs.yml file within your project's root directory. First some descriptive information for the site: site_name: Alinex Development Guide site_description: A book to learn modern web technologies. site_author: Alexander Schilling copyright: Copyright &copy; 2016 - 2019 <a href=\"https://alinex.de\">Alexander Schilling</a> While the site_name is used as heading the site_description and site_author goes into the meta data. And the copyright line will be displayed in the footer with optional HTML links as seen above. The navigation may be auto detected or defined using a navigation structure: nav: - Home: - README.md - alinex.md - Languages: - Overview: lang/README.md - Markdown: lang/markdown.m - Handlebars: lang/handlebars.md Chapters can not contain a direct page. A title can be given for each page. If not the title setting at the top of each page is used or the first heading. Now the theme definition, here we use the material theme as a basis: theme: name: material icon: logo: material/book-open-variant favicon: assets/favicon.ico language: en palette: primary: grey accent: orange font: text: Roboto code: Roboto Mono feature: tabs: true The logo can be a name from the material icons (displayed on the top left beside the page heading). The favicon has to be set to an image within the docs folder. If feature/tabs is set the first level of navigation is put at tabs on the top. repo_name: 'alinex/alinex.gitlab.io' repo_url: 'https://gitlab.com/alinex/alinex.gitlab.io' edit_uri: '' {: .border} Like shown in the image the repository will be displayed on the right and if no edit_uri: \"\" is set an icon to edit the page source is added, too. To prevent this in the example config edit_url is set to an empty string. extra: social: - icon: material/gitlab link: https://gitlab.com/alinex - icon: material/github link: https://github.com/alinex - icon: material/home link: https://alinex.de {: .border} The social links use the FontAwesome names as type with a link. They will be displayed at the bottom right corner of the page. extra_css: - assets/extra.css With the extra_css section you may add more stylesheets to the generated HTML which are used to: optimize the theme to be used with attributes Such an CSS file may look like: {!docs/assets/extra.css!} And at last some plugins and extensions for more markdown possibilities like described below: plugins: - search # Enable for PDF export #PDF# - pdf-export: #PDF# verbose: false #PDF# media_type: print #PDF# combined: true #PDF# combined_output_path: alinex-book.pdf markdown_extensions: - extras - toc: permalink: true - pymdownx.caret - pymdownx.tilde - pymdownx.mark - admonition - pymdownx.details - codehilite: guess_lang: false linenums: false - pymdownx.inlinehilite - pymdownx.tabbed - pymdownx.superfences - pymdownx.betterem: smart_enable: all - pymdownx.emoji: emoji_generator: !!python/name:pymdownx.emoji.to_svg - pymdownx.keys - pymdownx.smartsymbols - pymdownx.tasklist: custom_checkbox: true - markdown_blockdiag: format: svg - markdown_include.include See the complete setup of this book . The PDF plugin is marked with special comments to be enabled by a short script if needed only. See below. If VS Code with the Prettier plugin is used, set the tab width to 4 spaces for correct Markdown formatting in MkDocs.","title":"Configuration"},{"location":"env/mkdocs/#pdf-and-epub-export","text":"The already installed and included pdf-export plugin together with calibre will do both for you. Run the ./mkdocs-pdf.sh script which will: make a copy of mkdocs.yml enable the #PDF# lines build documentation with included PDF convert PDF to ePub cleanup","title":"PDF and ePub Export"},{"location":"env/mkdocs/#writing-documentation","text":"Pages are written in markdown Format and stored as *.md files within the doc folder. The Markdown implementation is nearly the same as used on GitHub but with some additions:","title":"Writing Documentation"},{"location":"env/mkdocs/#navigation-title","text":"To set an alternative title within the navigation define it at YAML at the top: title: Test Page # Test ...","title":"Navigation title"},{"location":"env/mkdocs/#simple-formats","text":"Structure the text using headings: # Heading 1 ## Heading 2 ### Heading 3 Blockquotes are done using > signs: > This is a block quote in which only the first line of the paragraph needs > to be indented by `>` but it can also be done on each line. This is a block quote in which only the first line of the paragraph needs to be indented by > but it can also be done on each line. A horizontal line may be used to separate: --- Use some inline text formatting: Format Example Result Italic _Italic_ or *Italic* Italic Bold __Bold__ or **Bold** Bold Superscript H^2^O H^2^O Subscript CH~3~CH~2~OH CH~3~CH~2~OH Insert ^^Insert^^ ^^Insert^^ Delete ~~Delete me~~ ~~Delete me~~ Mark ==mark me== ==mark me== Emoji :smile: :smile: Code `echo \"Hello\"` echo \"Hello\" Code + Highlighting `:::bash echo \"Hello\"` :::bash echo \"Hello\" Keys ++ctrl+alt+h++ ++ctrl+alt+h++ Link [Text](http://my-site.com) Text Image ![Logo](../logo.png) Image (Zoom) ![Logo](../logo.png){.tiny} {.tiny}","title":"Simple Formats"},{"location":"env/mkdocs/#lists","text":"Unordered list: - water - cola - beer water cola beer Ordered list: 1. select 2. take 3. buy select take buy And finally a task list is also possible: - [x] item 1 - [x] item A - [ ] item B more text - [x] item a - [ ] item b - [x] item c - [x] item C - [ ] item 2 - [ ] item 3 [x] item 1 [x] item A [ ] item B more text [x] item a [ ] item b [x] item c [x] item C [ ] item 2 [ ] item 3","title":"Lists"},{"location":"env/mkdocs/#tables","text":"Tables can have alignment: | Left | Center | Right | | :--- | :----: | ----: | | one | two | three | | 1 | 2 | 3 | Left Center Right one two three 1 2 3","title":"Tables"},{"location":"env/mkdocs/#block-messages","text":"The Admonition extension configured above will allow to add text blocks. !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. The different types will look like: !!! note Type: `note`, `seealso` !!! abstract Type: `abstract`, `summary`, `tldr` !!! info Type: `info`, `todo` !!! tip Type: `tip`, `hint`, `important` !!! success Type: `success`, `check`, `done` !!! question Type: `question`, `help`, `faq` !!! warning Type: `warning`, `caution`, `attention` !!! failure Type: `failure`, `fail`, `missing` !!! danger Type: `danger`, `error` !!! bug Type: `bug` !!! example Type: `example`, `snippet` !!! quote Type: `quote`, `cite` The title can also be specified in double quotes behind the type or remove the title by setting \"\" for title: !!! note \"Individual\" !!! note \"Individual\" The PyMarkdown.Details extension gives the same but as collapsible boxes: ??? note \"Initially closed\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. ???+ note \"Initially opened\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. ??? note \"Initially closed\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. ???+ note \"Initially opened\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Block Messages"},{"location":"env/mkdocs/#tabs","text":"Using Tabbed groups as tabs can be defined: === \"Bash\" ```bash #!/bin/bash echo \"Hello world!\" ``` === \"Explanation\" This is only a short example of how to make tabs. === \"Bash\" ```bash #!/bin/bash echo \"Hello world!\" ``` === \"Explanation\" This is only a short example of how to make tabs.","title":"Tabs"},{"location":"env/mkdocs/#code-blocks","text":"CodeHilite will help you in display code elements. You have multiple options to specify the language of a code block: ```sql SELECT count(*) FROM my_table; ``` SELECT count(*) FROM my_table; Or by indention and using a shebang: #!/bin/bash grep $1 $2 #!/bin/bash grep $1 $2 Or by indention and three colon start line: :::bash grep $1 $2 :::bash grep $1 $2 Using SuperFences specific lines can be highlighted by passing the line numbers to the hl_lines argument placed right after the language identifier. ```python hl_lines=\"3 4\" \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j]","title":"Code Blocks"},{"location":"env/mkdocs/#diagrams","text":"Using the blockdiag module graphics can be aut generated out of text representations. Simple Diagram: Name the blocks and use arrows between them: blockdiag { // simple graph A -> B -> C -> D; A -> E -> F -> G; } Comments are also possible like shown in line 2. This will result in: blockdiag { // simple graph A -> B -> C -> D; A -> E -> F -> G; } Style: You can style nodes and edges: blockdiag { A [label = \"foo\"]; B [style = dotted]; C [style = dashed]; D [color = pink]; E [color = \"#888888\", textcolor=\"#FFFFFF\"]; // Set labels to edges. (short text only) A -> B [label = \"click\", textcolor=\"red\"]; B -> C [style = dotted]; C -> D [style = dashed]; D -> E [color = \"red\"]; // Set numbered-badge to nodes. F [numbered = 99]; G [label = \"\", background = \"http://blockdiag.com/en/_static/python-logo.gif\"]; H [thick]; // Set arrow direction to edges. A -> F [dir = none]; F -> G [dir = forward]; B -> G [dir = back]; G -> H [dir = both]; C -> H [thick] } blockdiag { A [label = \"foo\"]; B [style = dotted]; C [style = dashed]; D [color = pink]; E [color = \"#888888\", textcolor=\"#FFFFFF\"]; // Set labels to edges. (short text only) A -> B [label = \"click\", textcolor=\"red\"]; B -> C [style = dotted]; C -> D [style = dashed]; D -> E [color = \"red\"]; // Set numbered-badge to nodes. F [numbered = 99]; G [label = \"\", background = \"http://blockdiag.com/en/_static/python-logo.gif\"]; // Set arrow direction to edges. A -> F [dir = none]; F -> G [dir = forward]; B -> G [dir = back]; G -> H [dir = both]; C -> H [thick] } Branches and Direction: blockdiag { // branching edges to multiple children A -> B, C; D -> E <- F -- G <-> H; } blockdiag { // branching edges to multiple children A -> B, C; D -> E <- F -- G <-> H; } Folding: blockdiag { A -> B -> C -> D -> E; // fold edge at C to D (D will be layouted at top level; left side) C -> D [folded]; } blockdiag { A -> B -> C -> D -> E; // fold edge at C to D (D will be layouted at top level; left side) C -> D [folded]; } Shapes: blockdiag { // standard node shapes box [shape = box]; square [shape = square]; roundedbox [shape = roundedbox]; dots [shape = dots]; circle [shape = circle]; ellipse [shape = ellipse]; diamond [shape = diamond]; minidiamond [shape = minidiamond]; note [shape = note]; mail [shape = mail]; cloud [shape = cloud]; actor [shape = actor]; beginpoint [shape = beginpoint]; endpoint [shape = endpoint]; box -> square -> roundedbox -> dots; circle -> ellipse -> diamond -> minidiamond; note -> mail -> cloud -> actor; beginpoint -> endpoint; // node shapes for flowcharts condition [shape = flowchart.condition]; database [shape = flowchart.database]; terminator [shape = flowchart.terminator]; input [shape = flowchart.input]; loopin [shape = flowchart.loopin]; loopout [shape = flowchart.loopout]; condition -> database -> terminator -> input; loopin -> loopout; // Set stacked to nodes. stacked [stacked]; diamond [shape = \"diamond\", stacked]; database [shape = \"flowchart.database\", stacked]; stacked -> diamond -> database; } blockdiag { // standard node shapes box [shape = box]; square [shape = square]; roundedbox [shape = roundedbox]; dots [shape = dots]; circle [shape = circle]; ellipse [shape = ellipse]; diamond [shape = diamond]; minidiamond [shape = minidiamond]; note [shape = note]; mail [shape = mail]; cloud [shape = cloud]; actor [shape = actor]; beginpoint [shape = beginpoint]; endpoint [shape = endpoint]; box -> square -> roundedbox -> dots; circle -> ellipse -> diamond -> minidiamond; note -> mail -> cloud -> actor; beginpoint -> endpoint; // node shapes for flowcharts condition [shape = flowchart.condition]; database [shape = flowchart.database]; terminator [shape = flowchart.terminator]; input [shape = flowchart.input]; loopin [shape = flowchart.loopin]; loopout [shape = flowchart.loopout]; condition -> database -> terminator -> input; loopin -> loopout; // Set stacked to nodes. stacked [stacked]; diamond [shape = \"diamond\", stacked]; database [shape = \"flowchart.database\", stacked]; stacked -> diamond -> database; } Groups: Sorry, could not get this working in mkdocs till now. Classes: blockdiag { // Define class (list of attributes) class emphasis [color = pink, style = dashed]; class redline [color = red, style = dotted]; A -> B -> C; // Set class to node A [class = \"emphasis\"]; // Set class to edge A -> B [class = \"redline\"]; } blockdiag { // Define class (list of attributes) class emphasis [color = pink, style = dashed]; class redline [color = red, style = dotted]; A -> B -> C; // Set class to node A [class = \"emphasis\"]; // Set class to edge A -> B [class = \"redline\"]; } Portrait mode: blockdiag { orientation = portrait A -> B -> C; B -> D; } blockdiag { orientation = portrait A -> B -> C; B -> D; }","title":"Diagrams"},{"location":"env/mkdocs/#abbreviations","text":"Abbreviations are defined as an extra paragraph mostly at the end of the document: The HTML specification is maintained by the W3C. _[HTML]: Hyper Text Markup Language _[W3C]: World Wide Web Consortium This will be rendered with a tooltip on each occurrence of this words: The HTML specification is maintained by the W3C. [HTML]: Hyper Text Markup Language [W3C]: World Wide Web Consortium Combined with the later described includes it is also possible to place all abbreviations in a single file and include it at the end of each page.","title":"Abbreviations"},{"location":"env/mkdocs/#footnotes","text":"The Footnote syntax follows the generally accepted syntax of the Markdown community. You add the footnote using [^1] with the correct number and define the content of this footnote later: Footnotes[^1] have a label[^@#$%] and the footnote's content. [^1]: This is a footnote content. [^@#$%]: A footnote on the label: \"@#\\$%\". This will look like: Footnotes[^1] have a label[^@#$%] and the footnote's content. [^1]: This is a footnote content. [^@#$%]: A footnote on the label: \"@#\\$%\". A footnote label must start with a caret ^ and may contain any inline text (including spaces) between a set of square brackets [] . Only the first caret has any special meaning. A footnote content must start with the label followed by a colon and at least one space. The label used to define the content must exactly match the label used in the body (including capitalization and white space). The content would then follow the label either on the same line or on the next line. The content may contain multiple lines, paragraphs, code blocks, blockquotes and most any other markdown syntax. The additional lines must be indented one level (four spaces or one tab). [^1]: The first paragraph of the definition. Paragraph two of the definition. > A blockquote with > multiple lines. a code block A final paragraph.","title":"Footnotes"},{"location":"env/mkdocs/#include","text":"It is possible to include other markdown with a simple statement: { !.gitignore!} No space after first curly brace to work! This statement will be replaced by the contents of the given file. The include extension will work recursively, so any included files within will also be included. This replacement is done prior to any other Markdown processing, so any Markdown syntax that you want can be used within your included files. The file path is relative to the project base (there mkdocs is been executed).","title":"Include"},{"location":"env/mkdocs/#attributes","text":"Using attributes it is possible to set the various HTML element's attributes for output. An example attribute list might look like this: {: #someid .someclass somekey='some value' } This shows the possible definitions: A word which starts with # will set the id of an element. A word which starts with . will be added to the list of classes assigned to an element. A key/value pair somekey='some value' will assign that pair to the element. This can be set on block level elements if defined on the last line of the block by itself. This is a paragraph. {: #an_id .a_class } The one exception is headers, as they are only ever allowed on one line. So there you neet to write: ### A hash style header ### {: #hash } If used on inline elements the attributes are defined immediately after the element without any separation: [link](http://example.com){: class=\"foo bar\" title=\"Some title!\" } If classes are used you may define them in an additional CSS file.","title":"Attributes"},{"location":"env/mkdocs/#gitlab-ci","text":"To get the documentation onto GitLab pages use the following configuration: {!.gitlab-ci.yml!} !!! warning The PDF generation is disabled here at the moment, because it will run in memory problems on gitlab.com.","title":"GitLab CI"},{"location":"env/mkdocs/#further-reading","text":"MkDocs Homepage Markdown Syntax GitHub Markdown {!docs/abbreviations.txt!}","title":"Further reading"},{"location":"env/trello/","text":"Trello Trello is a free online tool using a kanban board like visual display of cards. It's a general too which may be used for nearly everything but it is also very simple and reduced in features for coding. Each card has: title description comments check lists labels assignee due dates The planning lines are freely definable and cards are moved by drag and drop. A mobile app is also available to use. {!docs/abbreviations.txt!}","title":"Trello"},{"location":"env/trello/#trello","text":"Trello is a free online tool using a kanban board like visual display of cards. It's a general too which may be used for nearly everything but it is also very simple and reduced in features for coding. Each card has: title description comments check lists labels assignee due dates The planning lines are freely definable and cards are moved by drag and drop. A mobile app is also available to use. {!docs/abbreviations.txt!}","title":"Trello"},{"location":"env/vscode/","text":"VS Code Editor Microsoft Visual Studio Code is a free, open source code editor for developing and debugging. It runs on any OS and unlike large development environments (Microsoft Visual Studio, Eclipse...) it won't need a project setup before. VS Code uses the information it finds in the files and folders to provide project and platform-specific functions, such as matching auto complete. VS Code supports a variety of programming, markup, and database languages, from JavaScript, HTML, and CSS, through C #, C ++, and Python, to PHP and SQL, just to name a few. Common languages usually already included in the standard scope of the editor, other languages are available as an extension. In addition to pure syntax highlighting, auto completion is also provided for some languages, meaningfully supplementing user input: a pop-up displays previously declared variables, methods, functions, and objects that can be accessed in the program context, as well as extensive templates of commonly used pieces of code (templates and snippets). Installation On Debian you may install using the repository, to do the updates with your normal system updates: # add repository curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg sudo install -o root -g root -m 644 microsoft.gpg /etc/apt/trusted.gpg.d/ sudo sh -c 'echo \"deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\" > /etc/apt/sources.list.d/vscode.list' sudo apt-get install apt-transport-https # install VS Code sudo apt-get update sudo apt-get install code User Settings Visual Studio Code can be configured to your personal needs. The base for this is a JSON configuration file. Using the command Preferences: Open User Settings you can open a view with the default configuration on the left and your settings on the right side. You can only change the ones on the right side. But you can also use the configuration form which is reachable using the gear icon on the lower left or ++ctrl+,++. This dialog shows a tree structured form with some of the most essential settings for the core and the extensions. I used at least the following editor settings: { \"editor.formatOnSave\": true, \"editor.tabSize\": 2, \"editor.detectIndentation\": false, \"editor.multiCursorModifier\": \"ctrlCmd\" } The last one is necessary on my KDE based Linux Distribution because ++alt+left-button++ is already used by the os. Workspace or Project Settings You can also add a settings.json within the /.vscode folder in your project which will overwrite the user settings. This is a convenient way to set project specific settings which are needed for every user. User Interface The user interface of Visual Studio Code is divided into directory tree, editor and console. In VS code, multiple editor windows can be opened next to each other thanks to split or side-by-side editing. Multiple selection allows you to interactively change multiple lines at the same time, for example, when renaming variables. Code navigation enables targeted searches for specific codes, such as methods or symbols. The UI pattern is similar to other editors such as Sublime Text and Atom: on the left is a file manager that displays all the files and folders in the current directory. Right next to it is the actual editor. There is space for various panels, such as a terminal or a log for errors and warnings. The user interface can be flexibly adapted to your own needs. If you need to do something always consider to lookup for included commands using ++f1++ and search for them. Commands Use ++f1++ or the key combination ++ctrl+shift+p++ to open the command palette. The editor can be operated with keyboard commands via them so that you can do without the mouse almost completely. The command palette is an input box that displays a list of all available commands; it is therefore not necessary to remember the shortcuts of all commands. This list can be filtered by entering the command you are looking for and using the arrow keys to search. With Enter a command is executed. The command palette of VS Code knows different modes: ++greater++ default command mode commands are executed as usual. ++\"#\"++ and ++\"@\"++ to search for symbols such as methods and variables in the current workspace or file. ++colon++ jumps to the given line number ++ctrl+p++ search for open files by typing their name. All files in the currently open folder are searched. The command palette of VS Code can do even more use ++\"?\"++ displays a list of all modes. Shortcuts All the shortcuts can be shown directly in the editor calling Preferences -> Keyboard Shortcuts and you can also search and change them in this table. Althought the most used ones will be shown here: General ++ctrl+shift+p++ or ++f1++ Show Command Palette ++ctrl+p++ Quick Open, Go to File... ++ctrl+shift+n++ New window/instance ++ctrl+w++ Close window/instance ++ctrl+\",\"++ Open User Settings ++ctrl+k++ ++ctrl+s++ Keyboard Shortcuts Basic editing ++ctrl+x++ Cut line (empty selection) ++ctrl+c++ Copy line (empty selection) ++alt+down++ / ++alt+up++ Move line down/up ++ctrl+shift+k++ Delete line ++ctrl+enter++ / ++ctrl+shift+enter++ Insert line below/ above ++ctrl+shift+\"\\\"++ Jump to matching bracket ++ctrl+j++ / ++ctrl+\"[\"++ Indent/Outdent line ++ctrl+home++ / ++ctrl+end++ Go to beginning/end of file ++ctrl+up++ / ++ctrl+down++ Scroll line up/down ++alt+page-up++ / ++alt+page-down++ Scroll page up/down ++ctrl+shift+\"[\"++ / ++ctrl+shift+\"]\"++ Fold/unfold region ++ctrl+k++ ++ctrl+\"[\"++ / ++ctrl+k++ ++ctrl+\"]\"++ Fold/unfold all subregions ++ctrl+k++ ++ctrl+0++ / ++ctrl+k++ ++ctrl+j++ Fold/Unfold all regions ++ctrl+k++ ++ctrl+c++ Add line comment ++ctrl+k++ ++ctrl+u++ Remove line comment ++ctrl+\"/\"++ Toggle line comment ++ctrl+shift+a++ Toggle block comment ++alt+z++ Toggle word wrap Rich languages editing ++ctrl+space++ Trigger suggestion ++ctrl+shift+space++ Trigger parameter hints ++ctrl+shift+i++ Format document ++ctrl+k++ ++ctrl+f++ Format selection ++f12++ Go to Definition ++ctrl+shift+F10 Peek Definition ++ctrl+k++ ++f12++ Open Definition to the side ++ctrl+\".\"++ Quick Fix ++shift+f12++ Show References ++f2++ Rename Symbol ++ctrl+k++ ++ctrl+x++ Trim trailing whitespace ++ctrl+k++ ++m++ Change file language Multi-cursor and selection ++ctrl+left-button++ Insert cursor (changed by multi cursor setting above) ++shift+alt+up++ / ++shift+alt+down++ Insert cursor above/below ++ctrl+u++ Undo last cursor operation ++shift+alt+i++ Insert cursor at end of each line selected ++ctrl+i++ Select current line ++ctrl+shift+l++ Select all occurrences of current selection ++ctrl+f2++ Select all occurrences of current word ++shift+alt+right++ Expand selection ++shift+alt+left++ Shrink selection ++shift+alt+\"drag mouse\"++ Column (box) selection Display ++f11++ Toggle full screen ++shift+alt+0++ Toggle editor layout (horizontal/vertical) ++ctrl+\"=\"++ / ++ctrl+\"-\"++ Zoom in/out ++ctrl+b++ Toggle Sidebar visibility ++ctrl+shift+e++ Show Explorer / Toggle focus ++ctrl+shift+f++ Show Search ++ctrl+shift+g++ Show Source Control ++ctrl+shift+d++ Show Debug ++ctrl+shift+x++ Show Extensions ++ctrl+shift+h++ Replace in files ++ctrl+shift+j++ Toggle Search details ++ctrl+shift+c++ Open new command prompt/terminal ++ctrl+k++ ++ctrl+h++ Show Output panel ++ctrl+shift+v++ Open Markdown preview ++ctrl+k++ ++v++ Open Markdown preview to the side ++ctrl+k++ ++z++ Zen Mode (Esc Esc to exit) Search and replace ++ctrl+f++ Find ++ctrl+h++ Replace ++f3++ / ++shift+f3++ Find next/previous ++alt+enter++ Select all occurrences of Find match ++ctrl+d++ Add selection to next Find match ++ctrl+k++ ++ctrl+d++ Move last selection to next Find match Navigation ++ctrl+t++ Show all Symbols ++ctrl+g++ Go to Line... ++ctrl+p++ Go to File... ++ctrl+shift+o++ Go to Symbol... ++ctrl+shift+m++ Show Problems panel ++f8++ Go to next error or warning ++shift+f8++ Go to previous error or warning ++ctrl+shift+tab++ Navigate editor group history ++ctrl+alt+\"-\"++ Go back ++ctrl+shift+\"-\"++ Go forward ++ctrl+m++ Toggle Tab moves focus Editor management ++ctrl+w++ Close editor ++ctrl+k++ ++f++ Close folder ++ctrl+\"\\\"++ Split editor ++ctrl+1++ / ++ctrl+2++ / ++ctrl+3++ Focus into 1st, 2nd, 3rd editor group ++ctrl+k++ ++ctrl+left++ Focus into previous editor group ++ctrl+k++ ++ctrl+right++ Focus into next editor group ++ctrl+shift+page-up++ Move editor left ++ctrl+shift+page-down++ Move editor right ++ctrl+k++ ++left++ Move active editor group left/up ++ctrl+k++ ++right++ Move active editor group right/down File management ++ctrl+n++ New File ++ctrl+o++ Open File... ++ctrl+s++ Save ++ctrl+shift+s++ Save As... ++ctrl+w++ Close ++ctrl+k++ ++ctrl+w++ Close All ++ctrl+shift+t++ Reopen closed editor ++ctrl+k++ ++Enter Keep preview mode editor open ++ctrl+tab++ Open next ++ctrl+shift+tab++ Open previous ++ctrl+k++ ++p++ Copy path of active file ++ctrl+k++ ++r++ Reveal active file in Explorer ++ctrl+k++ ++d++ Show active file in new window/instance Debug ++f9++ Toggle breakpoint ++f5++ Start / Continue ++f11++ / ++shift+f11++ Step into/out ++f10++ Step over ++shift+f5++ Stop ++ctrl+k++ ++ctrl+i++ Show hover Integrated terminal ++ctrl+\"`\"++ Show integrated terminal ++ctrl+shift+\"`\"++ Create new terminal ++ctrl+shift+c++ Copy selection ++ctrl+shift+v++ Paste into active terminal ++ctrl+shift+up++ / ++ctrl+shift+down++ Scroll up/down ++shift+page-up++ / ++shift+page-down++ Scroll page up/down ++shift+home++ / ++shift+end++ Scroll to top/bottom Tasks You can run different tasks which are possible to be run on the command line directly from within the VSCode IDE. To define custom tasks call Configure Tasks from the global Tasks menu and select the Create tasks.json file from template entry. Then select Other format. You will get a JSON configuration in the current workspace and can define your tasks there: { \"version\": \"2.0.0\", \"tasks\": [ { \"label\": \"build\", \"type\": \"shell\", \"command\": \"cargo build\", \"group\": { \"kind\": \"build\", \"isDefault\": true }, \"problemMatcher\": [] }, { \"label\": \"test\", \"type\": \"shell\", \"command\": \"cargo test\", \"group\": { \"kind\": \"test\", \"isDefault\": true } } ] } The task's properties have the following semantic: label - The task's label used in the user interface. type - The task's type. For a custom task, this can either be shell or process . If shell is specified, the command is interpreted as a shell command. If process is specified, the command is interpreted as a process to execute. command - The actual command to execute. args - Optionally array used if they are not given as a one liner within the command call windows - Any Windows OS specific properties. group - Defines to which group the task belongs. presentation - Defines how the task output is handled in the user interface. Call the tasks using ++f1++ with tasks . Read more at VSCode Tasks . Debugger The sleek and flexible Code Editor gives developers all the functionality they need to get the job done. This is mainly due to the good integration of the debugger and the direct Git connection. VS Code is perfect for all scripting languages. There are some enhancements in Marketplace for better handling... With the Chrome Debugger extension, front-end developers can debug their JavaScript code with Google Chrome. VS Code uses the Chrome DevTools Protocol, which associates the files loaded in the browser with the files opened in the editor. This allows the developer to place breakpoints directly into VS code, watch variables, or see the entire call stack as they debug - all without leaving the editor. A very useful extension that you should definitely look at as a front-end developer. Extensions The standard functionality of VS Code can be extended with additional packages that can adapt almost anything, from the appearance and behavior of the user interface to core editor functions. Microsoft's code editor brings with it a package manager. Additional languages, debuggers and tools can be easily installed. In the chapters below I comment on some plugins. To get more information search for the extension in the editor and read the detailed description. Shortcut to get to them is ++ctrl+shift+x++ or open it with the square icon on the left. The editor will also sometimes show you some recommended extensions based on the files you opened. Project Manager Project Manager add a new sidebar to easily switch between projects. After configuring each project you can easily select the project in the new sidebar which switches to this project and Explorer and Source Control will both hold this. Code Formatting Prettier enables VS Code to automatically optimize the format to the default style guides. A good way is to enable it on save in the settings. You can customize some parameters, I used to set the following to mainly have markdown which is fully compatible with MkDocs for markdown parsing (spesifically the tabWidth of 4 spaces is needed to use lists with multiline points). { \"prettier.printWidth\": 100, \"prettier.tabWidth\": 4, \"editor.formatOnSave\": true } Also auto formatting on save is enabled in the above. Spellchecker Use Spell Right as spell checker which is using hunspell dictionaries. To make it work install also the hunspell dictionaries like: sudo apt install hunspell hunspell-en-us hunspell-de-de ln -s /usr/share/hunspell ~/.config/Code/Dictionaries By using ++f8++ or ++shift+f8++ you can jump to the next/previous error in the document. With the cursor over a misspelled word use ++ctrl+.++ to open a context menu with correct spelling or the ability to add the word to the dictionary. If you start using this you will have to add lots of specific words from the technical range into the user dictionary but after that it works better and better. The dictionary will be stored under .vscode/spellright.dict within your project. Sometimes you see encoding problems like in german Umlaut. This can be fixed by converting the dictionary files to UTF-8. To find the dictionary files call hunspell -D and then for the problematic dictionaries: sudo cp /usr/share/hunspell/de_DE.aff /usr/share/hunspell/de_DE.aff.bak sudo cp /usr/share/hunspell/de_DE.dic /usr/share/hunspell/de_DE.dic.bak sudo iconv -f iso-8859-1 -t UTF-8 /usr/share/hunspell/de_DE.aff.bak | sudo tee /usr/share/hunspell/de_DE.aff sudo iconv -f iso-8859-1 -t UTF-8 /usr/share/hunspell/de_DE.dic.bak | sudo tee /usr/share/hunspell/de_DE.dic You have to restart your IDE or if that won't work the whole system to take effect. Git Nowadays Git is a central tool for many developers. But if you have to switch to the command line or other GUI application while programming to check or uncheck code, it will interrupt the workflow. That's why Microsoft VS Code comes with Git integration by default. This provides the developer with the most important Git operations in the editor. GitLens supercharges the Git capabilities built into Visual Studio Code. It helps you to visualize code authorship at a glance via Git blame annotations and code lens, seamlessly navigate and explore Git repositories, gain valuable insights via powerful comparison commands, and so much more. All this is available through a new Icon on the left sidebar menu. Markdown Markdown All in One - All you need for Markdown (keyboard shortcuts, table of contents, auto preview and more). markdown lint - The Markdown lint will check the syntax of the markdown files and give hints for proper standardized markdown which will work. To configure MarkdownLint add something like the following: !!! example \"User Settings\" ```json \"markdownlint.config\": { \"MD007\": { \"indent\": 4 }, \"MD013\": false, \"MD030\": { \"ul_single\": 3, \"ol_single\": 2, \"ul_multi\": 3, \"ol_multi\": 2 }, \"MD041\": false } ``` See also the rules description. Rust Programming The Rust plugin includes: Rust Language Server integration. Auto completion (via racer or RLS ). Go To Definition (via racer or RLS ). Go To Symbol (via rustsym or RLS ). Code formatting (via rustfmt ). Code Snippets. Cargo tasks ( Ctrl-Shift-p and type cargo to view them). Also the following extensions may help: Better TOML - TOML configuration files syntax highlighting CodeLLDB - LLDB debugging for Rust programs Rust (RLS) - the language server search-crates-io - auto complete search within Cargo.toml crates - will display latest version in Cargo.toml and allows to switch through hover list Keep in mind that if you just installed Rust you have to sometimes reboot your systems to find the new commands in the path. After that VSCode will ask for tool chain... Mongo DB Visual Studio Code has great support for working with MongoDB databases. Through the Azure CosmosDB extension , you can create, manage and query MongoDB databases from within VS Code. After installing you will find it in the Activity Bar under Azure . You can: connect to a MongoDB server browse through your collections show entries change and update them make *.mongo scripts with code completion execute the scripts To setup the database connection open the Azure activity bar, select 'Attached Database Accounts' -> 'Attach Database Account...' then select 'MongoDB' and give the server address. You can navigate through the database like through a file system. PostgreSQL This will give you access to PostgreSQL databases and allows to query them. VueJS Extension Pack This already comes with a lot of extension needed for working with Vue: npm - This extension supports running npm scripts defined in the package.json file and validating the installed modules against the dependencies defined in the package.json NPM IntelliSense - Visual Studio Code plugin that auto completes npm modules in import statements Import Cost VSCode - This extension will display inline in the editor the size of the imported package. The extension utilizes webpack with babili-webpack-plugin in order to detect the imported size. Prettier - Code formatter - VS Code package to format your JavaScript / TypeScript / CSS using Prettier. vetur - Vue tooling for VS Code, powered by vue-language-server. vue-peek - This extension extends Vue code editing with Go To Definition and Peek Definition support for components and filenames in single-file components with a .vue extension. It allows quickly jumping to or peeking into files that are referenced as components (from template), or as module imports auto-rename-tag - Automatically rename paired HTML/XML tag, same as Visual Studio IDE does. auto-close-tag - Automatically add HTML/XML close tag, same as Visual Studio IDE or Sublime Text does. Sorting HTML and Jade attributes - Sorting of the tag attributes in the specified order. Bracket Pair Colorizer - This extension allows matching brackets to be identified with colors. The user can define which characters to match, and which colors to use. ESLint - Integrates ESLint into VS Code. If you are new to ESLint check the documentation. REST Client Write your requests in editor with syntax highlighting and auto completion, send and view the response in a separate pane with syntax highlighting. Also you can create a curl call out of it. Multiple requests in one file are supported by ### as a delimiter line. Edit with Shell This plugin lets you easily call shell commands and get the output within the editor. It can also send the marked text through the command and replace it with the output. You may add the key binding: { \"key\": \"ctrl-r ctrl-r\", \"command\": \"editWithShell.runCommand\", \"when\": \"editorTextFocus\" } Remote - SSH Allows you to open a remote folder from any remote machine, virtual machine, or container with a running SSH server and take full advantage of VS Code's feature set. Once connected to a server, you can interact with files and folders anywhere on the remote filesystem. Misc Material Icon Theme - lots of icons based on Material Design {!docs/abbreviations.txt!}","title":"VS Code"},{"location":"env/vscode/#vs-code-editor","text":"Microsoft Visual Studio Code is a free, open source code editor for developing and debugging. It runs on any OS and unlike large development environments (Microsoft Visual Studio, Eclipse...) it won't need a project setup before. VS Code uses the information it finds in the files and folders to provide project and platform-specific functions, such as matching auto complete. VS Code supports a variety of programming, markup, and database languages, from JavaScript, HTML, and CSS, through C #, C ++, and Python, to PHP and SQL, just to name a few. Common languages usually already included in the standard scope of the editor, other languages are available as an extension. In addition to pure syntax highlighting, auto completion is also provided for some languages, meaningfully supplementing user input: a pop-up displays previously declared variables, methods, functions, and objects that can be accessed in the program context, as well as extensive templates of commonly used pieces of code (templates and snippets).","title":"VS Code Editor"},{"location":"env/vscode/#installation","text":"On Debian you may install using the repository, to do the updates with your normal system updates: # add repository curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg sudo install -o root -g root -m 644 microsoft.gpg /etc/apt/trusted.gpg.d/ sudo sh -c 'echo \"deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\" > /etc/apt/sources.list.d/vscode.list' sudo apt-get install apt-transport-https # install VS Code sudo apt-get update sudo apt-get install code","title":"Installation"},{"location":"env/vscode/#user-settings","text":"Visual Studio Code can be configured to your personal needs. The base for this is a JSON configuration file. Using the command Preferences: Open User Settings you can open a view with the default configuration on the left and your settings on the right side. You can only change the ones on the right side. But you can also use the configuration form which is reachable using the gear icon on the lower left or ++ctrl+,++. This dialog shows a tree structured form with some of the most essential settings for the core and the extensions. I used at least the following editor settings: { \"editor.formatOnSave\": true, \"editor.tabSize\": 2, \"editor.detectIndentation\": false, \"editor.multiCursorModifier\": \"ctrlCmd\" } The last one is necessary on my KDE based Linux Distribution because ++alt+left-button++ is already used by the os.","title":"User Settings"},{"location":"env/vscode/#workspace-or-project-settings","text":"You can also add a settings.json within the /.vscode folder in your project which will overwrite the user settings. This is a convenient way to set project specific settings which are needed for every user.","title":"Workspace or Project Settings"},{"location":"env/vscode/#user-interface","text":"The user interface of Visual Studio Code is divided into directory tree, editor and console. In VS code, multiple editor windows can be opened next to each other thanks to split or side-by-side editing. Multiple selection allows you to interactively change multiple lines at the same time, for example, when renaming variables. Code navigation enables targeted searches for specific codes, such as methods or symbols. The UI pattern is similar to other editors such as Sublime Text and Atom: on the left is a file manager that displays all the files and folders in the current directory. Right next to it is the actual editor. There is space for various panels, such as a terminal or a log for errors and warnings. The user interface can be flexibly adapted to your own needs. If you need to do something always consider to lookup for included commands using ++f1++ and search for them.","title":"User Interface"},{"location":"env/vscode/#commands","text":"Use ++f1++ or the key combination ++ctrl+shift+p++ to open the command palette. The editor can be operated with keyboard commands via them so that you can do without the mouse almost completely. The command palette is an input box that displays a list of all available commands; it is therefore not necessary to remember the shortcuts of all commands. This list can be filtered by entering the command you are looking for and using the arrow keys to search. With Enter a command is executed. The command palette of VS Code knows different modes: ++greater++ default command mode commands are executed as usual. ++\"#\"++ and ++\"@\"++ to search for symbols such as methods and variables in the current workspace or file. ++colon++ jumps to the given line number ++ctrl+p++ search for open files by typing their name. All files in the currently open folder are searched. The command palette of VS Code can do even more use ++\"?\"++ displays a list of all modes.","title":"Commands"},{"location":"env/vscode/#shortcuts","text":"All the shortcuts can be shown directly in the editor calling Preferences -> Keyboard Shortcuts and you can also search and change them in this table. Althought the most used ones will be shown here: General ++ctrl+shift+p++ or ++f1++ Show Command Palette ++ctrl+p++ Quick Open, Go to File... ++ctrl+shift+n++ New window/instance ++ctrl+w++ Close window/instance ++ctrl+\",\"++ Open User Settings ++ctrl+k++ ++ctrl+s++ Keyboard Shortcuts Basic editing ++ctrl+x++ Cut line (empty selection) ++ctrl+c++ Copy line (empty selection) ++alt+down++ / ++alt+up++ Move line down/up ++ctrl+shift+k++ Delete line ++ctrl+enter++ / ++ctrl+shift+enter++ Insert line below/ above ++ctrl+shift+\"\\\"++ Jump to matching bracket ++ctrl+j++ / ++ctrl+\"[\"++ Indent/Outdent line ++ctrl+home++ / ++ctrl+end++ Go to beginning/end of file ++ctrl+up++ / ++ctrl+down++ Scroll line up/down ++alt+page-up++ / ++alt+page-down++ Scroll page up/down ++ctrl+shift+\"[\"++ / ++ctrl+shift+\"]\"++ Fold/unfold region ++ctrl+k++ ++ctrl+\"[\"++ / ++ctrl+k++ ++ctrl+\"]\"++ Fold/unfold all subregions ++ctrl+k++ ++ctrl+0++ / ++ctrl+k++ ++ctrl+j++ Fold/Unfold all regions ++ctrl+k++ ++ctrl+c++ Add line comment ++ctrl+k++ ++ctrl+u++ Remove line comment ++ctrl+\"/\"++ Toggle line comment ++ctrl+shift+a++ Toggle block comment ++alt+z++ Toggle word wrap Rich languages editing ++ctrl+space++ Trigger suggestion ++ctrl+shift+space++ Trigger parameter hints ++ctrl+shift+i++ Format document ++ctrl+k++ ++ctrl+f++ Format selection ++f12++ Go to Definition ++ctrl+shift+F10 Peek Definition ++ctrl+k++ ++f12++ Open Definition to the side ++ctrl+\".\"++ Quick Fix ++shift+f12++ Show References ++f2++ Rename Symbol ++ctrl+k++ ++ctrl+x++ Trim trailing whitespace ++ctrl+k++ ++m++ Change file language Multi-cursor and selection ++ctrl+left-button++ Insert cursor (changed by multi cursor setting above) ++shift+alt+up++ / ++shift+alt+down++ Insert cursor above/below ++ctrl+u++ Undo last cursor operation ++shift+alt+i++ Insert cursor at end of each line selected ++ctrl+i++ Select current line ++ctrl+shift+l++ Select all occurrences of current selection ++ctrl+f2++ Select all occurrences of current word ++shift+alt+right++ Expand selection ++shift+alt+left++ Shrink selection ++shift+alt+\"drag mouse\"++ Column (box) selection Display ++f11++ Toggle full screen ++shift+alt+0++ Toggle editor layout (horizontal/vertical) ++ctrl+\"=\"++ / ++ctrl+\"-\"++ Zoom in/out ++ctrl+b++ Toggle Sidebar visibility ++ctrl+shift+e++ Show Explorer / Toggle focus ++ctrl+shift+f++ Show Search ++ctrl+shift+g++ Show Source Control ++ctrl+shift+d++ Show Debug ++ctrl+shift+x++ Show Extensions ++ctrl+shift+h++ Replace in files ++ctrl+shift+j++ Toggle Search details ++ctrl+shift+c++ Open new command prompt/terminal ++ctrl+k++ ++ctrl+h++ Show Output panel ++ctrl+shift+v++ Open Markdown preview ++ctrl+k++ ++v++ Open Markdown preview to the side ++ctrl+k++ ++z++ Zen Mode (Esc Esc to exit) Search and replace ++ctrl+f++ Find ++ctrl+h++ Replace ++f3++ / ++shift+f3++ Find next/previous ++alt+enter++ Select all occurrences of Find match ++ctrl+d++ Add selection to next Find match ++ctrl+k++ ++ctrl+d++ Move last selection to next Find match Navigation ++ctrl+t++ Show all Symbols ++ctrl+g++ Go to Line... ++ctrl+p++ Go to File... ++ctrl+shift+o++ Go to Symbol... ++ctrl+shift+m++ Show Problems panel ++f8++ Go to next error or warning ++shift+f8++ Go to previous error or warning ++ctrl+shift+tab++ Navigate editor group history ++ctrl+alt+\"-\"++ Go back ++ctrl+shift+\"-\"++ Go forward ++ctrl+m++ Toggle Tab moves focus Editor management ++ctrl+w++ Close editor ++ctrl+k++ ++f++ Close folder ++ctrl+\"\\\"++ Split editor ++ctrl+1++ / ++ctrl+2++ / ++ctrl+3++ Focus into 1st, 2nd, 3rd editor group ++ctrl+k++ ++ctrl+left++ Focus into previous editor group ++ctrl+k++ ++ctrl+right++ Focus into next editor group ++ctrl+shift+page-up++ Move editor left ++ctrl+shift+page-down++ Move editor right ++ctrl+k++ ++left++ Move active editor group left/up ++ctrl+k++ ++right++ Move active editor group right/down File management ++ctrl+n++ New File ++ctrl+o++ Open File... ++ctrl+s++ Save ++ctrl+shift+s++ Save As... ++ctrl+w++ Close ++ctrl+k++ ++ctrl+w++ Close All ++ctrl+shift+t++ Reopen closed editor ++ctrl+k++ ++Enter Keep preview mode editor open ++ctrl+tab++ Open next ++ctrl+shift+tab++ Open previous ++ctrl+k++ ++p++ Copy path of active file ++ctrl+k++ ++r++ Reveal active file in Explorer ++ctrl+k++ ++d++ Show active file in new window/instance Debug ++f9++ Toggle breakpoint ++f5++ Start / Continue ++f11++ / ++shift+f11++ Step into/out ++f10++ Step over ++shift+f5++ Stop ++ctrl+k++ ++ctrl+i++ Show hover Integrated terminal ++ctrl+\"`\"++ Show integrated terminal ++ctrl+shift+\"`\"++ Create new terminal ++ctrl+shift+c++ Copy selection ++ctrl+shift+v++ Paste into active terminal ++ctrl+shift+up++ / ++ctrl+shift+down++ Scroll up/down ++shift+page-up++ / ++shift+page-down++ Scroll page up/down ++shift+home++ / ++shift+end++ Scroll to top/bottom","title":"Shortcuts"},{"location":"env/vscode/#tasks","text":"You can run different tasks which are possible to be run on the command line directly from within the VSCode IDE. To define custom tasks call Configure Tasks from the global Tasks menu and select the Create tasks.json file from template entry. Then select Other format. You will get a JSON configuration in the current workspace and can define your tasks there: { \"version\": \"2.0.0\", \"tasks\": [ { \"label\": \"build\", \"type\": \"shell\", \"command\": \"cargo build\", \"group\": { \"kind\": \"build\", \"isDefault\": true }, \"problemMatcher\": [] }, { \"label\": \"test\", \"type\": \"shell\", \"command\": \"cargo test\", \"group\": { \"kind\": \"test\", \"isDefault\": true } } ] } The task's properties have the following semantic: label - The task's label used in the user interface. type - The task's type. For a custom task, this can either be shell or process . If shell is specified, the command is interpreted as a shell command. If process is specified, the command is interpreted as a process to execute. command - The actual command to execute. args - Optionally array used if they are not given as a one liner within the command call windows - Any Windows OS specific properties. group - Defines to which group the task belongs. presentation - Defines how the task output is handled in the user interface. Call the tasks using ++f1++ with tasks . Read more at VSCode Tasks .","title":"Tasks"},{"location":"env/vscode/#debugger","text":"The sleek and flexible Code Editor gives developers all the functionality they need to get the job done. This is mainly due to the good integration of the debugger and the direct Git connection. VS Code is perfect for all scripting languages. There are some enhancements in Marketplace for better handling... With the Chrome Debugger extension, front-end developers can debug their JavaScript code with Google Chrome. VS Code uses the Chrome DevTools Protocol, which associates the files loaded in the browser with the files opened in the editor. This allows the developer to place breakpoints directly into VS code, watch variables, or see the entire call stack as they debug - all without leaving the editor. A very useful extension that you should definitely look at as a front-end developer.","title":"Debugger"},{"location":"env/vscode/#extensions","text":"The standard functionality of VS Code can be extended with additional packages that can adapt almost anything, from the appearance and behavior of the user interface to core editor functions. Microsoft's code editor brings with it a package manager. Additional languages, debuggers and tools can be easily installed. In the chapters below I comment on some plugins. To get more information search for the extension in the editor and read the detailed description. Shortcut to get to them is ++ctrl+shift+x++ or open it with the square icon on the left. The editor will also sometimes show you some recommended extensions based on the files you opened.","title":"Extensions"},{"location":"env/vscode/#project-manager","text":"Project Manager add a new sidebar to easily switch between projects. After configuring each project you can easily select the project in the new sidebar which switches to this project and Explorer and Source Control will both hold this.","title":"Project Manager"},{"location":"env/vscode/#code-formatting","text":"Prettier enables VS Code to automatically optimize the format to the default style guides. A good way is to enable it on save in the settings. You can customize some parameters, I used to set the following to mainly have markdown which is fully compatible with MkDocs for markdown parsing (spesifically the tabWidth of 4 spaces is needed to use lists with multiline points). { \"prettier.printWidth\": 100, \"prettier.tabWidth\": 4, \"editor.formatOnSave\": true } Also auto formatting on save is enabled in the above.","title":"Code Formatting"},{"location":"env/vscode/#spellchecker","text":"Use Spell Right as spell checker which is using hunspell dictionaries. To make it work install also the hunspell dictionaries like: sudo apt install hunspell hunspell-en-us hunspell-de-de ln -s /usr/share/hunspell ~/.config/Code/Dictionaries By using ++f8++ or ++shift+f8++ you can jump to the next/previous error in the document. With the cursor over a misspelled word use ++ctrl+.++ to open a context menu with correct spelling or the ability to add the word to the dictionary. If you start using this you will have to add lots of specific words from the technical range into the user dictionary but after that it works better and better. The dictionary will be stored under .vscode/spellright.dict within your project. Sometimes you see encoding problems like in german Umlaut. This can be fixed by converting the dictionary files to UTF-8. To find the dictionary files call hunspell -D and then for the problematic dictionaries: sudo cp /usr/share/hunspell/de_DE.aff /usr/share/hunspell/de_DE.aff.bak sudo cp /usr/share/hunspell/de_DE.dic /usr/share/hunspell/de_DE.dic.bak sudo iconv -f iso-8859-1 -t UTF-8 /usr/share/hunspell/de_DE.aff.bak | sudo tee /usr/share/hunspell/de_DE.aff sudo iconv -f iso-8859-1 -t UTF-8 /usr/share/hunspell/de_DE.dic.bak | sudo tee /usr/share/hunspell/de_DE.dic You have to restart your IDE or if that won't work the whole system to take effect.","title":"Spellchecker"},{"location":"env/vscode/#git","text":"Nowadays Git is a central tool for many developers. But if you have to switch to the command line or other GUI application while programming to check or uncheck code, it will interrupt the workflow. That's why Microsoft VS Code comes with Git integration by default. This provides the developer with the most important Git operations in the editor. GitLens supercharges the Git capabilities built into Visual Studio Code. It helps you to visualize code authorship at a glance via Git blame annotations and code lens, seamlessly navigate and explore Git repositories, gain valuable insights via powerful comparison commands, and so much more. All this is available through a new Icon on the left sidebar menu.","title":"Git"},{"location":"env/vscode/#markdown","text":"Markdown All in One - All you need for Markdown (keyboard shortcuts, table of contents, auto preview and more). markdown lint - The Markdown lint will check the syntax of the markdown files and give hints for proper standardized markdown which will work. To configure MarkdownLint add something like the following: !!! example \"User Settings\" ```json \"markdownlint.config\": { \"MD007\": { \"indent\": 4 }, \"MD013\": false, \"MD030\": { \"ul_single\": 3, \"ol_single\": 2, \"ul_multi\": 3, \"ol_multi\": 2 }, \"MD041\": false } ``` See also the rules description.","title":"Markdown"},{"location":"env/vscode/#rust-programming","text":"The Rust plugin includes: Rust Language Server integration. Auto completion (via racer or RLS ). Go To Definition (via racer or RLS ). Go To Symbol (via rustsym or RLS ). Code formatting (via rustfmt ). Code Snippets. Cargo tasks ( Ctrl-Shift-p and type cargo to view them). Also the following extensions may help: Better TOML - TOML configuration files syntax highlighting CodeLLDB - LLDB debugging for Rust programs Rust (RLS) - the language server search-crates-io - auto complete search within Cargo.toml crates - will display latest version in Cargo.toml and allows to switch through hover list Keep in mind that if you just installed Rust you have to sometimes reboot your systems to find the new commands in the path. After that VSCode will ask for tool chain...","title":"Rust Programming"},{"location":"env/vscode/#mongo-db","text":"Visual Studio Code has great support for working with MongoDB databases. Through the Azure CosmosDB extension , you can create, manage and query MongoDB databases from within VS Code. After installing you will find it in the Activity Bar under Azure . You can: connect to a MongoDB server browse through your collections show entries change and update them make *.mongo scripts with code completion execute the scripts To setup the database connection open the Azure activity bar, select 'Attached Database Accounts' -> 'Attach Database Account...' then select 'MongoDB' and give the server address. You can navigate through the database like through a file system.","title":"Mongo DB"},{"location":"env/vscode/#postgresql","text":"This will give you access to PostgreSQL databases and allows to query them.","title":"PostgreSQL"},{"location":"env/vscode/#vuejs-extension-pack","text":"This already comes with a lot of extension needed for working with Vue: npm - This extension supports running npm scripts defined in the package.json file and validating the installed modules against the dependencies defined in the package.json NPM IntelliSense - Visual Studio Code plugin that auto completes npm modules in import statements Import Cost VSCode - This extension will display inline in the editor the size of the imported package. The extension utilizes webpack with babili-webpack-plugin in order to detect the imported size. Prettier - Code formatter - VS Code package to format your JavaScript / TypeScript / CSS using Prettier. vetur - Vue tooling for VS Code, powered by vue-language-server. vue-peek - This extension extends Vue code editing with Go To Definition and Peek Definition support for components and filenames in single-file components with a .vue extension. It allows quickly jumping to or peeking into files that are referenced as components (from template), or as module imports auto-rename-tag - Automatically rename paired HTML/XML tag, same as Visual Studio IDE does. auto-close-tag - Automatically add HTML/XML close tag, same as Visual Studio IDE or Sublime Text does. Sorting HTML and Jade attributes - Sorting of the tag attributes in the specified order. Bracket Pair Colorizer - This extension allows matching brackets to be identified with colors. The user can define which characters to match, and which colors to use. ESLint - Integrates ESLint into VS Code. If you are new to ESLint check the documentation.","title":"VueJS Extension Pack"},{"location":"env/vscode/#rest-client","text":"Write your requests in editor with syntax highlighting and auto completion, send and view the response in a separate pane with syntax highlighting. Also you can create a curl call out of it. Multiple requests in one file are supported by ### as a delimiter line.","title":"REST Client"},{"location":"env/vscode/#edit-with-shell","text":"This plugin lets you easily call shell commands and get the output within the editor. It can also send the marked text through the command and replace it with the output. You may add the key binding: { \"key\": \"ctrl-r ctrl-r\", \"command\": \"editWithShell.runCommand\", \"when\": \"editorTextFocus\" }","title":"Edit with Shell"},{"location":"env/vscode/#remote-ssh","text":"Allows you to open a remote folder from any remote machine, virtual machine, or container with a running SSH server and take full advantage of VS Code's feature set. Once connected to a server, you can interact with files and folders anywhere on the remote filesystem.","title":"Remote - SSH"},{"location":"env/vscode/#misc","text":"Material Icon Theme - lots of icons based on Material Design {!docs/abbreviations.txt!}","title":"Misc"},{"location":"js/","text":"JavaScript JavaScript has been around for over 20 years. It is the dominant programming language in web development. In the beginning JavaScript was a language for the web client (browser). Then came the ability to use JavaScript on the web server (with Node.js). JavaScript is a scripting or programming language that was initially used to implement complex and dynamic things on web pages. It changes the HTML and CSS within the browser to do it's work. It looks similar to C and Java but is a functional (not object oriented) language. With the use of prototypes object orientation can be used. The language itself was static over a long time, but since 2015 it's development speed turned up. With the ability to use JavaScript on the server the function has gone into yearly editions and a lot of other helpers like transpilers are growing around to compensate some of it's problems. {!docs/abbreviations.txt!}","title":"JavaScript"},{"location":"js/#javascript","text":"JavaScript has been around for over 20 years. It is the dominant programming language in web development. In the beginning JavaScript was a language for the web client (browser). Then came the ability to use JavaScript on the web server (with Node.js). JavaScript is a scripting or programming language that was initially used to implement complex and dynamic things on web pages. It changes the HTML and CSS within the browser to do it's work. It looks similar to C and Java but is a functional (not object oriented) language. With the use of prototypes object orientation can be used. The language itself was static over a long time, but since 2015 it's development speed turned up. With the ability to use JavaScript on the server the function has gone into yearly editions and a lot of other helpers like transpilers are growing around to compensate some of it's problems. {!docs/abbreviations.txt!}","title":"JavaScript"},{"location":"js/coffee-styleguide/","text":"CoffeeScript Styleguide This guide should give an overview of the general style within the alinex system and it's packages. It is neither meant as a hard limit but neither as some nice to have rules. So as much as possible this rules have to be followed. General layout Within the code I use 2 spaces for indention neither tabs because they make reading problematic and also are not suited for coffee script if mixed with spaces. Naming Variables Over all modules some common variable names will be used for the same values: cb - for the callback method done - alternatively used for the callback for better reading err or error - for an error message object Classes start with an Uppercase letter: test = new Test To mark private properties/functions they may start with an underscore. Modules are always imported into a variable which is named after the module and if the module exports a class the uppercase name is used. Requirements Generally required modules belong on top of the file. But all things which will be required at the top will immediately load and slow down the initialization. Often some of these requirements won't be needed in every call. So it is better to load them dynamically then needed. To optimize readability the variables for dynamically loaded requirement will be set on top and the real requirement can be put anywhere in the code. Classes and Instances Classes are written starting with uppercase letter while it's instances start with lowercase letters. Constants They are used seldom but if so you have to name it completely in uppercase with underscores for word separator. Asynchronous code Through the whole code as much as possible only asynchronous code should be used without blocking. That brings the most performance out. Callbacks These code follows the node.js convention of providing a single callback as the last argument of your asynchronous function. To not make it too complex the async function may call it's callback synchronously in the same event loop or later. It is no true asynchronity by definition, so if you need it maybe think about your code structure or call the function through process.nextTick() yourself. Async module You may also use the Async module. It brings you a lot of methods to make asynchronous calls parallel or serial. Events Often events are also used to make use of the asynchronous behavior in class objects. Error/Exception handling Error and exception handling will be done in JavaScript/CoffeeScript in the following ways: Synchronous code will return an Error object instead of the normal return value: myfuncSync = () -> x = doSomething() if x is 1 return new Error \"Failed calculation\" return x result = myfuncSync() if result instanceof Error console.error result.message Asynchronous code should send the error as first parameter to the callback method or null if no error occurred: myfunc = (cb) -> x = doSomething() if x is 1 cb new Error \"Failed calculation\" else cb null, x myfunc (err, result) -> if err console.error err.message And at last an error may be thrown only if it is a hard error which signals an fatal error which may stop the overall process. Such an error may be catched in a try block, in a domain or on the process. Another solution is to use eventful methods which emit an error event Read more in the errorHandler documentation. Control flow To make the code more readable and prevent some hanging else-statements which you have to search to which indention level it belongs to return or callback as fast as possible. Bad: if err doSomething (err) -> return cb err else return true Good: return true unless err doSomething (err) -> return cb err Documentation All the code in any language will be documented inline using comments as possible in the specific language. No JavaDoc or alike have to be used. Only plain markup which will be automatically detected and converted to documentation. In each exported method the parameters will be documented in detail and the callback methods with their values, they will get. Some overall documentation like this will be stored in special markup files ending with .md . Debugging Best use the node-inspector for this: sudo npm install -g node-inspector node-inspector node --debug-brk lib/cli.js -v But additionally the debug package is used in all packages to make it possible to debug any time, also in the production code to get more information. debug = require('debug') 'mypack' debug \"the system is running\" You instantiate a new debug instance after requiring it with a name. This should be your package name (for the main part). For subparts of your package add a colon with a specifier like mypack:reader or mypack:writer:html . If you run the code above normally nothing will happen but if you set the DEBUG environment variable like: DEBUG=mypack node lib/index.js You will get the debug output. You can add multiple debug packages with colon or use asterisk like mypack* to select the package with all sub level. You can also add multiple debug statements and exclude some: DEBUG=config*,exec* DEBUG=*,-config* Testing Linting is used to pre-check the code. This will not only check the syntax but also the style and the above conventions of the styleguide. Mocha unit tests are used to check individual parts and the overall functionality. As much as possible should be tested but not only single units but all interface calls. An test coverage of 100% should be the goal. If used the builder will call them with builder -c test for you. Publishing To publish new packages the following steps should be done: Check dependencies and their version in package.json (maybe run npm outdated ) Cleanup to remove test packages Install and update packages Run the test suite Push code to repository Publish new package Create and publish new documentation The tasks are done in an easy way using the alinex-builder . # check what is changed and if all packages are up-to-date builder -c changes # do the steps 1-7 builder -c publish --minor Configuration The general configuration is stored in files to be also available without database connection. To make it easy readable and maintainable it should be written in YAML rather than JSON. alinex-config helps in working with such files. Dependencies To use singleton modules the dependencies for specific versions should be as open as possible. This makes it possible to use: npm dedupe This will flatten down the modules to the uppermost common module in hierarchy. Peer dependencies are also possible, in which a sub module specifies which parent it needs. This will be specified in the peerDependencies section. {!docs/abbreviations.txt!}","title":"CoffeeScript Styleguide"},{"location":"js/coffee-styleguide/#coffeescript-styleguide","text":"This guide should give an overview of the general style within the alinex system and it's packages. It is neither meant as a hard limit but neither as some nice to have rules. So as much as possible this rules have to be followed.","title":"CoffeeScript Styleguide"},{"location":"js/coffee-styleguide/#general-layout","text":"Within the code I use 2 spaces for indention neither tabs because they make reading problematic and also are not suited for coffee script if mixed with spaces.","title":"General layout"},{"location":"js/coffee-styleguide/#naming","text":"","title":"Naming"},{"location":"js/coffee-styleguide/#variables","text":"Over all modules some common variable names will be used for the same values: cb - for the callback method done - alternatively used for the callback for better reading err or error - for an error message object Classes start with an Uppercase letter: test = new Test To mark private properties/functions they may start with an underscore. Modules are always imported into a variable which is named after the module and if the module exports a class the uppercase name is used.","title":"Variables"},{"location":"js/coffee-styleguide/#requirements","text":"Generally required modules belong on top of the file. But all things which will be required at the top will immediately load and slow down the initialization. Often some of these requirements won't be needed in every call. So it is better to load them dynamically then needed. To optimize readability the variables for dynamically loaded requirement will be set on top and the real requirement can be put anywhere in the code.","title":"Requirements"},{"location":"js/coffee-styleguide/#classes-and-instances","text":"Classes are written starting with uppercase letter while it's instances start with lowercase letters.","title":"Classes and Instances"},{"location":"js/coffee-styleguide/#constants","text":"They are used seldom but if so you have to name it completely in uppercase with underscores for word separator.","title":"Constants"},{"location":"js/coffee-styleguide/#asynchronous-code","text":"Through the whole code as much as possible only asynchronous code should be used without blocking. That brings the most performance out.","title":"Asynchronous code"},{"location":"js/coffee-styleguide/#callbacks","text":"These code follows the node.js convention of providing a single callback as the last argument of your asynchronous function. To not make it too complex the async function may call it's callback synchronously in the same event loop or later. It is no true asynchronity by definition, so if you need it maybe think about your code structure or call the function through process.nextTick() yourself.","title":"Callbacks"},{"location":"js/coffee-styleguide/#async-module","text":"You may also use the Async module. It brings you a lot of methods to make asynchronous calls parallel or serial.","title":"Async module"},{"location":"js/coffee-styleguide/#events","text":"Often events are also used to make use of the asynchronous behavior in class objects.","title":"Events"},{"location":"js/coffee-styleguide/#errorexception-handling","text":"Error and exception handling will be done in JavaScript/CoffeeScript in the following ways: Synchronous code will return an Error object instead of the normal return value: myfuncSync = () -> x = doSomething() if x is 1 return new Error \"Failed calculation\" return x result = myfuncSync() if result instanceof Error console.error result.message Asynchronous code should send the error as first parameter to the callback method or null if no error occurred: myfunc = (cb) -> x = doSomething() if x is 1 cb new Error \"Failed calculation\" else cb null, x myfunc (err, result) -> if err console.error err.message And at last an error may be thrown only if it is a hard error which signals an fatal error which may stop the overall process. Such an error may be catched in a try block, in a domain or on the process. Another solution is to use eventful methods which emit an error event Read more in the errorHandler documentation.","title":"Error/Exception handling"},{"location":"js/coffee-styleguide/#control-flow","text":"To make the code more readable and prevent some hanging else-statements which you have to search to which indention level it belongs to return or callback as fast as possible. Bad: if err doSomething (err) -> return cb err else return true Good: return true unless err doSomething (err) -> return cb err","title":"Control flow"},{"location":"js/coffee-styleguide/#documentation","text":"All the code in any language will be documented inline using comments as possible in the specific language. No JavaDoc or alike have to be used. Only plain markup which will be automatically detected and converted to documentation. In each exported method the parameters will be documented in detail and the callback methods with their values, they will get. Some overall documentation like this will be stored in special markup files ending with .md .","title":"Documentation"},{"location":"js/coffee-styleguide/#debugging","text":"Best use the node-inspector for this: sudo npm install -g node-inspector node-inspector node --debug-brk lib/cli.js -v But additionally the debug package is used in all packages to make it possible to debug any time, also in the production code to get more information. debug = require('debug') 'mypack' debug \"the system is running\" You instantiate a new debug instance after requiring it with a name. This should be your package name (for the main part). For subparts of your package add a colon with a specifier like mypack:reader or mypack:writer:html . If you run the code above normally nothing will happen but if you set the DEBUG environment variable like: DEBUG=mypack node lib/index.js You will get the debug output. You can add multiple debug packages with colon or use asterisk like mypack* to select the package with all sub level. You can also add multiple debug statements and exclude some: DEBUG=config*,exec* DEBUG=*,-config*","title":"Debugging"},{"location":"js/coffee-styleguide/#testing","text":"Linting is used to pre-check the code. This will not only check the syntax but also the style and the above conventions of the styleguide. Mocha unit tests are used to check individual parts and the overall functionality. As much as possible should be tested but not only single units but all interface calls. An test coverage of 100% should be the goal. If used the builder will call them with builder -c test for you.","title":"Testing"},{"location":"js/coffee-styleguide/#publishing","text":"To publish new packages the following steps should be done: Check dependencies and their version in package.json (maybe run npm outdated ) Cleanup to remove test packages Install and update packages Run the test suite Push code to repository Publish new package Create and publish new documentation The tasks are done in an easy way using the alinex-builder . # check what is changed and if all packages are up-to-date builder -c changes # do the steps 1-7 builder -c publish --minor","title":"Publishing"},{"location":"js/coffee-styleguide/#configuration","text":"The general configuration is stored in files to be also available without database connection. To make it easy readable and maintainable it should be written in YAML rather than JSON. alinex-config helps in working with such files.","title":"Configuration"},{"location":"js/coffee-styleguide/#dependencies","text":"To use singleton modules the dependencies for specific versions should be as open as possible. This makes it possible to use: npm dedupe This will flatten down the modules to the uppermost common module in hierarchy. Peer dependencies are also possible, in which a sub module specifies which parent it needs. This will be specified in the peerDependencies section. {!docs/abbreviations.txt!}","title":"Dependencies"},{"location":"js/coffee/","text":"CoffeeScript CoffeeScript is an easier syntax for JavaScript. It has a syntax inspired from Python and Ruby. The code will be transformed into real JavaScript before executing. This can be done on the fly or as a compiler before packaging and executing it in the productive environment. Why To Use It I used CoffeeScript as development language because it was very easy to write and read in contrast to ES5. It made development and maintenance easier and faster, specially in the callback sequences of JavaScript. But with ES8 JavaScript has gained so much power, that it is no longer really necessary and development of this transpiler language got stuck. This page lists some tricks and tips which are mostly not mentioned in the official docs. Strings Check for substring if ~message.indexOf 'test' The above syntax uses the tilde as bitwise or operator to get a boolean readable value. If the substring is found it will be true (any number), and false (0) if it isn\u2019t. or use the following to get a real boolean type: x = Boolean ~message.indexOf 'test' Or use the negation: if !~message.indexOf 'test' Existence Operator A very handy operator is the existence operator '?' which can be used to prevent errors with accessing methods of undefined: x = table?.row?['col1']?[0] ? '-' In the above example it is used multiple times to check for existence in each level. So if any of the table, row, 'col1', element [0] didn't exist the first part will be set to undefined. The standalone existence operator after that will set a x to a default value if the first part is not defined or null. Arrays A very handy thing with arrays is the command concatenation used like: console.log text.split /\\n/ .filter (l) -> l[0] isnt '#' .map (l) -> l.split /\\s*,\\s*/ .filter (e) -> e? .map (e) -> e.replace '\\t', '\\\\t' .join '\\t' .join '\\n' In the above example a multiline text is split in lines filtered, so that lines starting with '#' are excluded then the line is splitted on ',' with surrounding whitespace empty columns will be removed the tabs within columns will be masked then the columns will be joined with tab separator and the lines will be joined together again This example shows how a complete data structure is parsed, optimized and put together in an optimized form. Objects Check for empty object unless obj? and Object.keys(obj).length This will run the following code only if the object under test is not defined or is an empty object. Control flow Try without catch try funcWhichMayThrowError() If you omit the catch the error will be suppressed. try { funcWhichMayThrowError(); } catch (_error) {} Loops Loop over array: for element in arr Counting loop: for i in [0..n] by 1 Reverse loop: for i in arr by -1 Loop over object: for key of object ... for key, value of object ... Callbacks To prevent checking for a given callback method on every call you may give it an empty default function on parameter definition: test = (src, cb = -> ) -> console.log 'do something' cb() Classes Classes are made really easy using CoffeeScript: # define a class class Test # make a constructor method constructor: -> # create instance t = new Test Use the @ sign in parameters to store the value in the same instance property: class Test # store given name in this.name constructor: (@name) -> # add another prototype to class Test::done = true Inheritance is also made easy: class Test constructor: (@name) -> # extend the class class Test2 extends Test constructor: -> # call the parent constructor super 'numberTwo' Local variables are created using = but properties are defined using : : class Test # local class variable/function foo = 1 # instance variable/function bar: 2 # or from the constructor constructor: -> @baz = 3 You may also use class properties/functions class Test # static class variable/function @foo: 1 @bar: (name) -> constructor: -> # access statics Test.bar() # or @constructor.bar() # access from outside Test.foo Test.bar 'baz' test = new Test test.constructor.foo {!docs/abbreviations.txt!}","title":"CoffeeScript"},{"location":"js/coffee/#coffeescript","text":"CoffeeScript is an easier syntax for JavaScript. It has a syntax inspired from Python and Ruby. The code will be transformed into real JavaScript before executing. This can be done on the fly or as a compiler before packaging and executing it in the productive environment.","title":"CoffeeScript"},{"location":"js/coffee/#why-to-use-it","text":"I used CoffeeScript as development language because it was very easy to write and read in contrast to ES5. It made development and maintenance easier and faster, specially in the callback sequences of JavaScript. But with ES8 JavaScript has gained so much power, that it is no longer really necessary and development of this transpiler language got stuck. This page lists some tricks and tips which are mostly not mentioned in the official docs.","title":"Why To Use It"},{"location":"js/coffee/#strings","text":"","title":"Strings"},{"location":"js/coffee/#check-for-substring","text":"if ~message.indexOf 'test' The above syntax uses the tilde as bitwise or operator to get a boolean readable value. If the substring is found it will be true (any number), and false (0) if it isn\u2019t. or use the following to get a real boolean type: x = Boolean ~message.indexOf 'test' Or use the negation: if !~message.indexOf 'test'","title":"Check for substring"},{"location":"js/coffee/#existence-operator","text":"A very handy operator is the existence operator '?' which can be used to prevent errors with accessing methods of undefined: x = table?.row?['col1']?[0] ? '-' In the above example it is used multiple times to check for existence in each level. So if any of the table, row, 'col1', element [0] didn't exist the first part will be set to undefined. The standalone existence operator after that will set a x to a default value if the first part is not defined or null.","title":"Existence Operator"},{"location":"js/coffee/#arrays","text":"A very handy thing with arrays is the command concatenation used like: console.log text.split /\\n/ .filter (l) -> l[0] isnt '#' .map (l) -> l.split /\\s*,\\s*/ .filter (e) -> e? .map (e) -> e.replace '\\t', '\\\\t' .join '\\t' .join '\\n' In the above example a multiline text is split in lines filtered, so that lines starting with '#' are excluded then the line is splitted on ',' with surrounding whitespace empty columns will be removed the tabs within columns will be masked then the columns will be joined with tab separator and the lines will be joined together again This example shows how a complete data structure is parsed, optimized and put together in an optimized form.","title":"Arrays"},{"location":"js/coffee/#objects","text":"","title":"Objects"},{"location":"js/coffee/#check-for-empty-object","text":"unless obj? and Object.keys(obj).length This will run the following code only if the object under test is not defined or is an empty object.","title":"Check for empty object"},{"location":"js/coffee/#control-flow","text":"","title":"Control flow"},{"location":"js/coffee/#try-without-catch","text":"try funcWhichMayThrowError() If you omit the catch the error will be suppressed. try { funcWhichMayThrowError(); } catch (_error) {}","title":"Try without catch"},{"location":"js/coffee/#loops","text":"Loop over array: for element in arr Counting loop: for i in [0..n] by 1 Reverse loop: for i in arr by -1 Loop over object: for key of object ... for key, value of object ...","title":"Loops"},{"location":"js/coffee/#callbacks","text":"To prevent checking for a given callback method on every call you may give it an empty default function on parameter definition: test = (src, cb = -> ) -> console.log 'do something' cb()","title":"Callbacks"},{"location":"js/coffee/#classes","text":"Classes are made really easy using CoffeeScript: # define a class class Test # make a constructor method constructor: -> # create instance t = new Test Use the @ sign in parameters to store the value in the same instance property: class Test # store given name in this.name constructor: (@name) -> # add another prototype to class Test::done = true Inheritance is also made easy: class Test constructor: (@name) -> # extend the class class Test2 extends Test constructor: -> # call the parent constructor super 'numberTwo' Local variables are created using = but properties are defined using : : class Test # local class variable/function foo = 1 # instance variable/function bar: 2 # or from the constructor constructor: -> @baz = 3 You may also use class properties/functions class Test # static class variable/function @foo: 1 @bar: (name) -> constructor: -> # access statics Test.bar() # or @constructor.bar() # access from outside Test.foo Test.bar 'baz' test = new Test test.constructor.foo {!docs/abbreviations.txt!}","title":"Classes"},{"location":"js/es-next/","text":"ES.Next JavaScript ECMAScript is the standard upon which JavaScript is based, and it\u2019s often abbreviated to ES. ES5 was 10 years in the making, from 1999 to 2009, and as such it was also a fundamental and very important revision of the language, but now much time has passed that it\u2019s not worth discussing how code before ES5 worked. Since 2015 the official version names use the year of it's final release but in the community also the ongoing short number is also be used. So here I will show both. Before 2015 there the standard was more statically but with the defined yearly release cycles of the proposal this all changed. The implementation is always way behind depending on the specific engine but by using transpilers most newer features may also be used nowadays. ECMAScript 2015 (ES6) ES6 also known as ECMAScript 2015 adds significant new syntax for writing complex applications, including classes and modules. It contains a lot of new features because a long time passed between ES5 and ES6. The most important changes in ES2015 include: Arrow functions Promises Iterators Generators let and const Classes Modules Template literals Default parameters The spread operator Destructuring assignments Enhanced object literals Map and Set See a lot of them described here ECMAScript 2016 (ES7) ES7 is a tiny release with only two new functionalities: Array.prototype.includes Exponentiation Operator See a lot of them described here ECMAScript 2017 (ES8) Compared to it's number of features this introduces only some but very useful features: String padding Object.values Object.entries Object.getOwnPropertyDescriptors() Trailing commas in function parameter lists and calls Async functions Shared memory and atomics See a lot of them described here ECMAScript 2018 (ES9) Major new features: Asynchronous Iteration Rest/Spread Properties New regular expression features: RegExp named capture groups RegExp Unicode Property Escapes RegExp Look-behind Assertions s (dotAll) flag for regular expressions Other new features: Promise.prototype.finally() Template Literal Revision See a lot of them described here ECMAScript 2019 (ES10) New features: Array.flat() Array.flatMap() Object.fromEntries() String.trimStart() and String.trimEnd() Optional Catch Binding Function.toString() Symbol.description Well Formed JSON.Stringify() Array.sort() Stability JSON \u2282 ECMAScript (JSON Superset) See a lot of them described here Proposals All the other proposal not included in any current standards are defined in 5 stages: Stage 0 - Straw man: just an idea, possible Babel plugin. Stage 1 - Proposal: this is worth working on. Stage 2 - Draft: initial spec. Stage 3 - Candidate: complete spec and initial browser implementations. Stage 4 - Finished: will be added to the next yearly release. Babel Babel is a JavaScript compiler which will take modern JavaScript code and transpile them into an older language using transformations and polyfills. Essentially this makes the code runnable in an environment which didn't support the newest JavaScript standards. To see what babel will do try it out in the online Babel REPL . It's an interactive online playground that shows you how babel will translate your code using different presets. But you can also use the babel-node CLI which comes with an REPL like node itself to run code interactively on your local machine. For the Alinex modules this is used to already use the newest language features while also get it working today. Installing Babel First you need to install Babel CLI within your project as development requirement and you will also need some additional presets: First you have to decide what babel should transform. The rules defines a range of possibilities which are transformed down till a specific version to run. Node 6 with support till stage-3 npm install babel-cli babel-preset-env babel-preset-stage-3 babel-polyfill --save-dev To make it usable you also have to add a configuration file .babelrc in your project root and specify the installed presets to be used: { \"presets\": [[\"env\", { \"targets\": { \"node\": 6 } }], \"stage-3\"], \"plugins\": [], \"comments\": false } Older NodeJS support all proposals npm install babel-cli babel-preset-es2015 babel-preset-stage-0 babel-polyfill --save-dev To make it usable you also have to add a configuration file .babelrc in your project root and specify the installed presets to be used: { \"presets\": [\"es2015\", \"stage-0\"], \"plugins\": [], \"comments\": false } Using Babel Now you can integrate babel with your project by defining it's call in the scripts section of package.json : { \"scripts\": { \"dev\": \"node_modules/.bin/babel-node src/index.js --require babel-polyfill\", \"build\": \"babel src -d dist --require babel-polyfill\", \"start\": \"node dist/index.js\", \"prepublish\": \"npm run build\" }, \"engines\": { \"node\": \">=6\" } } Also specify the engines information to help people use your module in the right version. This defines the babel transformation to: Be used as JIT compiler in development mode (run using npm run dev ) Be converted into ES5 code by running npm run build (into dist folder) Everything in dist folder can be used without babel Before publishing to npm the build command is called automatically The babel polyfills are needed to use the async/await which will come with node 7.6. {!docs/abbreviations.txt!}","title":"ES.Next"},{"location":"js/es-next/#esnext-javascript","text":"ECMAScript is the standard upon which JavaScript is based, and it\u2019s often abbreviated to ES. ES5 was 10 years in the making, from 1999 to 2009, and as such it was also a fundamental and very important revision of the language, but now much time has passed that it\u2019s not worth discussing how code before ES5 worked. Since 2015 the official version names use the year of it's final release but in the community also the ongoing short number is also be used. So here I will show both. Before 2015 there the standard was more statically but with the defined yearly release cycles of the proposal this all changed. The implementation is always way behind depending on the specific engine but by using transpilers most newer features may also be used nowadays.","title":"ES.Next JavaScript"},{"location":"js/es-next/#ecmascript-2015-es6","text":"ES6 also known as ECMAScript 2015 adds significant new syntax for writing complex applications, including classes and modules. It contains a lot of new features because a long time passed between ES5 and ES6. The most important changes in ES2015 include: Arrow functions Promises Iterators Generators let and const Classes Modules Template literals Default parameters The spread operator Destructuring assignments Enhanced object literals Map and Set See a lot of them described here","title":"ECMAScript 2015 (ES6)"},{"location":"js/es-next/#ecmascript-2016-es7","text":"ES7 is a tiny release with only two new functionalities: Array.prototype.includes Exponentiation Operator See a lot of them described here","title":"ECMAScript 2016 (ES7)"},{"location":"js/es-next/#ecmascript-2017-es8","text":"Compared to it's number of features this introduces only some but very useful features: String padding Object.values Object.entries Object.getOwnPropertyDescriptors() Trailing commas in function parameter lists and calls Async functions Shared memory and atomics See a lot of them described here","title":"ECMAScript 2017 (ES8)"},{"location":"js/es-next/#ecmascript-2018-es9","text":"Major new features: Asynchronous Iteration Rest/Spread Properties New regular expression features: RegExp named capture groups RegExp Unicode Property Escapes RegExp Look-behind Assertions s (dotAll) flag for regular expressions Other new features: Promise.prototype.finally() Template Literal Revision See a lot of them described here","title":"ECMAScript 2018 (ES9)"},{"location":"js/es-next/#ecmascript-2019-es10","text":"New features: Array.flat() Array.flatMap() Object.fromEntries() String.trimStart() and String.trimEnd() Optional Catch Binding Function.toString() Symbol.description Well Formed JSON.Stringify() Array.sort() Stability JSON \u2282 ECMAScript (JSON Superset) See a lot of them described here","title":"ECMAScript 2019 (ES10)"},{"location":"js/es-next/#proposals","text":"All the other proposal not included in any current standards are defined in 5 stages: Stage 0 - Straw man: just an idea, possible Babel plugin. Stage 1 - Proposal: this is worth working on. Stage 2 - Draft: initial spec. Stage 3 - Candidate: complete spec and initial browser implementations. Stage 4 - Finished: will be added to the next yearly release.","title":"Proposals"},{"location":"js/es-next/#babel","text":"Babel is a JavaScript compiler which will take modern JavaScript code and transpile them into an older language using transformations and polyfills. Essentially this makes the code runnable in an environment which didn't support the newest JavaScript standards. To see what babel will do try it out in the online Babel REPL . It's an interactive online playground that shows you how babel will translate your code using different presets. But you can also use the babel-node CLI which comes with an REPL like node itself to run code interactively on your local machine. For the Alinex modules this is used to already use the newest language features while also get it working today.","title":"Babel"},{"location":"js/es-next/#installing-babel","text":"First you need to install Babel CLI within your project as development requirement and you will also need some additional presets: First you have to decide what babel should transform. The rules defines a range of possibilities which are transformed down till a specific version to run.","title":"Installing Babel"},{"location":"js/es-next/#node-6-with-support-till-stage-3","text":"npm install babel-cli babel-preset-env babel-preset-stage-3 babel-polyfill --save-dev To make it usable you also have to add a configuration file .babelrc in your project root and specify the installed presets to be used: { \"presets\": [[\"env\", { \"targets\": { \"node\": 6 } }], \"stage-3\"], \"plugins\": [], \"comments\": false }","title":"Node 6 with support till stage-3"},{"location":"js/es-next/#older-nodejs-support-all-proposals","text":"npm install babel-cli babel-preset-es2015 babel-preset-stage-0 babel-polyfill --save-dev To make it usable you also have to add a configuration file .babelrc in your project root and specify the installed presets to be used: { \"presets\": [\"es2015\", \"stage-0\"], \"plugins\": [], \"comments\": false }","title":"Older NodeJS support all proposals"},{"location":"js/es-next/#using-babel","text":"Now you can integrate babel with your project by defining it's call in the scripts section of package.json : { \"scripts\": { \"dev\": \"node_modules/.bin/babel-node src/index.js --require babel-polyfill\", \"build\": \"babel src -d dist --require babel-polyfill\", \"start\": \"node dist/index.js\", \"prepublish\": \"npm run build\" }, \"engines\": { \"node\": \">=6\" } } Also specify the engines information to help people use your module in the right version. This defines the babel transformation to: Be used as JIT compiler in development mode (run using npm run dev ) Be converted into ES5 code by running npm run build (into dist folder) Everything in dist folder can be used without babel Before publishing to npm the build command is called automatically The babel polyfills are needed to use the async/await which will come with node 7.6. {!docs/abbreviations.txt!}","title":"Using Babel"},{"location":"js/es2015/","text":"ECMAScript 2015 (ES6) Arrow functions The arrow functions are an optional syntax which is shorter to write and makes JavaScript code change it's look: const foo = function foo() { //... }; May be changed to: const foo = () => { //... }; And also shorter as one-liner: const foo = param => doSomething(param); A change between array functions and normal functions is that within array function this no longer refers to the current function but to the surrounding context. Promises Promises are a way out of the callback hell. They are standardized after Promise A+ and are a part of ES6. Here are some special cases. Synchronous functions A list of functions (maybe mixed with promise based and synchronous) can be processed one after each other by concatenating them with an initially created fulfilled Promise. const jobs = []; // list of normal or Promise based functions // normal jobs.push(data => { if (data === undefined) throw new Error(\"Data argument is needed.\"); return data; }); // and as promised jobs.push(data => { if (typeof data === \"function\") { Promise.reject(new Error(\"Function as data not supported!\")); } Promise.resolve(data); }); function process(input: any): Promise<any> { let data = input; // run rules seriously let p = Promise.resolve(input); jobs.forEach(fn => { p = p.then(data => fn.call(this, data)); }); return p .then(data => data) .catch(err => (err instanceof Error ? Promise.reject(err) : err)); } This pattern also for three possibilities within each serious function: resolve(result) or return result to send the result and go on reject(new Error(...)) or throw new Error(...) to stop processing with fail reject(result) or throw result to stop further processing but with success Resolve outside of Promise In some cases you want to resolve a promise on a specific event from the outside: let promiseResolve; let promiseReject; var promise = new Promise((resolve, reject) => { promiseResolve = resolve; promiseReject = reject; }); promiseResolve(); Iterator An iterator is used to step over a list of objects. Therefore the next() method can be used to retrieve the next value in the sequence. At the end a StopIteration Exception is thrown. It may be used manually or in a for each or for ... in loop. var data = { name: \"Alex\", country: \"Germany\" }; var it = Iterator(data); for (var pair in it) print(pair); // prints each [key, value] pair in turn See more at MDN:Iterators . The same goes with for...of to be used on arrays to iterate over the value for (const v of [\"a\", \"b\", \"c\"]) { console.log(v); } //get the index as well, using `entries()` for (const [i, v] of [\"a\", \"b\", \"c\"].entries()) { console.log(i, v); } Generators A generator is like an iterator but it won't have a fixed set to iterate over but will generate the values on each iteration (on demand). It's a new programming concept introduced with EcmaScript6 which will be implemented in Node 0.12 or may be used in Node 0.11 using the --harmony flag. Within a generator you may use the yield keyword to pause and resume. See more at MDN:Generators and MDN:yield . let and const var is the traditionally scoped function. let is a new variable declaration which is block scoped. This means that declaring let variables in a for loop, inside an if or in a plain block is not going to let that variable \u201cescape\u201d the block, while var s are hoisted up to the function definition. const is just like let, but immutable. This is used very widely but only the variable itself is immutable if it references an array or object the contents of them may change also in const variables. In JavaScript moving forward, you\u2019ll see little to no var declarations any more, just let and const . Classes JavaScript is the only mainstream language with prototype-based inheritance. This makes it hard for programmers switching language. The mechanism is kept but it may be defined using class-based language. Now inheritance looks very easy and resembles other object-oriented programming languages: class Person { constructor(name) { this.name = name; } hello() { return \"Hello, I am \" + this.name + \".\"; } } class Actor extends Person { hello() { return super.hello() + \" I am an actor.\"; } } var tomCruise = new Actor(\"Tom Cruise\"); tomCruise.hello(); // prints \u201cHello, I am Tom Cruise. I am an actor.\u201d Classes do not have explicit class variable declarations, but you must initialize any variable in the constructor. The constructor is a special method called constructor which is called when a class is initialized via new . The parent class can be referenced using super() . And also getters and setters available by using: class Person { get fullName() { return `${this.firstName} ${this.lastName}`; } } class Person { set age(years) { this.theAge = years; } } Modules A first standardized and new module definition to import and export modules is given. Import It depends on what the module is exporting. If the module exports a collection of named exports you should use the collection import: import fs from \"fs\"; // works but not preferred import * as fs from \"fs\"; // recommended But the best way is to only import the methods you really need: import { readdirSync } from \"fs\"; // recommended Export You can write modules and export anything to other modules using the export keyword which will make them available to import under their name: export var foo = 2; export function bar() { /* ... */ } Also a standard element may be exported using default as name. Template Literals Template literals are a new syntax to create strings: const aString = `A string`; They provide a way to embed expressions into strings, effectively interpolating the values, by using the \\${a_variable} syntax: const var = 'test' const string = `something ${var}` //something test You can perform more complex expressions as well: const string = `something ${1 + 2 + 3}`; const string2 = `something ${foo() ? \"x\" : \"y\"}`; and strings can span over multiple lines: const string3 = `Hey this string is awesome!`; Default parameters Functions now support default parameters: const foo = function(index = 0, testing = true) { /* ... */ }; foo(); Spread operator You can expand an array, an object or a string using the spread operator ... . You may create a new array using: const a = [1, 2, 3]; const b = [...a, 4, 5, 6]; You can also create a copy of an array or object using const c = [...a]; const newObj = { ...oldObj }; This operator can also be used to take an array as function arguments in a very simple way: const f = (foo, bar) => {}; const a = [1, 2]; f(...a); Destructuring assignments Given an object, you can extract just some values and put them into named variables: const person = { firstName: \"Tom\", lastName: \"Cruise\", actor: true, age: 54 //made up }; const { firstName: name, age } = person; // name and age contain the desired values. The syntax also works on arrays: const a = [1,2,3,4,5] [first, second, , , fifth] = a Enhanced Object Literals Simpler syntax to include variables, instead of doing const something = \"y\"; const x = { something: something }; you can do const something = \"y\"; const x = { something }; A prototype can be specified with const anObject = { y: \"y\" }; const x = { __proto__: anObject }; or you can use super() from an function within an object const anObject = { y: \"y\", test: () => \"zoo\" }; const x = { __proto__: anObject, test() { return super.test() + \"x\"; } }; x.test(); //zoox Dynamic properties are also possible const x = { [\"a\" + \"_\" + \"b\"]: \"z\" }; x.a_b; //z Map and Set Map and Set (and their respective garbage collected WeakMap and WeakSet) are the official implementations of two very popular data structures. Other goodies Object.assign() copies all enumerable own properties from one or more objects, and return a new object. {!docs/abbreviations.txt!}","title":"ECMAScript 2015"},{"location":"js/es2015/#ecmascript-2015-es6","text":"","title":"ECMAScript 2015 (ES6)"},{"location":"js/es2015/#arrow-functions","text":"The arrow functions are an optional syntax which is shorter to write and makes JavaScript code change it's look: const foo = function foo() { //... }; May be changed to: const foo = () => { //... }; And also shorter as one-liner: const foo = param => doSomething(param); A change between array functions and normal functions is that within array function this no longer refers to the current function but to the surrounding context.","title":"Arrow functions"},{"location":"js/es2015/#promises","text":"Promises are a way out of the callback hell. They are standardized after Promise A+ and are a part of ES6. Here are some special cases.","title":"Promises"},{"location":"js/es2015/#synchronous-functions","text":"A list of functions (maybe mixed with promise based and synchronous) can be processed one after each other by concatenating them with an initially created fulfilled Promise. const jobs = []; // list of normal or Promise based functions // normal jobs.push(data => { if (data === undefined) throw new Error(\"Data argument is needed.\"); return data; }); // and as promised jobs.push(data => { if (typeof data === \"function\") { Promise.reject(new Error(\"Function as data not supported!\")); } Promise.resolve(data); }); function process(input: any): Promise<any> { let data = input; // run rules seriously let p = Promise.resolve(input); jobs.forEach(fn => { p = p.then(data => fn.call(this, data)); }); return p .then(data => data) .catch(err => (err instanceof Error ? Promise.reject(err) : err)); } This pattern also for three possibilities within each serious function: resolve(result) or return result to send the result and go on reject(new Error(...)) or throw new Error(...) to stop processing with fail reject(result) or throw result to stop further processing but with success","title":"Synchronous functions"},{"location":"js/es2015/#resolve-outside-of-promise","text":"In some cases you want to resolve a promise on a specific event from the outside: let promiseResolve; let promiseReject; var promise = new Promise((resolve, reject) => { promiseResolve = resolve; promiseReject = reject; }); promiseResolve();","title":"Resolve outside of Promise"},{"location":"js/es2015/#iterator","text":"An iterator is used to step over a list of objects. Therefore the next() method can be used to retrieve the next value in the sequence. At the end a StopIteration Exception is thrown. It may be used manually or in a for each or for ... in loop. var data = { name: \"Alex\", country: \"Germany\" }; var it = Iterator(data); for (var pair in it) print(pair); // prints each [key, value] pair in turn See more at MDN:Iterators . The same goes with for...of to be used on arrays to iterate over the value for (const v of [\"a\", \"b\", \"c\"]) { console.log(v); } //get the index as well, using `entries()` for (const [i, v] of [\"a\", \"b\", \"c\"].entries()) { console.log(i, v); }","title":"Iterator"},{"location":"js/es2015/#generators","text":"A generator is like an iterator but it won't have a fixed set to iterate over but will generate the values on each iteration (on demand). It's a new programming concept introduced with EcmaScript6 which will be implemented in Node 0.12 or may be used in Node 0.11 using the --harmony flag. Within a generator you may use the yield keyword to pause and resume. See more at MDN:Generators and MDN:yield .","title":"Generators"},{"location":"js/es2015/#let-and-const","text":"var is the traditionally scoped function. let is a new variable declaration which is block scoped. This means that declaring let variables in a for loop, inside an if or in a plain block is not going to let that variable \u201cescape\u201d the block, while var s are hoisted up to the function definition. const is just like let, but immutable. This is used very widely but only the variable itself is immutable if it references an array or object the contents of them may change also in const variables. In JavaScript moving forward, you\u2019ll see little to no var declarations any more, just let and const .","title":"let and const"},{"location":"js/es2015/#classes","text":"JavaScript is the only mainstream language with prototype-based inheritance. This makes it hard for programmers switching language. The mechanism is kept but it may be defined using class-based language. Now inheritance looks very easy and resembles other object-oriented programming languages: class Person { constructor(name) { this.name = name; } hello() { return \"Hello, I am \" + this.name + \".\"; } } class Actor extends Person { hello() { return super.hello() + \" I am an actor.\"; } } var tomCruise = new Actor(\"Tom Cruise\"); tomCruise.hello(); // prints \u201cHello, I am Tom Cruise. I am an actor.\u201d Classes do not have explicit class variable declarations, but you must initialize any variable in the constructor. The constructor is a special method called constructor which is called when a class is initialized via new . The parent class can be referenced using super() . And also getters and setters available by using: class Person { get fullName() { return `${this.firstName} ${this.lastName}`; } } class Person { set age(years) { this.theAge = years; } }","title":"Classes"},{"location":"js/es2015/#modules","text":"A first standardized and new module definition to import and export modules is given.","title":"Modules"},{"location":"js/es2015/#import","text":"It depends on what the module is exporting. If the module exports a collection of named exports you should use the collection import: import fs from \"fs\"; // works but not preferred import * as fs from \"fs\"; // recommended But the best way is to only import the methods you really need: import { readdirSync } from \"fs\"; // recommended","title":"Import"},{"location":"js/es2015/#export","text":"You can write modules and export anything to other modules using the export keyword which will make them available to import under their name: export var foo = 2; export function bar() { /* ... */ } Also a standard element may be exported using default as name.","title":"Export"},{"location":"js/es2015/#template-literals","text":"Template literals are a new syntax to create strings: const aString = `A string`; They provide a way to embed expressions into strings, effectively interpolating the values, by using the \\${a_variable} syntax: const var = 'test' const string = `something ${var}` //something test You can perform more complex expressions as well: const string = `something ${1 + 2 + 3}`; const string2 = `something ${foo() ? \"x\" : \"y\"}`; and strings can span over multiple lines: const string3 = `Hey this string is awesome!`;","title":"Template Literals"},{"location":"js/es2015/#default-parameters","text":"Functions now support default parameters: const foo = function(index = 0, testing = true) { /* ... */ }; foo();","title":"Default parameters"},{"location":"js/es2015/#spread-operator","text":"You can expand an array, an object or a string using the spread operator ... . You may create a new array using: const a = [1, 2, 3]; const b = [...a, 4, 5, 6]; You can also create a copy of an array or object using const c = [...a]; const newObj = { ...oldObj }; This operator can also be used to take an array as function arguments in a very simple way: const f = (foo, bar) => {}; const a = [1, 2]; f(...a);","title":"Spread operator"},{"location":"js/es2015/#destructuring-assignments","text":"Given an object, you can extract just some values and put them into named variables: const person = { firstName: \"Tom\", lastName: \"Cruise\", actor: true, age: 54 //made up }; const { firstName: name, age } = person; // name and age contain the desired values. The syntax also works on arrays: const a = [1,2,3,4,5] [first, second, , , fifth] = a","title":"Destructuring assignments"},{"location":"js/es2015/#enhanced-object-literals","text":"Simpler syntax to include variables, instead of doing const something = \"y\"; const x = { something: something }; you can do const something = \"y\"; const x = { something }; A prototype can be specified with const anObject = { y: \"y\" }; const x = { __proto__: anObject }; or you can use super() from an function within an object const anObject = { y: \"y\", test: () => \"zoo\" }; const x = { __proto__: anObject, test() { return super.test() + \"x\"; } }; x.test(); //zoox Dynamic properties are also possible const x = { [\"a\" + \"_\" + \"b\"]: \"z\" }; x.a_b; //z","title":"Enhanced Object Literals"},{"location":"js/es2015/#map-and-set","text":"Map and Set (and their respective garbage collected WeakMap and WeakSet) are the official implementations of two very popular data structures.","title":"Map and Set"},{"location":"js/es2015/#other-goodies","text":"Object.assign() copies all enumerable own properties from one or more objects, and return a new object. {!docs/abbreviations.txt!}","title":"Other goodies"},{"location":"js/es2016/","text":"ECMAScript 2016 (ES7) Array.prototype.includes() This feature introduces a more readable syntax for checking if an array contains an element. With ES6 and lower, to check if an array contained an element you had to use indexOf , which checks the index in the array, and returns -1 if the element is not there. With this feature introduced in ES7 we can do if (![1, 2].includes(3)) { console.log(\"Not found\"); } Exponentiation Operator The exponentiation operator ** is the equivalent of Math.pow() , but brought into the language instead of being a library function. Math.pow(4, 2) == 4 ** 2; {!docs/abbreviations.txt!}","title":"ECMAScript 2015"},{"location":"js/es2016/#ecmascript-2016-es7","text":"","title":"ECMAScript 2016 (ES7)"},{"location":"js/es2016/#arrayprototypeincludes","text":"This feature introduces a more readable syntax for checking if an array contains an element. With ES6 and lower, to check if an array contained an element you had to use indexOf , which checks the index in the array, and returns -1 if the element is not there. With this feature introduced in ES7 we can do if (![1, 2].includes(3)) { console.log(\"Not found\"); }","title":"Array.prototype.includes()"},{"location":"js/es2016/#exponentiation-operator","text":"The exponentiation operator ** is the equivalent of Math.pow() , but brought into the language instead of being a library function. Math.pow(4, 2) == 4 ** 2; {!docs/abbreviations.txt!}","title":"Exponentiation Operator"},{"location":"js/es2017/","text":"ECMAScript 2017 (ES8) String padding The purpose of string padding is to add characters to a string, so it reaches a specific length using padStart() and padEnd() . \u2018test\u2019.padStart(4) // \u2018test\u2019 \u2018test\u2019.padStart(5) // \u2019 test\u2019 \u2018test\u2019.padStart(8) // \u2019 test\u2019 \u2018test\u2019.padStart(8, \u2018abcd\u2019) // \u2018abcdtest\u2019 \u2018test\u2019.padEnd(4) // \u2018test\u2019 \u2018test\u2019.padEnd(5) // \u2018test \u2018 \u2018test\u2019.padEnd(8) // \u2018test \u2018 \u2018test\u2019.padEnd(8, \u2018abcd\u2019) // \u2018testabcd\u2019 Object.values() This method returns an array containing all the object own property values. const person = { name: \"Fred\", age: 87 }; Object.values(person); // ['Fred', 87] const people = [\"Fred\", \"Tony\"]; Object.values(people); // ['Fred', 'Tony'] Object.entries() This method returns an array containing all the object own properties, as an array of [key, value] pairs. const person = { name: \"Fred\", age: 87 }; Object.entries(person); // [['name', 'Fred'], ['age', 87]] const people = [\"Fred\", \"Tony\"]; Object.entries(people); // [['0', 'Fred'], ['1', 'Tony']] getOwnPropertyDescriptors() This method returns all own (non-inherited) properties descriptors of an object. A descriptor is a set of attributes of a property, and it\u2019s composed by a subset of the following: value : the value of the property writable : true the property can be changed get : a getter function for the property, called when the property is read set : a setter function for the property, called when the property is set to a value configurable : if false, the property cannot be removed nor any attribute can be changed, except its value enumerable : true if the property is enumerable If an object for example has just a setter, it\u2019s not correctly copied to a new object, using Object.assign() . const person1 = { set name(newName) { console.log(newName); } }; const person2 = {}; Object.assign(person2, person1); This won\u2019t work, but the following will work: const person3 = {}; Object.defineProperties(person3, Object.getOwnPropertyDescriptors(person1)); Trailing commas This feature allows to have trailing commas in function declarations, and in functions calls. It is only some formatting sugar to encourage developers to not use comma at the start of line: const doSomething = (var1, var2) => { //... }; doSomething(\"test2\", \"test2\"); Async functions Async functions are a combination of promises and generators to reduce the boilerplate around promises, and the \u201cdon\u2019t break the chain\u201d limitation of chaining promises. It\u2019s a higher level abstraction over promises. Promises were introduced to solve the famous callback hell problem, but they introduced complexity on their own, and syntax complexity. They were good primitives around which a better syntax could be exposed to the developers: enter async functions. Code making use of asynchronous functions can be written instead of function doSomethingAsync() { return new Promise(resolve => { setTimeout(() => resolve(\"I did something\"), 3000); }); } with async/await as async function doSomething() { console.log(await doSomethingAsync()); } Shared Memory and Atomics WebWorkers are used to create multi-threaded programs in the browser. They offer a messaging protocol via events. You can create a shared memory array between web workers and their creator, using a SharedArrayBuffer . Since it\u2019s unknown how much time writing to a shared memory portion takes to propagate, Atomics are a way to enforce that when reading a value, any kind of writing operation is completed. {!docs/abbreviations.txt!}","title":"ECMAScript 2017"},{"location":"js/es2017/#ecmascript-2017-es8","text":"","title":"ECMAScript 2017 (ES8)"},{"location":"js/es2017/#string-padding","text":"The purpose of string padding is to add characters to a string, so it reaches a specific length using padStart() and padEnd() . \u2018test\u2019.padStart(4) // \u2018test\u2019 \u2018test\u2019.padStart(5) // \u2019 test\u2019 \u2018test\u2019.padStart(8) // \u2019 test\u2019 \u2018test\u2019.padStart(8, \u2018abcd\u2019) // \u2018abcdtest\u2019 \u2018test\u2019.padEnd(4) // \u2018test\u2019 \u2018test\u2019.padEnd(5) // \u2018test \u2018 \u2018test\u2019.padEnd(8) // \u2018test \u2018 \u2018test\u2019.padEnd(8, \u2018abcd\u2019) // \u2018testabcd\u2019","title":"String padding"},{"location":"js/es2017/#objectvalues","text":"This method returns an array containing all the object own property values. const person = { name: \"Fred\", age: 87 }; Object.values(person); // ['Fred', 87] const people = [\"Fred\", \"Tony\"]; Object.values(people); // ['Fred', 'Tony']","title":"Object.values()"},{"location":"js/es2017/#objectentries","text":"This method returns an array containing all the object own properties, as an array of [key, value] pairs. const person = { name: \"Fred\", age: 87 }; Object.entries(person); // [['name', 'Fred'], ['age', 87]] const people = [\"Fred\", \"Tony\"]; Object.entries(people); // [['0', 'Fred'], ['1', 'Tony']]","title":"Object.entries()"},{"location":"js/es2017/#getownpropertydescriptors","text":"This method returns all own (non-inherited) properties descriptors of an object. A descriptor is a set of attributes of a property, and it\u2019s composed by a subset of the following: value : the value of the property writable : true the property can be changed get : a getter function for the property, called when the property is read set : a setter function for the property, called when the property is set to a value configurable : if false, the property cannot be removed nor any attribute can be changed, except its value enumerable : true if the property is enumerable If an object for example has just a setter, it\u2019s not correctly copied to a new object, using Object.assign() . const person1 = { set name(newName) { console.log(newName); } }; const person2 = {}; Object.assign(person2, person1); This won\u2019t work, but the following will work: const person3 = {}; Object.defineProperties(person3, Object.getOwnPropertyDescriptors(person1));","title":"getOwnPropertyDescriptors()"},{"location":"js/es2017/#trailing-commas","text":"This feature allows to have trailing commas in function declarations, and in functions calls. It is only some formatting sugar to encourage developers to not use comma at the start of line: const doSomething = (var1, var2) => { //... }; doSomething(\"test2\", \"test2\");","title":"Trailing commas"},{"location":"js/es2017/#async-functions","text":"Async functions are a combination of promises and generators to reduce the boilerplate around promises, and the \u201cdon\u2019t break the chain\u201d limitation of chaining promises. It\u2019s a higher level abstraction over promises. Promises were introduced to solve the famous callback hell problem, but they introduced complexity on their own, and syntax complexity. They were good primitives around which a better syntax could be exposed to the developers: enter async functions. Code making use of asynchronous functions can be written instead of function doSomethingAsync() { return new Promise(resolve => { setTimeout(() => resolve(\"I did something\"), 3000); }); } with async/await as async function doSomething() { console.log(await doSomethingAsync()); }","title":"Async functions"},{"location":"js/es2017/#shared-memory-and-atomics","text":"WebWorkers are used to create multi-threaded programs in the browser. They offer a messaging protocol via events. You can create a shared memory array between web workers and their creator, using a SharedArrayBuffer . Since it\u2019s unknown how much time writing to a shared memory portion takes to propagate, Atomics are a way to enforce that when reading a value, any kind of writing operation is completed. {!docs/abbreviations.txt!}","title":"Shared Memory and Atomics"},{"location":"js/es2018/","text":"ECMAScript 2018 (ES9) Asynchronous Iteration Synchronous iteration was introduced with ES6, now comes the counterpart. With Asynchronous Iteration we get asynchronous iterators and asynchronous iterables. Asynchronous iterators just like regular iterators, except their next() method returns a promise for a { value, done } pair. To consume asynchronous iterables, we can now use the await keyword with for \u2026 of loops. for await (const line of readLines(file)) { console.log(line) } Rest/Spread Properties When destructuring an object, Object Rest Properties allow you to collect the remaining properties of an object onto a new object. Think of it as a magic magnet attracting all leftovers. const data = { a: 1, b: 2, c: 3, d: 4 } const { c, a, ...x} console.log(a) // 1 console.log(c) // 3 console.log(x) // { b: 2, d: 4 } RegExp named capture groups Such expressions may look like: (?<year>[0-9]{4}) Here we have tagged the previous capture group #1 with the name year. After matching, you can access the captured string via matchObj.groups.year . Let\u2019s rewrite the previous code so that it uses named capture groups: const RE_DATE = /(?<year>[0-9]{4})-(?<month>[0-9]{2})-(?<day>[0-9]{2})/; const matchObj = RE_DATE.exec('1999-12-31'); const year = matchObj.groups.year; // 1999 const month = matchObj.groups.month; // 12 const day = matchObj.groups.day; // 31 Named capture groups also create indexed entries; as if they were numbered capture groups: const year2 = matchObj[1]; // 1999 const month2 = matchObj[2]; // 12 const day2 = matchObj[3]; // 31 Destructuring can help with getting data out of the match object: const { groups: { day, year } } = RE_DATE.exec('1999-12-31'); console.log(year); // 1999 console.log(day); // 31 You get the following benefits: It\u2019s easier to find the \u201cID\u201d of a capture group. The matching code becomes self-descriptive, as the ID of a capture group describes what is being captured. You don\u2019t have to change the matching code if you change the order of the capture groups. The names of the capture groups also make the regular expression easier to understand, as you can see directly what each group is for. You can freely mix numbered and named capture groups. Back references \\k<name> in a regular expression means: match the string that was previously matched by the named capture group name. For example: const RE_TWICE = /^(?<word>[a-z]+)!\\k<word>$/; RE_TWICE.test('abc!abc'); // true RE_TWICE.test('abc!ab'); // false The back reference syntax for numbered capture groups works for named capture groups, too. And you can freely mix both syntaxes. replace() and named capture groups The string method replace() supports named capture groups in two ways. First, you can mention their names in the replacement string: const RE_DATE = /(?<year>[0-9]{4})-(?<month>[0-9]{2})-(?<day>[0-9]{2})/; console.log('1999-12-31'.replace(RE_DATE, '$<month>/$<day>/$<year>')); // 12/31/1999 Second, each replacement function receives an additional parameter that holds an object with data captured via named groups. For example (line A): const RE_DATE = /(?<year>[0-9]{4})-(?<month>[0-9]{2})-(?<day>[0-9]{2})/; console.log( '1999-12-31'.replace( RE_DATE, ( all, y, m, d, offset, input, { year, month, day } // (A) ) => month + '/' + day + '/' + year ) ); // 12/31/1999 The last parameter is new and contains one property for each of the three named capture groups year, month and day. We use destructuring to access those properties. console.log( '1999-12-31'.replace(RE_DATE, (...args) => { const { year, month, day } = args[args.length - 1]; return month + '/' + day + '/' + year; }) ); // 12/31/1999 RegExp Unicode Property Escapes Unicode characters can be matched by mentioning their Unicode character properties inside the curly braces of \\p{} like: /^\\p{White_Space}+$/u.test('\\t \\n\\r') // => true /^\\p{Script=Greek}+$/u.test('\u03bc\u03b5\u03c4\u03ac') // => true As you can see, one of the benefits of property escapes is is that they make regular expressions more self-descriptive. Like in other regular expression escapes you can negate it by using uppercase. So while \\p matches character like defined \\P matches only the ones not defined. Examples of properties Name : a unique name, composed of uppercase letters, digits, hyphens and spaces. A : Name=LATIN CAPITAL LETTER A \ud83d\ude00 : Name=GRINNING FACE General_Category : categorizes characters. x : General_Category=Lowercase_Letter $ : General_Category=Currency_Symbol White_Space : used for marking invisible spacing characters, such as spaces, tabs and newlines. \\t : White_Space \u03c0 : White_Space=False Age : version of the Unicode Standard in which a character was introduced. The euro sign \u20ac was added in version 2.1 of the Unicode standard. \u20ac : Age=2.1 Block : a contiguous range of code points. Blocks don\u2019t overlap and their names are unique. S : Block=Basic_Latin \u0414 : Block=Cyrillic Script : is a collection of characters used by one or more writing systems. \u03b1 : Script=Greek \u05d0 : Script=Hebrew RegExp Look-behind Assertions Look-behind assertions work like lookahead assertions, but in the opposite direction. Positive look-behind assertions For a positive look-behind assertion, the text preceding the current location must match the assertion (but nothing else happens). const RE_DOLLAR_PREFIX = /(?<=\\$)foo/g; '$foo %foo foo'.replace(RE_DOLLAR_PREFIX, 'bar'); // '$bar %foo foo' As you can see, foo is only replaced if it is preceded by a dollar sign. You can also see that the dollar sign is not part of the total match, because the latter is completely replaced by 'bar'. Negative look-behind assertions A negative look-behind assertion only matches if the current location is not preceded by the assertion, but has no other effect. const RE_NO_DOLLAR_PREFIX = /(?<!\\$)foo/g; '$foo %foo foo'.replace(RE_NO_DOLLAR_PREFIX, 'bar'); // '$foo %bar bar' s (dotAll) flag for regular expressions The proposal introduces the regular expression flag /s (short for \u201csingle line\u201d), which leads to the dot matching line terminators: /^.$/s.test('\\n') // => true Promise.prototype.finally() Promise.prototype.finally() finalizes the whole promises implementation, allowing you to register a callback to be invoked when a promise is settled (either fulfilled, or rejected). A typical use case is to hide a spinner after a fetch() request: instead of duplicating the logic inside the last .then() and .catch() , one can now place it inside .finally() Template Literal Revision The problem is that even with the raw version, you don\u2019t have total freedom within template literals in ES2016. After a backslash, some sequences of characters are not legal anymore: \\u starts a Unicode escape, which must look like \\u{1F4A4} or \\u004B. \\x starts a hex escape, which must look like \\x4B. \\ plus digit starts an octal escape (such as \\141). Octal escapes are forbidden in template literals and strict mode string literals. That prevents tagged template literals such as: latex`\\unicode` windowsPath`C:\\uuu\\xxx\\111` The solution is drop all syntactic restrictions related to escape sequences. Then illegal escape sequences simply show up verbatim in the raw representation. But what about the cooked representation? Every template string with an illegal escape sequence is an undefined element in the cooked Array: {!docs/abbreviations.txt!}","title":"ECMAScript 2018"},{"location":"js/es2018/#ecmascript-2018-es9","text":"","title":"ECMAScript 2018 (ES9)"},{"location":"js/es2018/#asynchronous-iteration","text":"Synchronous iteration was introduced with ES6, now comes the counterpart. With Asynchronous Iteration we get asynchronous iterators and asynchronous iterables. Asynchronous iterators just like regular iterators, except their next() method returns a promise for a { value, done } pair. To consume asynchronous iterables, we can now use the await keyword with for \u2026 of loops. for await (const line of readLines(file)) { console.log(line) }","title":"Asynchronous Iteration"},{"location":"js/es2018/#restspread-properties","text":"When destructuring an object, Object Rest Properties allow you to collect the remaining properties of an object onto a new object. Think of it as a magic magnet attracting all leftovers. const data = { a: 1, b: 2, c: 3, d: 4 } const { c, a, ...x} console.log(a) // 1 console.log(c) // 3 console.log(x) // { b: 2, d: 4 }","title":"Rest/Spread Properties"},{"location":"js/es2018/#regexp-named-capture-groups","text":"Such expressions may look like: (?<year>[0-9]{4}) Here we have tagged the previous capture group #1 with the name year. After matching, you can access the captured string via matchObj.groups.year . Let\u2019s rewrite the previous code so that it uses named capture groups: const RE_DATE = /(?<year>[0-9]{4})-(?<month>[0-9]{2})-(?<day>[0-9]{2})/; const matchObj = RE_DATE.exec('1999-12-31'); const year = matchObj.groups.year; // 1999 const month = matchObj.groups.month; // 12 const day = matchObj.groups.day; // 31 Named capture groups also create indexed entries; as if they were numbered capture groups: const year2 = matchObj[1]; // 1999 const month2 = matchObj[2]; // 12 const day2 = matchObj[3]; // 31 Destructuring can help with getting data out of the match object: const { groups: { day, year } } = RE_DATE.exec('1999-12-31'); console.log(year); // 1999 console.log(day); // 31 You get the following benefits: It\u2019s easier to find the \u201cID\u201d of a capture group. The matching code becomes self-descriptive, as the ID of a capture group describes what is being captured. You don\u2019t have to change the matching code if you change the order of the capture groups. The names of the capture groups also make the regular expression easier to understand, as you can see directly what each group is for. You can freely mix numbered and named capture groups.","title":"RegExp named capture groups"},{"location":"js/es2018/#back-references","text":"\\k<name> in a regular expression means: match the string that was previously matched by the named capture group name. For example: const RE_TWICE = /^(?<word>[a-z]+)!\\k<word>$/; RE_TWICE.test('abc!abc'); // true RE_TWICE.test('abc!ab'); // false The back reference syntax for numbered capture groups works for named capture groups, too. And you can freely mix both syntaxes.","title":"Back references"},{"location":"js/es2018/#replace-and-named-capture-groups","text":"The string method replace() supports named capture groups in two ways. First, you can mention their names in the replacement string: const RE_DATE = /(?<year>[0-9]{4})-(?<month>[0-9]{2})-(?<day>[0-9]{2})/; console.log('1999-12-31'.replace(RE_DATE, '$<month>/$<day>/$<year>')); // 12/31/1999 Second, each replacement function receives an additional parameter that holds an object with data captured via named groups. For example (line A): const RE_DATE = /(?<year>[0-9]{4})-(?<month>[0-9]{2})-(?<day>[0-9]{2})/; console.log( '1999-12-31'.replace( RE_DATE, ( all, y, m, d, offset, input, { year, month, day } // (A) ) => month + '/' + day + '/' + year ) ); // 12/31/1999 The last parameter is new and contains one property for each of the three named capture groups year, month and day. We use destructuring to access those properties. console.log( '1999-12-31'.replace(RE_DATE, (...args) => { const { year, month, day } = args[args.length - 1]; return month + '/' + day + '/' + year; }) ); // 12/31/1999","title":"replace() and named capture groups"},{"location":"js/es2018/#regexp-unicode-property-escapes","text":"Unicode characters can be matched by mentioning their Unicode character properties inside the curly braces of \\p{} like: /^\\p{White_Space}+$/u.test('\\t \\n\\r') // => true /^\\p{Script=Greek}+$/u.test('\u03bc\u03b5\u03c4\u03ac') // => true As you can see, one of the benefits of property escapes is is that they make regular expressions more self-descriptive. Like in other regular expression escapes you can negate it by using uppercase. So while \\p matches character like defined \\P matches only the ones not defined.","title":"RegExp Unicode Property Escapes"},{"location":"js/es2018/#examples-of-properties","text":"Name : a unique name, composed of uppercase letters, digits, hyphens and spaces. A : Name=LATIN CAPITAL LETTER A \ud83d\ude00 : Name=GRINNING FACE General_Category : categorizes characters. x : General_Category=Lowercase_Letter $ : General_Category=Currency_Symbol White_Space : used for marking invisible spacing characters, such as spaces, tabs and newlines. \\t : White_Space \u03c0 : White_Space=False Age : version of the Unicode Standard in which a character was introduced. The euro sign \u20ac was added in version 2.1 of the Unicode standard. \u20ac : Age=2.1 Block : a contiguous range of code points. Blocks don\u2019t overlap and their names are unique. S : Block=Basic_Latin \u0414 : Block=Cyrillic Script : is a collection of characters used by one or more writing systems. \u03b1 : Script=Greek \u05d0 : Script=Hebrew","title":"Examples of properties"},{"location":"js/es2018/#regexp-look-behind-assertions","text":"Look-behind assertions work like lookahead assertions, but in the opposite direction.","title":"RegExp Look-behind Assertions"},{"location":"js/es2018/#positive-look-behind-assertions","text":"For a positive look-behind assertion, the text preceding the current location must match the assertion (but nothing else happens). const RE_DOLLAR_PREFIX = /(?<=\\$)foo/g; '$foo %foo foo'.replace(RE_DOLLAR_PREFIX, 'bar'); // '$bar %foo foo' As you can see, foo is only replaced if it is preceded by a dollar sign. You can also see that the dollar sign is not part of the total match, because the latter is completely replaced by 'bar'.","title":"Positive look-behind assertions"},{"location":"js/es2018/#negative-look-behind-assertions","text":"A negative look-behind assertion only matches if the current location is not preceded by the assertion, but has no other effect. const RE_NO_DOLLAR_PREFIX = /(?<!\\$)foo/g; '$foo %foo foo'.replace(RE_NO_DOLLAR_PREFIX, 'bar'); // '$foo %bar bar'","title":"Negative look-behind assertions"},{"location":"js/es2018/#s-dotall-flag-for-regular-expressions","text":"The proposal introduces the regular expression flag /s (short for \u201csingle line\u201d), which leads to the dot matching line terminators: /^.$/s.test('\\n') // => true","title":"s (dotAll) flag for regular expressions"},{"location":"js/es2018/#promiseprototypefinally","text":"Promise.prototype.finally() finalizes the whole promises implementation, allowing you to register a callback to be invoked when a promise is settled (either fulfilled, or rejected). A typical use case is to hide a spinner after a fetch() request: instead of duplicating the logic inside the last .then() and .catch() , one can now place it inside .finally()","title":"Promise.prototype.finally()"},{"location":"js/es2018/#template-literal-revision","text":"The problem is that even with the raw version, you don\u2019t have total freedom within template literals in ES2016. After a backslash, some sequences of characters are not legal anymore: \\u starts a Unicode escape, which must look like \\u{1F4A4} or \\u004B. \\x starts a hex escape, which must look like \\x4B. \\ plus digit starts an octal escape (such as \\141). Octal escapes are forbidden in template literals and strict mode string literals. That prevents tagged template literals such as: latex`\\unicode` windowsPath`C:\\uuu\\xxx\\111` The solution is drop all syntactic restrictions related to escape sequences. Then illegal escape sequences simply show up verbatim in the raw representation. But what about the cooked representation? Every template string with an illegal escape sequence is an undefined element in the cooked Array: {!docs/abbreviations.txt!}","title":"Template Literal Revision"},{"location":"js/es2019/","text":"ECMAScript 2019 (ES10) Array.flat() The flat() method creates a new array with all sub-array elements concatenated into it recursively up to the specified depth. const numbers = [1, 2, [3, 4, [5, 6]]]; // default depth is 1 numbers.flat(); > [1, 2, 3, 4, [5, 6]] Array.flatMap() The flatMap() method first maps each element using a mapping function, then flattens the result into a new array. It is identical to a map followed by a flat of depth 1, but flatMap is often quite useful, as merging both into one method is slightly more efficient. const numbers = [1, 2, 3]; numbers.flatMap(x => [x * 2]); > [2, 4, 6] Object.fromEntries() The Object.fromEntries() method transforms a list of key-value pairs into an object. const records = [['name','Mathew'], ['age', 32]]; const obj = Object.fromEntries(records); > { name: 'Mathew', age: 32} String.trimStart() and String.trimEnd() The trimStart() method removes whitespace from the beginning of a string. The trimEnd() method removes whitespace from the end of a string. You can think why another new method already there are two method trimRight()and trimLeft() but it will be alias of above new methods Optional Catch Binding Allow developers to use try/catch without creating an unused binding. You are free to go ahead make use of catch block without a parameter. try { ... } catch { ... } Function.toString() The toString() method returns a string representing the source code of the function.Earlier white spaces,new lines and comments will be removed when you do now they are retained with original source code Symbol.description The read-only description property is a string returning the optional description of Symbol objects. Well Formed JSON.Stringify() To prevent JSON.stringify from returning ill-formed Unicode strings. Array.Sort Stability Elements with same sort value retain their order. It now uses always the stable TimSort algorithm. JSON \u2282 ECMAScript (JSON Superset) The line separator (U+2028) and paragraph separator (U+2029) symbols are now allowed in string literals. Previously, these were treated as line terminators and resulted in SyntaxError exceptions. {!docs/abbreviations.txt!}","title":"ECMAScript 2019"},{"location":"js/es2019/#ecmascript-2019-es10","text":"","title":"ECMAScript 2019 (ES10)"},{"location":"js/es2019/#arrayflat","text":"The flat() method creates a new array with all sub-array elements concatenated into it recursively up to the specified depth. const numbers = [1, 2, [3, 4, [5, 6]]]; // default depth is 1 numbers.flat(); > [1, 2, 3, 4, [5, 6]]","title":"Array.flat()"},{"location":"js/es2019/#arrayflatmap","text":"The flatMap() method first maps each element using a mapping function, then flattens the result into a new array. It is identical to a map followed by a flat of depth 1, but flatMap is often quite useful, as merging both into one method is slightly more efficient. const numbers = [1, 2, 3]; numbers.flatMap(x => [x * 2]); > [2, 4, 6]","title":"Array.flatMap()"},{"location":"js/es2019/#objectfromentries","text":"The Object.fromEntries() method transforms a list of key-value pairs into an object. const records = [['name','Mathew'], ['age', 32]]; const obj = Object.fromEntries(records); > { name: 'Mathew', age: 32}","title":"Object.fromEntries()"},{"location":"js/es2019/#stringtrimstart-and-stringtrimend","text":"The trimStart() method removes whitespace from the beginning of a string. The trimEnd() method removes whitespace from the end of a string. You can think why another new method already there are two method trimRight()and trimLeft() but it will be alias of above new methods","title":"String.trimStart() and String.trimEnd()"},{"location":"js/es2019/#optional-catch-binding","text":"Allow developers to use try/catch without creating an unused binding. You are free to go ahead make use of catch block without a parameter. try { ... } catch { ... }","title":"Optional Catch Binding"},{"location":"js/es2019/#functiontostring","text":"The toString() method returns a string representing the source code of the function.Earlier white spaces,new lines and comments will be removed when you do now they are retained with original source code","title":"Function.toString()"},{"location":"js/es2019/#symboldescription","text":"The read-only description property is a string returning the optional description of Symbol objects.","title":"Symbol.description"},{"location":"js/es2019/#well-formed-jsonstringify","text":"To prevent JSON.stringify from returning ill-formed Unicode strings.","title":"Well Formed JSON.Stringify()"},{"location":"js/es2019/#arraysort-stability","text":"Elements with same sort value retain their order. It now uses always the stable TimSort algorithm.","title":"Array.Sort Stability"},{"location":"js/es2019/#json-ecmascript-json-superset","text":"The line separator (U+2028) and paragraph separator (U+2029) symbols are now allowed in string literals. Previously, these were treated as line terminators and resulted in SyntaxError exceptions. {!docs/abbreviations.txt!}","title":"JSON \u2282 ECMAScript (JSON Superset)"},{"location":"js/es2020/","text":"ECMAScript 2020 (ES11) Optional Chaining I'm sure that most developers are familiar with an error of this kind: TypeError: Cannot read property 'x' of undefined This error basically means that we tried to access a property on something that is not an object. Accessing an Object Property const flower = { colors: { red: true } } console.log(flower.colors.red) // this will work console.log(flower.species.lily) // TypeError: Cannot read property 'lily' of undefined The optional chaining operator, which consists of a question mark and a dot: ?. , can be used to indicate that an error should not be thrown. Instead, if there is no value, undefined will be returned. console.log(flower.species?.lily) // undefined Optional chaining can also be used when accessing array values or calling a function. let flowers = ['lily', 'daisy', 'rose'] console.log(flowers[1]) // daisy flowers = null console.log(flowers[1]) // TypeError: Cannot read property '1' of null console.log(flowers?.[1]) // undefined let plantFlowers = () => { return 'orchids' } console.log(plantFlowers()) // orchids plantFlowers = null console.log(plantFlowers()) // TypeError: plantFlowers is not a function console.log(plantFlowers?.()) // undefined Nullish Coalescing Until recently, whenever there was a need to provide a fallback value, the logical operator || had to be used. It works in most cases, but it can't be applied in some scenarios. For instance, if the initial value is a Boolean or a number. Let's take a look at an example below, where we want to assign a number to a variable, or default it to 7 if the initial value is not a number: let number = 1 let myNumber = number || 7 The myNumber variable is equal to 1, because the left-hand value (number) is a truthy value, as 1 is a positive number. However, what if the number variable is not 1, but 0? let number = 0 let myNumber = number || 7 0 is a falsy value, and even though it is a number, the myNumber variable will have the right-hand value assigned to it. Therefore, myNumber is now equal to 7. However, that's not really what we want. Fortunately, instead of writing additional code and checks to confirm if the number variable is indeed a number, we can use the nullish coalescing operator. It consists of two question marks: ??. let number = 0 let myNumber = number ?? 7 The right-hand side value will only be assigned if the left-hand value is equal to null or undefined. Therefore, in the example above, the myNumber variable is equal to 0. Private Fields Many programming languages that have classes allow defining class properties as public, protected, or private. Public properties can be accessed from outside of a class and by its subclasses, while protected classes can only be accessed by subclasses. However, private properties can only be accessed from inside of a class. JavaScript supports class syntax since ES6, but only now were private fields introduced. To define a private property, it has to be prefixed with the hash symbol: #. class Flower { #leaf_color = \"green\"; constructor(name) { this.name = name; } get_color() { return this.#leaf_color; } } const orchid = new Flower(\"orchid\"); console.log(orchid.get_color()); // green console.log(orchid.#leaf_color) // Private name #leaf_color is not defined If we try to access a private property from outside, an error will be thrown. Static Fields To use a class method, a class had to be instantiated first, as shown below. class Flower { add_leaves() { console.log(\"Adding leaves\"); } } const rose = new Flower(); rose.add_leaves(); Flower.add_leaves() // TypeError: Flower.add_leaves is not a function Trying to access a method without instantiating the Flower class would result in an error. Thanks to static fields, a class method can now be declared with the static keyword and called from outside of a class. class Flower { constructor(type) { this.type = type; } static create_flower(type) { return new Flower(type); } } const rose = Flower.create_flower(\"rose\"); // Works fine Top Level Await So far, to await for a promise to finish, a function in which await is used would need to be defined with the async keyword. const func = async () => { const response = await fetch(url) } Unfortunately, if there was a need to await for something in a global scope, it would not be possible, and usually required an immediately invoked function expression (IIFE). (async () => { const response = await fetch(url) })() Thanks to Top Level Await, there is no need for wrapping code in an async function anymore, and this code will work. const response = await fetch(url) This feature could be useful for resolving module dependencies or using a fallback source if the initial one failed. let Vue try { Vue = await import('url_1_to_vue') } catch { Vue = await import('url_2_to_vue) } Promise.allSettled To wait for multiple promises to finish, Promise.all([promise_1, promise_2]) can be used. The problem is that if one of them fails, then an error will be thrown. Nevertheless, there are cases in which it is ok for one of the promises to fail, and the rest should still resolve. To achieve that, ES11 introduced Promise.allSettled. promise_1 = Promise.resolve('hello') primise_2 = new Promise((resolve, reject) => setTimeout(reject, 200, 'problem')) Promise.allSettled([promise_1, promise_2]) .then(([promise_1_result, promise_2_result]) => { console.log(promise_1_result) // {status: 'fulfilled', value: 'hello'} console.log(promise_2_result) // {status: 'rejected', reason: 'problem'} }) A resolved promise will return an object with status and value properties, while rejected ones will have status and reason. Dynamic Import You might have used dynamic imports when using webpack for module bundling. Finally, native support for this feature is here. // Alert.js file export default { show() { // Your alert } } // Some other file import('/components/Alert.js') .then(Alert => { Alert.show() }) Considering the fact that a lot applications use module bundlers like webpack for transpiling and optimizing code, this feature isn't such a big deal right now. MatchAll MatchAll is useful for applying the same regular expression to a string if you need to find all matches and get their positions. The match method only returns items that were matched. const regex = /\\b(apple)+\\b/; const fruits = \"pear, apple, banana, apple, orange, apple\"; for (const match of fruits.match(regex)) { console.log(match); } // Output // // 'apple' // 'apple' matchAll in contrast, returns a bit more information, including index of the string found. for (const match of fruits.matchAll(regex)) { console.log(match); } // Output // // [ // 'apple', // 'apple', // index: 6, // input: 'pear, apple, banana, apple, orange, apple', // groups: undefined // ], // [ // 'apple', // 'apple', // index: 21, // input: 'pear, apple, banana, apple, orange, apple', // groups: undefined // ], // [ // 'apple', // 'apple', // index: 36, // input: 'pear, apple, banana, apple, orange, apple', // groups: undefined // ] globalThis JavaScript can run in different environments like browsers or Node.js. A global object in browsers is available under window variable, but in Node it is an object called global. To make it easier to use a global object no matter in which environment code is running, globalThis was introduced. // In a browser window == globalThis // true // In node.js global == globalThis // true BigInt The maximum number that can be reliably represented in JavaScript is 2^53 - 1. BigInt will allow creation of numbers even bigger than that. const theBiggerNumber = 9007199254740991n const evenBiggerNumber = BigInt(9007199254740991) {!docs/abbreviations.txt!}","title":"ECMAScript 2020"},{"location":"js/es2020/#ecmascript-2020-es11","text":"","title":"ECMAScript 2020 (ES11)"},{"location":"js/es2020/#optional-chaining","text":"I'm sure that most developers are familiar with an error of this kind: TypeError: Cannot read property 'x' of undefined This error basically means that we tried to access a property on something that is not an object. Accessing an Object Property const flower = { colors: { red: true } } console.log(flower.colors.red) // this will work console.log(flower.species.lily) // TypeError: Cannot read property 'lily' of undefined The optional chaining operator, which consists of a question mark and a dot: ?. , can be used to indicate that an error should not be thrown. Instead, if there is no value, undefined will be returned. console.log(flower.species?.lily) // undefined Optional chaining can also be used when accessing array values or calling a function. let flowers = ['lily', 'daisy', 'rose'] console.log(flowers[1]) // daisy flowers = null console.log(flowers[1]) // TypeError: Cannot read property '1' of null console.log(flowers?.[1]) // undefined let plantFlowers = () => { return 'orchids' } console.log(plantFlowers()) // orchids plantFlowers = null console.log(plantFlowers()) // TypeError: plantFlowers is not a function console.log(plantFlowers?.()) // undefined","title":"Optional Chaining"},{"location":"js/es2020/#nullish-coalescing","text":"Until recently, whenever there was a need to provide a fallback value, the logical operator || had to be used. It works in most cases, but it can't be applied in some scenarios. For instance, if the initial value is a Boolean or a number. Let's take a look at an example below, where we want to assign a number to a variable, or default it to 7 if the initial value is not a number: let number = 1 let myNumber = number || 7 The myNumber variable is equal to 1, because the left-hand value (number) is a truthy value, as 1 is a positive number. However, what if the number variable is not 1, but 0? let number = 0 let myNumber = number || 7 0 is a falsy value, and even though it is a number, the myNumber variable will have the right-hand value assigned to it. Therefore, myNumber is now equal to 7. However, that's not really what we want. Fortunately, instead of writing additional code and checks to confirm if the number variable is indeed a number, we can use the nullish coalescing operator. It consists of two question marks: ??. let number = 0 let myNumber = number ?? 7 The right-hand side value will only be assigned if the left-hand value is equal to null or undefined. Therefore, in the example above, the myNumber variable is equal to 0.","title":"Nullish Coalescing"},{"location":"js/es2020/#private-fields","text":"Many programming languages that have classes allow defining class properties as public, protected, or private. Public properties can be accessed from outside of a class and by its subclasses, while protected classes can only be accessed by subclasses. However, private properties can only be accessed from inside of a class. JavaScript supports class syntax since ES6, but only now were private fields introduced. To define a private property, it has to be prefixed with the hash symbol: #. class Flower { #leaf_color = \"green\"; constructor(name) { this.name = name; } get_color() { return this.#leaf_color; } } const orchid = new Flower(\"orchid\"); console.log(orchid.get_color()); // green console.log(orchid.#leaf_color) // Private name #leaf_color is not defined If we try to access a private property from outside, an error will be thrown.","title":"Private Fields"},{"location":"js/es2020/#static-fields","text":"To use a class method, a class had to be instantiated first, as shown below. class Flower { add_leaves() { console.log(\"Adding leaves\"); } } const rose = new Flower(); rose.add_leaves(); Flower.add_leaves() // TypeError: Flower.add_leaves is not a function Trying to access a method without instantiating the Flower class would result in an error. Thanks to static fields, a class method can now be declared with the static keyword and called from outside of a class. class Flower { constructor(type) { this.type = type; } static create_flower(type) { return new Flower(type); } } const rose = Flower.create_flower(\"rose\"); // Works fine","title":"Static Fields"},{"location":"js/es2020/#top-level-await","text":"So far, to await for a promise to finish, a function in which await is used would need to be defined with the async keyword. const func = async () => { const response = await fetch(url) } Unfortunately, if there was a need to await for something in a global scope, it would not be possible, and usually required an immediately invoked function expression (IIFE). (async () => { const response = await fetch(url) })() Thanks to Top Level Await, there is no need for wrapping code in an async function anymore, and this code will work. const response = await fetch(url) This feature could be useful for resolving module dependencies or using a fallback source if the initial one failed. let Vue try { Vue = await import('url_1_to_vue') } catch { Vue = await import('url_2_to_vue) }","title":"Top Level Await"},{"location":"js/es2020/#promiseallsettled","text":"To wait for multiple promises to finish, Promise.all([promise_1, promise_2]) can be used. The problem is that if one of them fails, then an error will be thrown. Nevertheless, there are cases in which it is ok for one of the promises to fail, and the rest should still resolve. To achieve that, ES11 introduced Promise.allSettled. promise_1 = Promise.resolve('hello') primise_2 = new Promise((resolve, reject) => setTimeout(reject, 200, 'problem')) Promise.allSettled([promise_1, promise_2]) .then(([promise_1_result, promise_2_result]) => { console.log(promise_1_result) // {status: 'fulfilled', value: 'hello'} console.log(promise_2_result) // {status: 'rejected', reason: 'problem'} }) A resolved promise will return an object with status and value properties, while rejected ones will have status and reason.","title":"Promise.allSettled"},{"location":"js/es2020/#dynamic-import","text":"You might have used dynamic imports when using webpack for module bundling. Finally, native support for this feature is here. // Alert.js file export default { show() { // Your alert } } // Some other file import('/components/Alert.js') .then(Alert => { Alert.show() }) Considering the fact that a lot applications use module bundlers like webpack for transpiling and optimizing code, this feature isn't such a big deal right now.","title":"Dynamic Import"},{"location":"js/es2020/#matchall","text":"MatchAll is useful for applying the same regular expression to a string if you need to find all matches and get their positions. The match method only returns items that were matched. const regex = /\\b(apple)+\\b/; const fruits = \"pear, apple, banana, apple, orange, apple\"; for (const match of fruits.match(regex)) { console.log(match); } // Output // // 'apple' // 'apple' matchAll in contrast, returns a bit more information, including index of the string found. for (const match of fruits.matchAll(regex)) { console.log(match); } // Output // // [ // 'apple', // 'apple', // index: 6, // input: 'pear, apple, banana, apple, orange, apple', // groups: undefined // ], // [ // 'apple', // 'apple', // index: 21, // input: 'pear, apple, banana, apple, orange, apple', // groups: undefined // ], // [ // 'apple', // 'apple', // index: 36, // input: 'pear, apple, banana, apple, orange, apple', // groups: undefined // ]","title":"MatchAll"},{"location":"js/es2020/#globalthis","text":"JavaScript can run in different environments like browsers or Node.js. A global object in browsers is available under window variable, but in Node it is an object called global. To make it easier to use a global object no matter in which environment code is running, globalThis was introduced. // In a browser window == globalThis // true // In node.js global == globalThis // true","title":"globalThis"},{"location":"js/es2020/#bigint","text":"The maximum number that can be reliably represented in JavaScript is 2^53 - 1. BigInt will allow creation of numbers even bigger than that. const theBiggerNumber = 9007199254740991n const evenBiggerNumber = BigInt(9007199254740991) {!docs/abbreviations.txt!}","title":"BigInt"},{"location":"js/flow/","text":"Flow Annotated JS Flow is a static type checker which checks your code for errors through static type annotations. These types allows to tell Flow how you want your code to work, and Flow will make sure it does work that way. It is an alternative against TypeScript . // @flow function square(n: number): number { return n * n; // normally the error is here } square(\"2\"); // with flow it is reported here Installing Flow This can be setup as extension to babel which will strip of the flow definition on transpiling: npm install --save-dev flow-bin flow-typed babel-preset-flow npm run flow init sudo npm install flow-bin # needed for some IDEs node_modules/.bin/flow-typed install # will download your libraries Also you have to add the preset flow to babel. Now the compiler will strip off all flow extensions and the builded code will work as if it was never there. After adding new modules you should update your flow-typed libraries: node_modules/.bin/flow-typed install --overwrite Using Flow To take advantage of the flow checking you run it using: npm run flow # or if installed globally flow This command may be run automatically from the test or dev task. And some editors like Atom can directly validate and show problems while editing, now. You may also have a look at some Examples from the net. Flow annotations You have to start each file with // @flow as the first comment line. Classes Declare properties It is necessary to define each used property. This not only helps in type analyzation but also to make it more readable: // @flow class SchemaAny { data: any; constructor(data: any) { this.data = data; } } Command concatenation If this should be done, don't define the return type as the current class. Better use this as return type which will also work if a subclass calls methods from it's super class: /* @flow */ class A { x(): this { return this; } } class B extends A { y(): this { return this; } } var w = new B(); w.x().y(); Uncheck by casting with any It's not a very nice solution, but you can cast an object to any. And no unsafe any will be leaked. /* @flow */ class A { get clone(): this { // return Object.assign(Object.create(this), this) // flow error return Object.assign((Object.create(this): any), this); } } class B extends A {} var b = new B(); var c = b.clone; Here the Object.create(this) will normally give an error: Co variant property clone incompatible with contra-variant use in call of method assign. The fix is to cast it by any. {!docs/abbreviations.txt!}","title":"Flow"},{"location":"js/flow/#flow-annotated-js","text":"Flow is a static type checker which checks your code for errors through static type annotations. These types allows to tell Flow how you want your code to work, and Flow will make sure it does work that way. It is an alternative against TypeScript . // @flow function square(n: number): number { return n * n; // normally the error is here } square(\"2\"); // with flow it is reported here","title":"Flow Annotated JS"},{"location":"js/flow/#installing-flow","text":"This can be setup as extension to babel which will strip of the flow definition on transpiling: npm install --save-dev flow-bin flow-typed babel-preset-flow npm run flow init sudo npm install flow-bin # needed for some IDEs node_modules/.bin/flow-typed install # will download your libraries Also you have to add the preset flow to babel. Now the compiler will strip off all flow extensions and the builded code will work as if it was never there. After adding new modules you should update your flow-typed libraries: node_modules/.bin/flow-typed install --overwrite","title":"Installing Flow"},{"location":"js/flow/#using-flow","text":"To take advantage of the flow checking you run it using: npm run flow # or if installed globally flow This command may be run automatically from the test or dev task. And some editors like Atom can directly validate and show problems while editing, now. You may also have a look at some Examples from the net.","title":"Using Flow"},{"location":"js/flow/#flow-annotations","text":"You have to start each file with // @flow as the first comment line.","title":"Flow annotations"},{"location":"js/flow/#classes","text":"","title":"Classes"},{"location":"js/flow/#declare-properties","text":"It is necessary to define each used property. This not only helps in type analyzation but also to make it more readable: // @flow class SchemaAny { data: any; constructor(data: any) { this.data = data; } }","title":"Declare properties"},{"location":"js/flow/#command-concatenation","text":"If this should be done, don't define the return type as the current class. Better use this as return type which will also work if a subclass calls methods from it's super class: /* @flow */ class A { x(): this { return this; } } class B extends A { y(): this { return this; } } var w = new B(); w.x().y();","title":"Command concatenation"},{"location":"js/flow/#uncheck-by-casting-with-any","text":"It's not a very nice solution, but you can cast an object to any. And no unsafe any will be leaked. /* @flow */ class A { get clone(): this { // return Object.assign(Object.create(this), this) // flow error return Object.assign((Object.create(this): any), this); } } class B extends A {} var b = new B(); var c = b.clone; Here the Object.create(this) will normally give an error: Co variant property clone incompatible with contra-variant use in call of method assign. The fix is to cast it by any. {!docs/abbreviations.txt!}","title":"Uncheck by casting with any"},{"location":"js/modules/","text":"JavaScript Modules Development JavaScript Compiler Babel Linting eslint Type Annotations using Flow Hot Reloading with nodemon Test Framework mocha with Chai assertions Code coverage started through nyc CLI calling istanbul Reporting Debug output debug Colors using Chalk Winston - logging Security CASL - Authorization library Local System Execa - optimized working with child processes Networking Axios - Promise based HTTP client request - HTTP client Database mongoose object relartional access to MongoDB Server ExpressJS webserver Feathers REST and realtime API with Authentication Profiler feathers-mongoose as model helper Client FeathersJS Client for server connection Web frameworks Quasar Framework which works with the Vue.js library Vuex for state management feathers-vuex to connect state to server Vuelidate for validation vue-i18n for app translations {!docs/abbreviations.txt!}","title":"Modules"},{"location":"js/modules/#javascript-modules","text":"","title":"JavaScript Modules"},{"location":"js/modules/#development","text":"JavaScript Compiler Babel Linting eslint Type Annotations using Flow Hot Reloading with nodemon Test Framework mocha with Chai assertions Code coverage started through nyc CLI calling istanbul","title":"Development"},{"location":"js/modules/#reporting","text":"Debug output debug Colors using Chalk Winston - logging","title":"Reporting"},{"location":"js/modules/#security","text":"CASL - Authorization library","title":"Security"},{"location":"js/modules/#local-system","text":"Execa - optimized working with child processes","title":"Local System"},{"location":"js/modules/#networking","text":"Axios - Promise based HTTP client request - HTTP client","title":"Networking"},{"location":"js/modules/#database","text":"mongoose object relartional access to MongoDB","title":"Database"},{"location":"js/modules/#server","text":"ExpressJS webserver Feathers REST and realtime API with Authentication Profiler feathers-mongoose as model helper","title":"Server"},{"location":"js/modules/#client","text":"FeathersJS Client for server connection","title":"Client"},{"location":"js/modules/#web-frameworks","text":"Quasar Framework which works with the Vue.js library Vuex for state management feathers-vuex to connect state to server Vuelidate for validation vue-i18n for app translations {!docs/abbreviations.txt!}","title":"Web frameworks"},{"location":"js/nodejs/","text":"Node.JS Node.js is a cross-platform runtime environment for developing server-side Web applications based on the JavaScript language. This runtime is based on Google's V8 JavaScript engine which will interpret and execute the JavaScript. Node.js main features are: event-driven architecture asynchronous I/O suitable for web applications real-time communication handle thousands of concurrent connections great package management big community Release plan Each even number is a long term support version. So software I release is always based on LTS versions: Version EcmaScript Release Maintenance End of Life 6.x End of Life ES2016 2016-04-26 2018-04-30 April 2019 8.x Maintenance LTS ES2017 2017-05-30 April 2019 December 2019 10.x Active LTS ES2018 2018-04-24 April 2020 April 2021 12.x Current Release ES2019 2019-04-23 April 2021 April 2022 14.x Pending ? 2020-04-21 April 2022 April 2023 See node.green to have a look at how far the support of NodeJS goes for each specific element. {!docs/abbreviations.txt!}","title":"Node.JS"},{"location":"js/nodejs/#nodejs","text":"Node.js is a cross-platform runtime environment for developing server-side Web applications based on the JavaScript language. This runtime is based on Google's V8 JavaScript engine which will interpret and execute the JavaScript. Node.js main features are: event-driven architecture asynchronous I/O suitable for web applications real-time communication handle thousands of concurrent connections great package management big community","title":"Node.JS"},{"location":"js/nodejs/#release-plan","text":"Each even number is a long term support version. So software I release is always based on LTS versions: Version EcmaScript Release Maintenance End of Life 6.x End of Life ES2016 2016-04-26 2018-04-30 April 2019 8.x Maintenance LTS ES2017 2017-05-30 April 2019 December 2019 10.x Active LTS ES2018 2018-04-24 April 2020 April 2021 12.x Current Release ES2019 2019-04-23 April 2021 April 2022 14.x Pending ? 2020-04-21 April 2022 April 2023 See node.green to have a look at how far the support of NodeJS goes for each specific element. {!docs/abbreviations.txt!}","title":"Release plan"},{"location":"js/npm/","text":"NodeJS Package Manager Traditionally npm is used as package manager for NodeJS but you may also use yarn package manager which offered more speed till npm V5 (NodeJS 8). Within this book we mostly keep going with the default npm but both works the same. NodeJS But before you need the NodeJS system installed using the instructions on the website. It already comes with the NPM package manager. Which version to use is based on the modules you want to run. But while NodeJS is mostly very good in backward compatibility also over major versions you should to go with the latest stable LTS (long term support) version. Node version manager For development systems you may test different versions so you may want to switch between versions on demand. This is possible using the nvm version manager. This allows you to try out your code in different versions and find problems. Install it using the script: curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.2/install.sh | bash Then you may directly use it. It can install other versions and switch between versions easily. Some of the commands are: Command Usage nvm current display currently activated version nvm ls [<version>] list installed versions nvm ls-remote [<version>] list versions available for install nvm version <version> get best matching installed version nvm version-remote <version> get best matching remote version nvm install <version> download and install a version nvm uninstall <version> uninstall a version nvm use <version> modify path to use version nvm which <version> show path there this version is installed All node installations are done within the ~/.nvm directory. If you detect that the module won't work with specific versions of node you should define the working versions in package.json . Upgrade to new version The following command will not only install a new version but also install all global modules which were installed in the current version. $ nvm current v8.0.0 $ nvm version-remote 8 v8.1.0 $ nvm install 8.1 --reinstall-packages-from=8.0 Downloading and installing node v8.1.0... Downloading https://nodejs.org/dist/v8.1.0/node-v8.1.0-linux-x64.tar.xz... ######################################################################## 100,0% Computing checksum with sha256sum Checksums matched! Now using node v8.1.0 (npm v5.0.3) Reinstalling global packages from v8.0.0... + flow-bin@0.47.0 added 9 packages and updated 1 package in 2.463s Linking global packages from v8.0.0... But after you changed your node version you should call npm install in your module again. NPM npm makes it easy for JavaScript developers to share and reuse code, and it makes it easy to update the code that you're sharing. npm is a package manager for JavaScript a free and open package repository The npm package repository contains hundreds of thousands of packages. Which can be easily installed and included in your code. NPM Installation npm is bundled with NodeJS and installed with it. To update it to the newest version you may call: sudo npm update -g npm See the usage below. New in npm 5 standardized lock-file package-lock.json is supported --save is no longer necessary but --save-dev and --save-optional have to be used if needed speeding up installation by using symlinks to centralized store top level preinstall scripts now run before anything else added prepack and postpack , which will not run on install only on publish prepublishOnly now runs before the archive to publish is created optimized output lots of other fixes and optimization Yarn Yarn is an alternative JavaScript package manager built by Facebook, Google, Exponent and Tilde. Yarn is only a new CLI client that fetches modules from the npm registry. But now with the upcoming npm 5 shipped with Node 8 the differences are smelting down. yarn.lock In package.json , the file where both npm and Yarn keep track of the project\u2019s dependencies, version numbers aren\u2019t always exact. Instead, you can define a range of versions. This way you can choose a specific major and minor version of a package, but allow npm to install the latest patch that might fix some bugs. In an ideal world of semantic versioning, patched releases won\u2019t include any breaking changes. This, unfortunately, is not always true. The strategy employed by npm may result into two machines with the same package.json file, having different versions of a package installed, possibly introducing bugs. To avoid package version mis-matches, an exact installed version is pinned down in a lock file. Every time a module is added, Yarn creates (or updates) a yarn.lock file. This way you can guarantee another machine installs the exact same package, while still having a range of allowed versions defined in package.json. It is automatically working like npm with npm-shrinkwrap.json or npm since V5. Parallel Installation Whenever npm or Yarn needs to install a package, it carries out a series of tasks. In npm, these tasks are executed per package and sequentially, meaning it will wait for a package to be fully installed before moving on to the next. Yarn executes these tasks in parallel, increasing performance. Cleaner Output By default npm is very verbose. For example, it recursively lists all installed packages when running npm install <package> . Yarn on the other hand, isn\u2019t verbose at all. When details can be obtained via other commands, it lists significantly less information with appropriate emojis. Yarn Installation Use npm to install yarn: sudo npm install -g yarn To update Yarn run the same call again. Management Tasks The following tasks are often needed and will be displayed how to do them in both. Other than some functional differences, Yarn also has different commands. Some npm commands were removed, others modified and a couple of interesting commands were added. Install globally # npm call $ sudo npm install -g <package> # yarn call $ sudo yarn global add <package> Unlike npm, where global operations are performed using the -g or --global flag, Yarn commands need to be prefixed with global. Install Module # npm call $ npm install <package> # npm befor v5 needs the save flag $ npm install <package> --save # yarn call $ yarn add <package> This will install dependencies from the package.json file and allows you to add new packages. For development modules use: # npm call $ npm install <package> --save-dev # yarn call $ yarn add <package> --dev Remove Package # npm call $ npm remove <package> --save # yarn call $ yarn remove <package> This will remove the package and the dependencies from the package.json file. Outdated packages # npm call, first set the version in package.json $ npm outdated # yarn call, will ask for new version $ yarn outdated This will list the packages which are outdated. Upgrade package # npm call $ npm update <package> --save # yarn call $ yarn upgrade <package> This command upgrades packages to the latest version conforming to the version rules set in package.json (and recreates yarn.lock ). Interestingly, when specifying a package, it updates that package to latest release and updates the tag defined in package.json . This means this command might update packages to a new major release. Upgrade interactive # npm call $ sudo npm install -g npm-check $ npm-check -u # yarn call $ yarn upgrade-interactive This tool allows you to interactively decide what to upgrade. Run script # npm call, the option -s is optional and suppress the mostly unwanted error hints $ npm run <script> -s # yarn call $ yarn run <script> # yarn alternative if script name not equal yarn commands $ yarn <script> You can run the scripts defined in package.json . With both you can call the binaries located in node_modules/.bin directly without prefixing this path. NPM Login # npm call, first set the version in package.json $ npm login # yarn call, will ask for new version $ yarn login Both will ask and store the credentials to access the npm package repository. Publish # npm call, first set the version in package.json $ npm publish # yarn call, will ask for new version $ yarn publish While npm published the package using the version from package.json , Yarn will interactively ask for it and update package.json for you. You can also group packages in a scope (namespace). Each user has a scope with it's user or organization name. To publish something in this scope you have to use @<scope>/<name> and also need to add --access public because private packages are only valid for payed customers. License Management At the time of writing, no npm equivalent is available. yarn licenses ls lists the licenses of all installed packages. yarn licenses generate-disclaimer generates a disclaimer containing the contents of all licenses of all packages. Some licenses state that you must include the project\u2019s license in your project, making this a rather useful tool to do that. Yarn Analysis This command peeks into the dependency graph and figures out why given package is installed in your project. Perhaps you explicitly added it, perhaps it\u2019s a dependency of a package you installed. yarn why helps you figure that out. {!docs/abbreviations.txt!}","title":"NPM"},{"location":"js/npm/#nodejs-package-manager","text":"Traditionally npm is used as package manager for NodeJS but you may also use yarn package manager which offered more speed till npm V5 (NodeJS 8). Within this book we mostly keep going with the default npm but both works the same.","title":"NodeJS Package Manager"},{"location":"js/npm/#nodejs","text":"But before you need the NodeJS system installed using the instructions on the website. It already comes with the NPM package manager. Which version to use is based on the modules you want to run. But while NodeJS is mostly very good in backward compatibility also over major versions you should to go with the latest stable LTS (long term support) version.","title":"NodeJS"},{"location":"js/npm/#node-version-manager","text":"For development systems you may test different versions so you may want to switch between versions on demand. This is possible using the nvm version manager. This allows you to try out your code in different versions and find problems. Install it using the script: curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.2/install.sh | bash Then you may directly use it. It can install other versions and switch between versions easily. Some of the commands are: Command Usage nvm current display currently activated version nvm ls [<version>] list installed versions nvm ls-remote [<version>] list versions available for install nvm version <version> get best matching installed version nvm version-remote <version> get best matching remote version nvm install <version> download and install a version nvm uninstall <version> uninstall a version nvm use <version> modify path to use version nvm which <version> show path there this version is installed All node installations are done within the ~/.nvm directory. If you detect that the module won't work with specific versions of node you should define the working versions in package.json .","title":"Node version manager"},{"location":"js/npm/#upgrade-to-new-version","text":"The following command will not only install a new version but also install all global modules which were installed in the current version. $ nvm current v8.0.0 $ nvm version-remote 8 v8.1.0 $ nvm install 8.1 --reinstall-packages-from=8.0 Downloading and installing node v8.1.0... Downloading https://nodejs.org/dist/v8.1.0/node-v8.1.0-linux-x64.tar.xz... ######################################################################## 100,0% Computing checksum with sha256sum Checksums matched! Now using node v8.1.0 (npm v5.0.3) Reinstalling global packages from v8.0.0... + flow-bin@0.47.0 added 9 packages and updated 1 package in 2.463s Linking global packages from v8.0.0... But after you changed your node version you should call npm install in your module again.","title":"Upgrade to new version"},{"location":"js/npm/#npm","text":"npm makes it easy for JavaScript developers to share and reuse code, and it makes it easy to update the code that you're sharing. npm is a package manager for JavaScript a free and open package repository The npm package repository contains hundreds of thousands of packages. Which can be easily installed and included in your code.","title":"NPM"},{"location":"js/npm/#npm-installation","text":"npm is bundled with NodeJS and installed with it. To update it to the newest version you may call: sudo npm update -g npm See the usage below.","title":"NPM Installation"},{"location":"js/npm/#new-in-npm-5","text":"standardized lock-file package-lock.json is supported --save is no longer necessary but --save-dev and --save-optional have to be used if needed speeding up installation by using symlinks to centralized store top level preinstall scripts now run before anything else added prepack and postpack , which will not run on install only on publish prepublishOnly now runs before the archive to publish is created optimized output lots of other fixes and optimization","title":"New in npm 5"},{"location":"js/npm/#yarn","text":"Yarn is an alternative JavaScript package manager built by Facebook, Google, Exponent and Tilde. Yarn is only a new CLI client that fetches modules from the npm registry. But now with the upcoming npm 5 shipped with Node 8 the differences are smelting down.","title":"Yarn"},{"location":"js/npm/#yarnlock","text":"In package.json , the file where both npm and Yarn keep track of the project\u2019s dependencies, version numbers aren\u2019t always exact. Instead, you can define a range of versions. This way you can choose a specific major and minor version of a package, but allow npm to install the latest patch that might fix some bugs. In an ideal world of semantic versioning, patched releases won\u2019t include any breaking changes. This, unfortunately, is not always true. The strategy employed by npm may result into two machines with the same package.json file, having different versions of a package installed, possibly introducing bugs. To avoid package version mis-matches, an exact installed version is pinned down in a lock file. Every time a module is added, Yarn creates (or updates) a yarn.lock file. This way you can guarantee another machine installs the exact same package, while still having a range of allowed versions defined in package.json. It is automatically working like npm with npm-shrinkwrap.json or npm since V5.","title":"yarn.lock"},{"location":"js/npm/#parallel-installation","text":"Whenever npm or Yarn needs to install a package, it carries out a series of tasks. In npm, these tasks are executed per package and sequentially, meaning it will wait for a package to be fully installed before moving on to the next. Yarn executes these tasks in parallel, increasing performance.","title":"Parallel Installation"},{"location":"js/npm/#cleaner-output","text":"By default npm is very verbose. For example, it recursively lists all installed packages when running npm install <package> . Yarn on the other hand, isn\u2019t verbose at all. When details can be obtained via other commands, it lists significantly less information with appropriate emojis.","title":"Cleaner Output"},{"location":"js/npm/#yarn-installation","text":"Use npm to install yarn: sudo npm install -g yarn To update Yarn run the same call again.","title":"Yarn Installation"},{"location":"js/npm/#management-tasks","text":"The following tasks are often needed and will be displayed how to do them in both. Other than some functional differences, Yarn also has different commands. Some npm commands were removed, others modified and a couple of interesting commands were added.","title":"Management Tasks"},{"location":"js/npm/#install-globally","text":"# npm call $ sudo npm install -g <package> # yarn call $ sudo yarn global add <package> Unlike npm, where global operations are performed using the -g or --global flag, Yarn commands need to be prefixed with global.","title":"Install globally"},{"location":"js/npm/#install-module","text":"# npm call $ npm install <package> # npm befor v5 needs the save flag $ npm install <package> --save # yarn call $ yarn add <package> This will install dependencies from the package.json file and allows you to add new packages. For development modules use: # npm call $ npm install <package> --save-dev # yarn call $ yarn add <package> --dev","title":"Install Module"},{"location":"js/npm/#remove-package","text":"# npm call $ npm remove <package> --save # yarn call $ yarn remove <package> This will remove the package and the dependencies from the package.json file.","title":"Remove Package"},{"location":"js/npm/#outdated-packages","text":"# npm call, first set the version in package.json $ npm outdated # yarn call, will ask for new version $ yarn outdated This will list the packages which are outdated.","title":"Outdated packages"},{"location":"js/npm/#upgrade-package","text":"# npm call $ npm update <package> --save # yarn call $ yarn upgrade <package> This command upgrades packages to the latest version conforming to the version rules set in package.json (and recreates yarn.lock ). Interestingly, when specifying a package, it updates that package to latest release and updates the tag defined in package.json . This means this command might update packages to a new major release.","title":"Upgrade package"},{"location":"js/npm/#upgrade-interactive","text":"# npm call $ sudo npm install -g npm-check $ npm-check -u # yarn call $ yarn upgrade-interactive This tool allows you to interactively decide what to upgrade.","title":"Upgrade interactive"},{"location":"js/npm/#run-script","text":"# npm call, the option -s is optional and suppress the mostly unwanted error hints $ npm run <script> -s # yarn call $ yarn run <script> # yarn alternative if script name not equal yarn commands $ yarn <script> You can run the scripts defined in package.json . With both you can call the binaries located in node_modules/.bin directly without prefixing this path.","title":"Run script"},{"location":"js/npm/#npm-login","text":"# npm call, first set the version in package.json $ npm login # yarn call, will ask for new version $ yarn login Both will ask and store the credentials to access the npm package repository.","title":"NPM Login"},{"location":"js/npm/#publish","text":"# npm call, first set the version in package.json $ npm publish # yarn call, will ask for new version $ yarn publish While npm published the package using the version from package.json , Yarn will interactively ask for it and update package.json for you. You can also group packages in a scope (namespace). Each user has a scope with it's user or organization name. To publish something in this scope you have to use @<scope>/<name> and also need to add --access public because private packages are only valid for payed customers.","title":"Publish"},{"location":"js/npm/#license-management","text":"At the time of writing, no npm equivalent is available. yarn licenses ls lists the licenses of all installed packages. yarn licenses generate-disclaimer generates a disclaimer containing the contents of all licenses of all packages. Some licenses state that you must include the project\u2019s license in your project, making this a rather useful tool to do that.","title":"License Management"},{"location":"js/npm/#yarn-analysis","text":"This command peeks into the dependency graph and figures out why given package is installed in your project. Perhaps you explicitly added it, perhaps it\u2019s a dependency of a package you installed. yarn why helps you figure that out. {!docs/abbreviations.txt!}","title":"Yarn Analysis"},{"location":"js/spec/","text":"JavaScript JavaScript is a scripting language which grows out of the Netscape Browser. It is now over 20 years old and since it started as a browser language in one browser it grew to the ultimate web scripting language supported in all browsers. In the last years the Node.JS project brought JavaScript to a general language usable on the server or desktop. The core is standardized as ECMAScript in different Versions. The language itself was kept the same for some years but just now it develops further and get more modern in ECMAScript6 or ECMAScript7. Read more on modern JavaScript under ES.Next . As of now, JavaScript can be used everywhere not only in the browser but also on the server using Node.JS . Specification JavaScript is an interpreter language with a C-like syntax. It is a dynamic typed language in which variables point to values which are associated to types. It is object driven in which most things are objects which are associative arrays with prototypes. Prototypes are used for inheritance instead of classes. But class based features could be simulated through prototypes, too. The language is event driven and it's core is an event engine through which the functions will be called. This gives the possibility of doing things in parallel within one process while waiting for external data. Basics Statements : Within JavaScript statements will end with a ; or at the end of line (in most cases). Comments : One liners with double slash // or multiline with slash and an asterisk /*...*/ . Type Conversions : By calling the Type Class with the value: String(value) also automatic type conversion will be used. DataTypes : Number, String, Boolean, Object, Symbol Arrays : Arrays are a special Object Variables : Use let or var to declare them. String concatenation : Use + to combine two strings into a new one. Comparisons : Strict equal is done by === or negative by !== Tenary operator : Supports also ? Books I can not give any recommendation because there are so much of them and I never really learned from an book but grew with the language and it's community. You can find them in book stores or also online. If you know the language a reference like: Mozilla Developer Network W3 School And as always if you don't know how StackOverflow is your friend. {!docs/abbreviations.txt!}","title":"Specification"},{"location":"js/spec/#javascript","text":"JavaScript is a scripting language which grows out of the Netscape Browser. It is now over 20 years old and since it started as a browser language in one browser it grew to the ultimate web scripting language supported in all browsers. In the last years the Node.JS project brought JavaScript to a general language usable on the server or desktop. The core is standardized as ECMAScript in different Versions. The language itself was kept the same for some years but just now it develops further and get more modern in ECMAScript6 or ECMAScript7. Read more on modern JavaScript under ES.Next . As of now, JavaScript can be used everywhere not only in the browser but also on the server using Node.JS .","title":"JavaScript"},{"location":"js/spec/#specification","text":"JavaScript is an interpreter language with a C-like syntax. It is a dynamic typed language in which variables point to values which are associated to types. It is object driven in which most things are objects which are associative arrays with prototypes. Prototypes are used for inheritance instead of classes. But class based features could be simulated through prototypes, too. The language is event driven and it's core is an event engine through which the functions will be called. This gives the possibility of doing things in parallel within one process while waiting for external data.","title":"Specification"},{"location":"js/spec/#basics","text":"Statements : Within JavaScript statements will end with a ; or at the end of line (in most cases). Comments : One liners with double slash // or multiline with slash and an asterisk /*...*/ . Type Conversions : By calling the Type Class with the value: String(value) also automatic type conversion will be used. DataTypes : Number, String, Boolean, Object, Symbol Arrays : Arrays are a special Object Variables : Use let or var to declare them. String concatenation : Use + to combine two strings into a new one. Comparisons : Strict equal is done by === or negative by !== Tenary operator : Supports also ?","title":"Basics"},{"location":"js/spec/#books","text":"I can not give any recommendation because there are so much of them and I never really learned from an book but grew with the language and it's community. You can find them in book stores or also online. If you know the language a reference like: Mozilla Developer Network W3 School And as always if you don't know how StackOverflow is your friend. {!docs/abbreviations.txt!}","title":"Books"},{"location":"js/typescript/","text":"TypeScript TypeScript is a superset of JavaScript which primarily provides optional static typing, classes and interfaces. One of the big benefits is to enable IDEs to provide a richer environment for spotting common errors as you type the code. Instead of only adding types to standard JavaScript like flow does TypeScript is a language of its own. It is aimed for large projects to get more robust software, while still being deployable as regular JavaScript. I for myself find TypeScript better supported, better community and faster developing so after my first steps with flow+babel I switched to TypeScript. Installation First you need to install TypeScript: npm install typescript ts-node @types/node --save-dev To make it usable you also have to add a configuration file tsconfig.json in your project root: { \"compilerOptions\": { \"target\": \"es6\", \"module\": \"commonjs\", \"outDir\": \"dist\", \"declaration\": true, \"sourceMap\": true, \"removeComments\": true, \"strict\": true, \"noImplicitAny\": true, \"strictNullChecks\": true, \"noImplicitThis\": true, \"alwaysStrict\": true, \"noUnusedLocals\": true, \"noUnusedParameters\": true, \"noImplicitReturns\": true, \"noFallthroughCasesInSwitch\": true }, \"files\": [ \"./node_modules/@types/mocha/index.d.ts\", \"./node_modules/@types/node/index.d.ts\" ], \"include\": [\"src/**/*.ts\"], \"exclude\": [\"node_modules\"] } Usage Now you can integrate it in your package.json : { \"scripts\": { \"lint\": \"tslint -c tslint.json 'src/**/*.ts'\", \"test\": \"mocha -r ts-node/register test/mocha/**.ts\", \"coverage\": \"nyc mocha\", \"dev\": \"NODE_ENV=development nodemon --exec ./node_modules/.bin/ts-node -- ./src/index.ts\", \"build\": \"tsc\", \"start\": \"node dist/index\", \"prepublish\": \"npm run build\" }, \"engines\": { \"node\": \">=6\" } } Also specify the engines information to help people use your module in the right version. This defines the transformation to: Be used as JIT compiler in dev mode (run using npm run dev ) Be converted into ES6 code by running npm run build (into dist folder) Everything in dist folder can be used without TypeScript Before publishing to npm the build command is called automatically Test In testing typescript private members and functions are not accessible. You will get a compile error. Netherless it will be accessible in the generated JavaScript at the moment. But for testing it is often needed to access them. This should be done by changing the source code without really making the generated code complexer. To do this I implemented it in the following solution. Use the access modifiers as follows: public - completely accessible protected - things which should be available in extended classes or test private - something you only need in the class As you see I use the protected for something needed in test like: class Work { protected load: integer constructor() { this.load = 1 } } exports default Work Now to access the load property I use a wrapper class within the test folder (so it will not go into the dist): import * as Work from \"../../src/Work\" class TWork extends Work { public getLoad(): integer { return this.load } } exports default TWork Now within the test I use the added getLoad() method: import * TWork from \"../wrapper/TWork\" const work = new TWork() expect(work.getLoad()).to.equal(1) The directory structure used looks like: src/Work.ts test/ mocha/test.ts wrapper/TWork.ts dist/Work.js {!docs/abbreviations.txt!}","title":"TypeScript"},{"location":"js/typescript/#typescript","text":"TypeScript is a superset of JavaScript which primarily provides optional static typing, classes and interfaces. One of the big benefits is to enable IDEs to provide a richer environment for spotting common errors as you type the code. Instead of only adding types to standard JavaScript like flow does TypeScript is a language of its own. It is aimed for large projects to get more robust software, while still being deployable as regular JavaScript. I for myself find TypeScript better supported, better community and faster developing so after my first steps with flow+babel I switched to TypeScript.","title":"TypeScript"},{"location":"js/typescript/#installation","text":"First you need to install TypeScript: npm install typescript ts-node @types/node --save-dev To make it usable you also have to add a configuration file tsconfig.json in your project root: { \"compilerOptions\": { \"target\": \"es6\", \"module\": \"commonjs\", \"outDir\": \"dist\", \"declaration\": true, \"sourceMap\": true, \"removeComments\": true, \"strict\": true, \"noImplicitAny\": true, \"strictNullChecks\": true, \"noImplicitThis\": true, \"alwaysStrict\": true, \"noUnusedLocals\": true, \"noUnusedParameters\": true, \"noImplicitReturns\": true, \"noFallthroughCasesInSwitch\": true }, \"files\": [ \"./node_modules/@types/mocha/index.d.ts\", \"./node_modules/@types/node/index.d.ts\" ], \"include\": [\"src/**/*.ts\"], \"exclude\": [\"node_modules\"] }","title":"Installation"},{"location":"js/typescript/#usage","text":"Now you can integrate it in your package.json : { \"scripts\": { \"lint\": \"tslint -c tslint.json 'src/**/*.ts'\", \"test\": \"mocha -r ts-node/register test/mocha/**.ts\", \"coverage\": \"nyc mocha\", \"dev\": \"NODE_ENV=development nodemon --exec ./node_modules/.bin/ts-node -- ./src/index.ts\", \"build\": \"tsc\", \"start\": \"node dist/index\", \"prepublish\": \"npm run build\" }, \"engines\": { \"node\": \">=6\" } } Also specify the engines information to help people use your module in the right version. This defines the transformation to: Be used as JIT compiler in dev mode (run using npm run dev ) Be converted into ES6 code by running npm run build (into dist folder) Everything in dist folder can be used without TypeScript Before publishing to npm the build command is called automatically","title":"Usage"},{"location":"js/typescript/#test","text":"In testing typescript private members and functions are not accessible. You will get a compile error. Netherless it will be accessible in the generated JavaScript at the moment. But for testing it is often needed to access them. This should be done by changing the source code without really making the generated code complexer. To do this I implemented it in the following solution. Use the access modifiers as follows: public - completely accessible protected - things which should be available in extended classes or test private - something you only need in the class As you see I use the protected for something needed in test like: class Work { protected load: integer constructor() { this.load = 1 } } exports default Work Now to access the load property I use a wrapper class within the test folder (so it will not go into the dist): import * as Work from \"../../src/Work\" class TWork extends Work { public getLoad(): integer { return this.load } } exports default TWork Now within the test I use the added getLoad() method: import * TWork from \"../wrapper/TWork\" const work = new TWork() expect(work.getLoad()).to.equal(1) The directory structure used looks like: src/Work.ts test/ mocha/test.ts wrapper/TWork.ts dist/Work.js {!docs/abbreviations.txt!}","title":"Test"},{"location":"lang/","text":"Coding Languages This part will work as an introduction to each of the used languages. It gives you a short introduction but to fully understand the language you should also read the referenced documents and try it out some days. I used all of the following languages but neither do I currently use them on and on. See my Blog for where my programming activities are. Which of the languages to use depends on the project and its goals. For the time of writing my preferences are JavaScript for client and for server if development speed matters or Rust if stability and performance matters. The Languages are parted into: Markup and Templates Markdown Handlebars Sass Scripting Languages I use interpreted languages for fast solutions: Bash JavaScript TypeScript which is an optimization of JavaScript CoffeeScript But then a project get's really big, it's attractive veneer quickly washes away. Compiler Languages Here the compiler languages are coming back: Go Rust You need more time to write code but it comes out as more robust and production ready solutions. Java is missing here completely while I know it and also made a big project with it I believe that this language is neither the best solution. Often such applications need a lot of resources are not really fast and often a nightmare to find problems with NullPointer within. It's not generally this way but the code I had to manage tends this way. Comparison Keep in mind that comparisons are always wrong because they only look at one very specific setup. But they may show some differences in technology. Web server As I want to work on a web project which needs a stable and fast web server I made an application serving a static HTML file in different languages. I did not program everything on my own but used the most popular and stable modules for all languages. As a result I compare them against a plain Apache 2.4 installation. To make it comparable all serve the same very simple HTML file which they will found in their base directory. I did run all three directly after each other on the same machine (i7 4CPU Cores, 8GB RAM). Apache 2.4 $ wrk -t4 -c400 -d30s http://127.0.0.1 Running 30s test @ http://127.0.0.1 4 threads and 400 connections Thread Stats Avg Stdev Max +/- Stdev Latency 3.14ms 10.29ms 346.49ms 98.67% Req/Sec 3.51k 2.97k 13.96k 63.27% 363271 requests in 30.04s, 115.78MB read Requests/sec: 12094.09 Transfer/sec: 3.85MB load average: 8,36, 4,16, 2,35 Go using net/http $ wrk -t4 -c400 -d30s http://127.0.0.1:8080 Running 30s test @ http://127.0.0.1:8080 4 threads and 400 connections Thread Stats Avg Stdev Max +/- Stdev Latency 24.62ms 42.73ms 634.52ms 89.12% Req/Sec 12.86k 3.38k 25.59k 75.25% 1537844 requests in 30.09s, 393.03MB read Non-2xx or 3xx responses: 467 Requests/sec: 51104.56 Transfer/sec: 13.06MB load average: 4,69, 3,83, 2,37 Rust using Iron $ wrk -t4 -c400 -d30s http://127.0.0.1:8080 Running 30s test @ http://127.0.0.1:8080 4 threads and 400 connections Thread Stats Avg Stdev Max +/- Stdev Latency 406.52us 463.76us 72.85ms 95.36% Req/Sec 39.20k 36.83k 78.90k 55.17% 2340369 requests in 30.06s, 412.91MB read Requests/sec: 77864.45 Transfer/sec: 13.74MB load average: 14,28, 6,16, 3,25 NodeJS using http-server Running 30s test @ http://127.0.0.1:8080 4 threads and 400 connections Thread Stats Avg Stdev Max +/- Stdev Latency 67.42ms 12.20ms 300.07ms 74.27% Req/Sec 1.47k 417.43 2.02k 45.78% 174900 requests in 30.05s, 56.04MB read Requests/sec: 5820.36 Transfer/sec: 1.87MB load average: 1.41 0.93 0.97 Apache did a low number of requests, Go was about 4 times faster but failed for some requests. The most requests were answered by Rust with no errors. Also the latency was very low but the system on full load. NodeJS as scripting language got the lowest number of request per sec but working only on one CPU it should also be better than apache if 4 cores will be used in a cluster mode. But the system load was the lowest, too. So if performance matters Rust is the best to go but if only the pure static delivery is needed Nginx & Co should be checked against an own implementation. See a complete web server benchmarking . {!docs/abbreviations.txt!}","title":"Overview"},{"location":"lang/#coding-languages","text":"This part will work as an introduction to each of the used languages. It gives you a short introduction but to fully understand the language you should also read the referenced documents and try it out some days. I used all of the following languages but neither do I currently use them on and on. See my Blog for where my programming activities are. Which of the languages to use depends on the project and its goals. For the time of writing my preferences are JavaScript for client and for server if development speed matters or Rust if stability and performance matters. The Languages are parted into:","title":"Coding Languages"},{"location":"lang/#markup-and-templates","text":"Markdown Handlebars Sass","title":"Markup and Templates"},{"location":"lang/#scripting-languages","text":"I use interpreted languages for fast solutions: Bash JavaScript TypeScript which is an optimization of JavaScript CoffeeScript But then a project get's really big, it's attractive veneer quickly washes away.","title":"Scripting Languages"},{"location":"lang/#compiler-languages","text":"Here the compiler languages are coming back: Go Rust You need more time to write code but it comes out as more robust and production ready solutions. Java is missing here completely while I know it and also made a big project with it I believe that this language is neither the best solution. Often such applications need a lot of resources are not really fast and often a nightmare to find problems with NullPointer within. It's not generally this way but the code I had to manage tends this way.","title":"Compiler Languages"},{"location":"lang/#comparison","text":"Keep in mind that comparisons are always wrong because they only look at one very specific setup. But they may show some differences in technology.","title":"Comparison"},{"location":"lang/#web-server","text":"As I want to work on a web project which needs a stable and fast web server I made an application serving a static HTML file in different languages. I did not program everything on my own but used the most popular and stable modules for all languages. As a result I compare them against a plain Apache 2.4 installation. To make it comparable all serve the same very simple HTML file which they will found in their base directory. I did run all three directly after each other on the same machine (i7 4CPU Cores, 8GB RAM). Apache 2.4 $ wrk -t4 -c400 -d30s http://127.0.0.1 Running 30s test @ http://127.0.0.1 4 threads and 400 connections Thread Stats Avg Stdev Max +/- Stdev Latency 3.14ms 10.29ms 346.49ms 98.67% Req/Sec 3.51k 2.97k 13.96k 63.27% 363271 requests in 30.04s, 115.78MB read Requests/sec: 12094.09 Transfer/sec: 3.85MB load average: 8,36, 4,16, 2,35 Go using net/http $ wrk -t4 -c400 -d30s http://127.0.0.1:8080 Running 30s test @ http://127.0.0.1:8080 4 threads and 400 connections Thread Stats Avg Stdev Max +/- Stdev Latency 24.62ms 42.73ms 634.52ms 89.12% Req/Sec 12.86k 3.38k 25.59k 75.25% 1537844 requests in 30.09s, 393.03MB read Non-2xx or 3xx responses: 467 Requests/sec: 51104.56 Transfer/sec: 13.06MB load average: 4,69, 3,83, 2,37 Rust using Iron $ wrk -t4 -c400 -d30s http://127.0.0.1:8080 Running 30s test @ http://127.0.0.1:8080 4 threads and 400 connections Thread Stats Avg Stdev Max +/- Stdev Latency 406.52us 463.76us 72.85ms 95.36% Req/Sec 39.20k 36.83k 78.90k 55.17% 2340369 requests in 30.06s, 412.91MB read Requests/sec: 77864.45 Transfer/sec: 13.74MB load average: 14,28, 6,16, 3,25 NodeJS using http-server Running 30s test @ http://127.0.0.1:8080 4 threads and 400 connections Thread Stats Avg Stdev Max +/- Stdev Latency 67.42ms 12.20ms 300.07ms 74.27% Req/Sec 1.47k 417.43 2.02k 45.78% 174900 requests in 30.05s, 56.04MB read Requests/sec: 5820.36 Transfer/sec: 1.87MB load average: 1.41 0.93 0.97 Apache did a low number of requests, Go was about 4 times faster but failed for some requests. The most requests were answered by Rust with no errors. Also the latency was very low but the system on full load. NodeJS as scripting language got the lowest number of request per sec but working only on one CPU it should also be better than apache if 4 cores will be used in a cluster mode. But the system load was the lowest, too. So if performance matters Rust is the best to go but if only the pure static delivery is needed Nginx & Co should be checked against an own implementation. See a complete web server benchmarking . {!docs/abbreviations.txt!}","title":"Web server"},{"location":"lang/bash/","text":"Bash The bash shell is the most common shell in Linux/Unix systems and also called the GNU shell. It is a superset of the sh shell, so everything that works there also works in bash . But bash is not only a shell, it is also a powerful scripting language but for the higher level techniques a good understanding of the system and the language is needed. With complex data structures it gets a little cryptic as long as you don't know the syntax. Here the language structure and special use cases will be explained. Script vs Shell Everything which is possible in a script can also be directly typed within the shell. That allows to debug a code by running each line one by one in the console. When the program being executed is a shell script, bash will create a new bash process using a fork. This sub shell reads the lines from the shell script one line at a time. Commands on each line are read, interpreted and executed as if they would have come directly from the keyboard. Don't change the file contents while a script is running, because this can influence the running program and tend to incalculable results. Syntax Comments: use # to start a comment Commands: calls the function, built-in or within the PATH Arguments: separated by spaces behind the command Exits Status: is the integer return value which is 0 for success: exit codes Shebang: #!/bin/bash as first line is used to run script as bash if executed Redirection and Pipes There are three file descriptors: STDIN (0), STDOUT (1) and STDERR (2) which can be used. They can be directed into a file or piped as STDIN to the next command. Redirection Code Example STDOUT to file ls -l > ls-l.txt STDERR to file grep da * 2> grep-errors.txt STDOUT to STDERR grep da * 1>&2 STDERR to STDOUT grep * 2>&1 STDOUT and STDERR to file grep da * &> grep-errors.txt pipe STDOUT ls -l | grep \"\\.txt$\" pipe STDOUT and STDERR ls -l |& grep \"\\.txt$\" pipe only STDERR ls -l >/dev/null 2>&1 | grep \"\\.txt$\" Variables A variable is created by setting a value to it like test=\"xxxx\" . They are only accessible in the current shell. To make them also available to sub shells you have to export it. The following types of variables are possible: String variables Integer variables Constant variables Array variables Within functions local variables can be defined by using the local prefix. The following variables have special use cases: Variable Usage $* Expands to the positional parameters, starting from one. When the expansion occurs within double quotes, it expands to a single word with the value of each parameter separated by the first character of the IFS special variable. $@ Expands to the positional parameters, starting from one. When the expansion occurs within double quotes, each parameter expands to a separate word. $# Expands to the number of positional parameters in decimal. $? Expands to the exit status of the most recently executed foreground pipeline. $- A hyphen expands to the current option flags as specified upon invocation, by the set built-in command, or those set by the shell itself (such as the -i). $$ Expands to the process ID of the shell. $! Expands to the process ID of the most recently executed background (asynchronous) command. $0 Expands to the name of the shell or shell script. $_ The underscore variable is set at shell startup and contains the absolute file name of the shell or script being executed as passed in the argument list. Subsequently, it expands to the last argument to the previous command, after expansion. It is also set to the full pathname of each command executed and placed in the environment exported to that command. When checking mail, this parameter holds the name of the mail file. CDPATH A colon-separated list of directories used as a search path for the cd command. HOME The current user's home directory; the default for cd . The value of this variable is also used by tilde expansion. IFS A list of characters that separate fields; used when the shell splits words as part of expansion. MAIL If this parameter is set to a file name and the MAILPATH variable is not set, Bash informs the user of the arrival of mail in the specified file. MAILPATH A colon-separated list of file names which the shell periodically checks for new mail. OPTARG The value of the last option argument processed by getopts . OPTIND The index of the last option argument processed by getopts . PATH A colon-separated list of directories in which the shell looks for commands. PS1 The primary prompt string. The default value is '\\s-\\v\\$ ' . PS2 The secondary prompt string. The default value is '> ' . auto_resume This variable controls how the shell interacts with the user and job control. BASH The full pathname used to execute the current instance of Bash. BASH_ENV If this variable is set when Bash is invoked to execute a shell script, its value is expanded and used as the name of a startup file to read before executing the script. BASH_VERSION The version number of the current instance of Bash. BASH_VERSINFO A read-only array variable whose members hold version information for this instance of Bash. COLUMNS Used by the select built-in to determine the terminal width when printing selection lists. COMP_CWORD An index of the word containing the current cursor position. COMP_LINE The current command line. COMP_POINT The index of the current cursor position relative to the beginning of the current command. COMP_WORDS An array variable consisting of the individual words in the current command line. COMPREPLY An array variable from which Bash reads the possible completions generated by a shell function invoked by the programmable completion facility. DIRSTACK An array variable containing the current contents of the directory stack. EUID The numeric effective user ID of the current user. FCEDIT The editor used as a default by the -e option to the fc command. FIGNORE A colon-separated list of suffixes to ignore when performing file name completion. FUNCNAME The name of any currently-executing shell function. GLOBIGNORE A colon-separated list of patterns defining the set of file names to be ignored by file name expansion. GROUPS An array variable containing the list of groups of which the current user is a member. histchars Up to three characters which control history expansion, quick substitution, and tokenization. HISTCMD The history number, or index in the history list, of the current command. HISTCONTROL Defines whether a command is added to the history file. HISTFILE The name of the file to which the command history is saved. The default value is ~/.bash_history . HISTFILESIZE The maximum number of lines contained in the history file, defaults to 500. HISTIGNORE A colon-separated list of patterns used to decide which command lines should be saved in the history list. HISTSIZE The maximum number of commands to remember on the history list, default is 500. HOSTFILE Contains the name of a file in the same format as /etc/hosts that should be read when the shell needs to complete a hostname. HOSTNAME The name of the current host. HOSTTYPE A string describing the machine Bash is running on. IGNOREEOF Controls the action of the shell on receipt of an EOF character as the sole input. INPUTRC The name of the Readline initialization file, overriding the default /etc/inputrc . LANG Used to determine the locale category for any category not specifically selected with a variable starting with LC_ . LC_ALL This variable overrides the value of LANG and any other LC_ variable specifying a locale category. LC_COLLATE This variable determines the collation order used when sorting the results. LC_CTYPE This variable determines the interpretation of characters and the behavior of character classes within file name expansion and pattern matching. LC_MESSAGES This variable determines the locale used to translate double-quoted strings preceded by a \"$\" sign. LC_NUMERIC This variable determines the locale category used for number formatting. LINENO The line number in the script or shell function currently executing. LINES Used by the select built-in to determine the column length for printing selection lists. MACHTYPE A string that fully describes the system type on which Bash is executing, in the standard GNU CPU-COMPANY-SYSTEM format. MAILCHECK How often (in seconds) that the shell should check for mail in the files specified in the MAILPATH or MAIL variables. OLDPWD The previous working directory as set by the cd built-in. OPTERR If set to the value 1, Bash displays error messages generated by getopts. OSTYPE A string describing the operating system Bash is running on. PIPESTATUS An array variable containing a list of exit status values from the processes in the most recently executed foreground pipeline. POSIXLY_CORRECT If this variable is in the environment when bash starts, the shell enters POSIX mode. PPID The process ID of the shell's parent process. PROMPT_COMMAND If set, the value is interpreted as a command to execute before the printing of each primary prompt (PS1). PS3 The value of this variable is used as the prompt for the select command. Defaults to '#? ' PS4 The value is the prompt printed before the command line is echoed when the -x option is set; defaults to '+ ' . PWD The current working directory as set by the cd command. RANDOM Each time this parameter is referenced, a random integer between 0 and 32767 is generated. Assigning a value to this variable seeds the random number generator. REPLY The default variable for the read built-in. SECONDS This variable expands to the number of seconds since the shell was started. SHELLOPTS A colon-separated list of enabled shell options. SHLVL Incremented by one each time a new instance of Bash is started. TIMEFORMAT The value of this parameter is used as a format string specifying how the timing information for pipelines prefixed with the time reserved word should be displayed. TMOUT If set to a value greater than zero, TMOUT is treated as the default timeout for the read built-in. In an interative shell, the value is interpreted as the number of seconds to wait for input after issuing the primary prompt when the shell is interactive. UID The numeric, real user ID of the current user. Functions To define functions two types are possible: function quit { exit } hello () { echo Hello! } hello quit Parameters given to the functions are readable as $1 ... like from any command. Expansion Brace expansion is a mechanism by which arbitrary strings may be generated. Patterns to be brace-expanded take the form of an optional PREAMBLE, followed by a series of comma-separated strings between a pair of braces, followed by an optional POSTSCRIPT. The preamble is prefixed to each string contained within the braces, and the postscript is then appended to each resulting string, expanding left to right. Brace expansions may be nested. The results of each expanded string are not sorted; left to right order is preserved: $ echo sp{el,il,al}l spell spill spall Variable expansion will replace a variable with it's content, Command substitution will replace a command with it's output before evaluating the outer line. $(command) # backticks alternative `command` Arithmetic expansion allows the evaluation of an arithmetic expression and the substitution of the result. The format for arithmetic expansion is: $(( EXPRESSION )) The expression is treated as if it were within double quotes. All tokens in the expression undergo parameter expansion, command substitution, and quote removal. Arithmetic substitutions may be nested. Evaluation of arithmetic expressions is done in fixed-width integers with no check for overflow - although division by zero is trapped and recognized as an error. The operators are roughly the same as in the C programming language. In order of decreasing precedence, the list looks like this: Operator Meaning VAR++ and VAR-- variable post-increment and post-decrement ++VAR and --VAR variable pre-increment and pre-decrement - and + unary minus and plus ! and ~ logical and bitwise negation ** exponentiation * , / and % multiplication, division, remainder + and - addition, subtraction << and >> left and right bitwise shifts <= , >= , < and > comparison operators == and != equality and inequality & bitwise AND ^ bitwise exclusive OR | bitwise OR && logical AND || logical OR expr ? expr : expr conditional evaluation = , *= , /= , %= , += , -= , <<= , >>= , &= , ^= and |= assignments , separator between expressions File name expansion is done using * , ? , and [ . If one of these characters appears, then the word is regarded as a PATTERN , and replaced with an alphabetically sorted list of file names matching the pattern. If no matching file names are found, and the shell option nullglob is disabled, the word is left unchanged. Conditionals if [ \"foo\" = \"foo\" ]; then echo expression evaluated as true else echo expression evaluated as false fi Test Conditions In Bash there are different extensions to write conditions: [ ... ] is POSIX command [[ ... ]] is a Bash extension [[ X ]] is a single construct that makes X be parsed magically. < , && , || and () are treated specially, and word splitting rules are different. There are also further differences like = and =~ . Type POSIX Bash < [ a \\< b ] comparison needs \\ else redirects output [[ a < b ]] && or || [ a = a ] && [ b = b ] deprecated: [ a = a -a b = b ] [[ a = a && b = b ]] ( ([ a = a ] || [ a = b ]) && [ a = b ] deprecated: [ \\( a = a -o a = b \\) -a a = b ] [[ (a = a || a = b) && a = b ]] word splitting x='a b' [ \"$x\" = 'a b' ] quotes needed to prevent expand [[ $x = 'a b' ]] = or == printf 'ab' | grep -Eq 'a.' [[ ab = a? ]] pattern matching applies =~ or !~ printf 'ab' | grep -Eq 'ab?' [[ ab =~ ab? ]] POSIX regexp As you see the bash expressions are often better readable. Regular Expressions Operator Effect . Matches any single character. ? The preceding item is optional and will be matched, at most, once. * The preceding item will be matched zero or more times. + The preceding item will be matched one or more times. {N} The preceding item is matched exactly N times. {N,} The preceding item is matched N or more times. {N,M} The preceding item is matched at least N times, but not more than M times. - represents the range if it's not first or last in a list or the ending point of a range in a list. ^ Matches the empty string at the beginning of a line; also represents the characters not in the range of a list. $ Matches the empty string at the end of a line. \\b Matches the empty string at the edge of a word. \\B Matches the empty string provided it's not at the edge of a word. \\< Match the empty string at the beginning of word. Loops Bash has three different kind of loops: The for loop is a little bit different from other programming languages. Basically, it let's you iterate over a series of 'words' within a string. The while executes a piece of code if the control expression is true , and only stops when it is false . The until loop is almost equal to the while loop, except that the code is executed while the control expression evaluates to false . Each loop may be exited using break or continued with the next round using continue . for i in $( ls ); do echo item: $i done The variable i will take the different values contained in $( ls ) . The block enclosed in do and done is executed in each iteration. To use for as a counter use the range support of bash: for i in {1..10}; do echo $i done But you can also use a C-like for loop: for (( c=1; c<=5; c++ )); do echo \"Welcome $c times\" done In while and until you give the condition at the start and use a code block like used in for : COUNTER=0 while [ $COUNTER -lt 10 ]; do echo The counter is $COUNTER let COUNTER+=1 done COUNTER=20 until [ $COUNTER -lt 10 ]; do echo COUNTER $COUNTER let COUNTER-=1 done Options Shell options can either be set different from the default upon calling the shell, or be set during shell operation. To change an option use set -<option> to enable or set +<option> to disable an option. Option Usage noclobber prevents existing files from being overwritten by redirection operations noglob prevents special characters from being expanded u treat unset variables as an error x set debug mode which will print each expanded command before executing Include modules Using the source <path> command the given script will be included like it is directly written there the source command is set. A relative path is based on the current directory of the calling shell. {!docs/abbreviations.txt!}","title":"Bash"},{"location":"lang/bash/#bash","text":"The bash shell is the most common shell in Linux/Unix systems and also called the GNU shell. It is a superset of the sh shell, so everything that works there also works in bash . But bash is not only a shell, it is also a powerful scripting language but for the higher level techniques a good understanding of the system and the language is needed. With complex data structures it gets a little cryptic as long as you don't know the syntax. Here the language structure and special use cases will be explained.","title":"Bash"},{"location":"lang/bash/#script-vs-shell","text":"Everything which is possible in a script can also be directly typed within the shell. That allows to debug a code by running each line one by one in the console. When the program being executed is a shell script, bash will create a new bash process using a fork. This sub shell reads the lines from the shell script one line at a time. Commands on each line are read, interpreted and executed as if they would have come directly from the keyboard. Don't change the file contents while a script is running, because this can influence the running program and tend to incalculable results.","title":"Script vs Shell"},{"location":"lang/bash/#syntax","text":"Comments: use # to start a comment Commands: calls the function, built-in or within the PATH Arguments: separated by spaces behind the command Exits Status: is the integer return value which is 0 for success: exit codes Shebang: #!/bin/bash as first line is used to run script as bash if executed","title":"Syntax"},{"location":"lang/bash/#redirection-and-pipes","text":"There are three file descriptors: STDIN (0), STDOUT (1) and STDERR (2) which can be used. They can be directed into a file or piped as STDIN to the next command. Redirection Code Example STDOUT to file ls -l > ls-l.txt STDERR to file grep da * 2> grep-errors.txt STDOUT to STDERR grep da * 1>&2 STDERR to STDOUT grep * 2>&1 STDOUT and STDERR to file grep da * &> grep-errors.txt pipe STDOUT ls -l | grep \"\\.txt$\" pipe STDOUT and STDERR ls -l |& grep \"\\.txt$\" pipe only STDERR ls -l >/dev/null 2>&1 | grep \"\\.txt$\"","title":"Redirection and Pipes"},{"location":"lang/bash/#variables","text":"A variable is created by setting a value to it like test=\"xxxx\" . They are only accessible in the current shell. To make them also available to sub shells you have to export it. The following types of variables are possible: String variables Integer variables Constant variables Array variables Within functions local variables can be defined by using the local prefix. The following variables have special use cases: Variable Usage $* Expands to the positional parameters, starting from one. When the expansion occurs within double quotes, it expands to a single word with the value of each parameter separated by the first character of the IFS special variable. $@ Expands to the positional parameters, starting from one. When the expansion occurs within double quotes, each parameter expands to a separate word. $# Expands to the number of positional parameters in decimal. $? Expands to the exit status of the most recently executed foreground pipeline. $- A hyphen expands to the current option flags as specified upon invocation, by the set built-in command, or those set by the shell itself (such as the -i). $$ Expands to the process ID of the shell. $! Expands to the process ID of the most recently executed background (asynchronous) command. $0 Expands to the name of the shell or shell script. $_ The underscore variable is set at shell startup and contains the absolute file name of the shell or script being executed as passed in the argument list. Subsequently, it expands to the last argument to the previous command, after expansion. It is also set to the full pathname of each command executed and placed in the environment exported to that command. When checking mail, this parameter holds the name of the mail file. CDPATH A colon-separated list of directories used as a search path for the cd command. HOME The current user's home directory; the default for cd . The value of this variable is also used by tilde expansion. IFS A list of characters that separate fields; used when the shell splits words as part of expansion. MAIL If this parameter is set to a file name and the MAILPATH variable is not set, Bash informs the user of the arrival of mail in the specified file. MAILPATH A colon-separated list of file names which the shell periodically checks for new mail. OPTARG The value of the last option argument processed by getopts . OPTIND The index of the last option argument processed by getopts . PATH A colon-separated list of directories in which the shell looks for commands. PS1 The primary prompt string. The default value is '\\s-\\v\\$ ' . PS2 The secondary prompt string. The default value is '> ' . auto_resume This variable controls how the shell interacts with the user and job control. BASH The full pathname used to execute the current instance of Bash. BASH_ENV If this variable is set when Bash is invoked to execute a shell script, its value is expanded and used as the name of a startup file to read before executing the script. BASH_VERSION The version number of the current instance of Bash. BASH_VERSINFO A read-only array variable whose members hold version information for this instance of Bash. COLUMNS Used by the select built-in to determine the terminal width when printing selection lists. COMP_CWORD An index of the word containing the current cursor position. COMP_LINE The current command line. COMP_POINT The index of the current cursor position relative to the beginning of the current command. COMP_WORDS An array variable consisting of the individual words in the current command line. COMPREPLY An array variable from which Bash reads the possible completions generated by a shell function invoked by the programmable completion facility. DIRSTACK An array variable containing the current contents of the directory stack. EUID The numeric effective user ID of the current user. FCEDIT The editor used as a default by the -e option to the fc command. FIGNORE A colon-separated list of suffixes to ignore when performing file name completion. FUNCNAME The name of any currently-executing shell function. GLOBIGNORE A colon-separated list of patterns defining the set of file names to be ignored by file name expansion. GROUPS An array variable containing the list of groups of which the current user is a member. histchars Up to three characters which control history expansion, quick substitution, and tokenization. HISTCMD The history number, or index in the history list, of the current command. HISTCONTROL Defines whether a command is added to the history file. HISTFILE The name of the file to which the command history is saved. The default value is ~/.bash_history . HISTFILESIZE The maximum number of lines contained in the history file, defaults to 500. HISTIGNORE A colon-separated list of patterns used to decide which command lines should be saved in the history list. HISTSIZE The maximum number of commands to remember on the history list, default is 500. HOSTFILE Contains the name of a file in the same format as /etc/hosts that should be read when the shell needs to complete a hostname. HOSTNAME The name of the current host. HOSTTYPE A string describing the machine Bash is running on. IGNOREEOF Controls the action of the shell on receipt of an EOF character as the sole input. INPUTRC The name of the Readline initialization file, overriding the default /etc/inputrc . LANG Used to determine the locale category for any category not specifically selected with a variable starting with LC_ . LC_ALL This variable overrides the value of LANG and any other LC_ variable specifying a locale category. LC_COLLATE This variable determines the collation order used when sorting the results. LC_CTYPE This variable determines the interpretation of characters and the behavior of character classes within file name expansion and pattern matching. LC_MESSAGES This variable determines the locale used to translate double-quoted strings preceded by a \"$\" sign. LC_NUMERIC This variable determines the locale category used for number formatting. LINENO The line number in the script or shell function currently executing. LINES Used by the select built-in to determine the column length for printing selection lists. MACHTYPE A string that fully describes the system type on which Bash is executing, in the standard GNU CPU-COMPANY-SYSTEM format. MAILCHECK How often (in seconds) that the shell should check for mail in the files specified in the MAILPATH or MAIL variables. OLDPWD The previous working directory as set by the cd built-in. OPTERR If set to the value 1, Bash displays error messages generated by getopts. OSTYPE A string describing the operating system Bash is running on. PIPESTATUS An array variable containing a list of exit status values from the processes in the most recently executed foreground pipeline. POSIXLY_CORRECT If this variable is in the environment when bash starts, the shell enters POSIX mode. PPID The process ID of the shell's parent process. PROMPT_COMMAND If set, the value is interpreted as a command to execute before the printing of each primary prompt (PS1). PS3 The value of this variable is used as the prompt for the select command. Defaults to '#? ' PS4 The value is the prompt printed before the command line is echoed when the -x option is set; defaults to '+ ' . PWD The current working directory as set by the cd command. RANDOM Each time this parameter is referenced, a random integer between 0 and 32767 is generated. Assigning a value to this variable seeds the random number generator. REPLY The default variable for the read built-in. SECONDS This variable expands to the number of seconds since the shell was started. SHELLOPTS A colon-separated list of enabled shell options. SHLVL Incremented by one each time a new instance of Bash is started. TIMEFORMAT The value of this parameter is used as a format string specifying how the timing information for pipelines prefixed with the time reserved word should be displayed. TMOUT If set to a value greater than zero, TMOUT is treated as the default timeout for the read built-in. In an interative shell, the value is interpreted as the number of seconds to wait for input after issuing the primary prompt when the shell is interactive. UID The numeric, real user ID of the current user.","title":"Variables"},{"location":"lang/bash/#functions","text":"To define functions two types are possible: function quit { exit } hello () { echo Hello! } hello quit Parameters given to the functions are readable as $1 ... like from any command.","title":"Functions"},{"location":"lang/bash/#expansion","text":"Brace expansion is a mechanism by which arbitrary strings may be generated. Patterns to be brace-expanded take the form of an optional PREAMBLE, followed by a series of comma-separated strings between a pair of braces, followed by an optional POSTSCRIPT. The preamble is prefixed to each string contained within the braces, and the postscript is then appended to each resulting string, expanding left to right. Brace expansions may be nested. The results of each expanded string are not sorted; left to right order is preserved: $ echo sp{el,il,al}l spell spill spall Variable expansion will replace a variable with it's content, Command substitution will replace a command with it's output before evaluating the outer line. $(command) # backticks alternative `command` Arithmetic expansion allows the evaluation of an arithmetic expression and the substitution of the result. The format for arithmetic expansion is: $(( EXPRESSION )) The expression is treated as if it were within double quotes. All tokens in the expression undergo parameter expansion, command substitution, and quote removal. Arithmetic substitutions may be nested. Evaluation of arithmetic expressions is done in fixed-width integers with no check for overflow - although division by zero is trapped and recognized as an error. The operators are roughly the same as in the C programming language. In order of decreasing precedence, the list looks like this: Operator Meaning VAR++ and VAR-- variable post-increment and post-decrement ++VAR and --VAR variable pre-increment and pre-decrement - and + unary minus and plus ! and ~ logical and bitwise negation ** exponentiation * , / and % multiplication, division, remainder + and - addition, subtraction << and >> left and right bitwise shifts <= , >= , < and > comparison operators == and != equality and inequality & bitwise AND ^ bitwise exclusive OR | bitwise OR && logical AND || logical OR expr ? expr : expr conditional evaluation = , *= , /= , %= , += , -= , <<= , >>= , &= , ^= and |= assignments , separator between expressions File name expansion is done using * , ? , and [ . If one of these characters appears, then the word is regarded as a PATTERN , and replaced with an alphabetically sorted list of file names matching the pattern. If no matching file names are found, and the shell option nullglob is disabled, the word is left unchanged.","title":"Expansion"},{"location":"lang/bash/#conditionals","text":"if [ \"foo\" = \"foo\" ]; then echo expression evaluated as true else echo expression evaluated as false fi","title":"Conditionals"},{"location":"lang/bash/#test-conditions","text":"In Bash there are different extensions to write conditions: [ ... ] is POSIX command [[ ... ]] is a Bash extension [[ X ]] is a single construct that makes X be parsed magically. < , && , || and () are treated specially, and word splitting rules are different. There are also further differences like = and =~ . Type POSIX Bash < [ a \\< b ] comparison needs \\ else redirects output [[ a < b ]] && or || [ a = a ] && [ b = b ] deprecated: [ a = a -a b = b ] [[ a = a && b = b ]] ( ([ a = a ] || [ a = b ]) && [ a = b ] deprecated: [ \\( a = a -o a = b \\) -a a = b ] [[ (a = a || a = b) && a = b ]] word splitting x='a b' [ \"$x\" = 'a b' ] quotes needed to prevent expand [[ $x = 'a b' ]] = or == printf 'ab' | grep -Eq 'a.' [[ ab = a? ]] pattern matching applies =~ or !~ printf 'ab' | grep -Eq 'ab?' [[ ab =~ ab? ]] POSIX regexp As you see the bash expressions are often better readable.","title":"Test Conditions"},{"location":"lang/bash/#regular-expressions","text":"Operator Effect . Matches any single character. ? The preceding item is optional and will be matched, at most, once. * The preceding item will be matched zero or more times. + The preceding item will be matched one or more times. {N} The preceding item is matched exactly N times. {N,} The preceding item is matched N or more times. {N,M} The preceding item is matched at least N times, but not more than M times. - represents the range if it's not first or last in a list or the ending point of a range in a list. ^ Matches the empty string at the beginning of a line; also represents the characters not in the range of a list. $ Matches the empty string at the end of a line. \\b Matches the empty string at the edge of a word. \\B Matches the empty string provided it's not at the edge of a word. \\< Match the empty string at the beginning of word.","title":"Regular Expressions"},{"location":"lang/bash/#loops","text":"Bash has three different kind of loops: The for loop is a little bit different from other programming languages. Basically, it let's you iterate over a series of 'words' within a string. The while executes a piece of code if the control expression is true , and only stops when it is false . The until loop is almost equal to the while loop, except that the code is executed while the control expression evaluates to false . Each loop may be exited using break or continued with the next round using continue . for i in $( ls ); do echo item: $i done The variable i will take the different values contained in $( ls ) . The block enclosed in do and done is executed in each iteration. To use for as a counter use the range support of bash: for i in {1..10}; do echo $i done But you can also use a C-like for loop: for (( c=1; c<=5; c++ )); do echo \"Welcome $c times\" done In while and until you give the condition at the start and use a code block like used in for : COUNTER=0 while [ $COUNTER -lt 10 ]; do echo The counter is $COUNTER let COUNTER+=1 done COUNTER=20 until [ $COUNTER -lt 10 ]; do echo COUNTER $COUNTER let COUNTER-=1 done","title":"Loops"},{"location":"lang/bash/#options","text":"Shell options can either be set different from the default upon calling the shell, or be set during shell operation. To change an option use set -<option> to enable or set +<option> to disable an option. Option Usage noclobber prevents existing files from being overwritten by redirection operations noglob prevents special characters from being expanded u treat unset variables as an error x set debug mode which will print each expanded command before executing","title":"Options"},{"location":"lang/bash/#include-modules","text":"Using the source <path> command the given script will be included like it is directly written there the source command is set. A relative path is based on the current directory of the calling shell. {!docs/abbreviations.txt!}","title":"Include modules"},{"location":"lang/go/","text":"Go My first experience with go! In my effort to learn a new language I started to checkout Go and Rust both for some days and do a bit in both to decide which one to choose for a deeper experience. I worked a lot with Java later with NodeJS but both have some problems for me. Java is not after my liking, while NodeJS is great for fast prototyping but get problems if the project grows really big. So in this article I describe the findings of Go as an notebook for myself. Installation This is done really easy under Linux: # download curl -O https://storage.googleapis.com/golang/go1.8.1.linux-amd64.tar.gz # install sudo tar -C /usr/local -xzf go1.8.1.linux-amd64.tar.gz rm go1.8.1.linux-amd64.tar.gz # set path to go and compiled programs echo \"export GOPATH=$HOME/go\" >> ~/.bashrc echo \"export GOPATH=$HOME/go\" >> ~/.profile echo \"export PATH=$PATH:/usr/local/go/bin:$GOPATH/bin\" >> ~/.bashrc echo \"export PATH=$PATH:/usr/local/go/bin:$GOPATH/bin\" >> ~/.profile export GOPATH=$HOME/go export PATH=$PATH:/usr/local/go/bin:$GOPATH/bin # install additional helpers go get -u github.com/nsf/gocode go get -u github.com/golang/lint/golint Other installation methods also exist see the Project Site . You will find all the go related stuff below a go folder in your home directory. Learning To dive into I worked through the following resources: Getting Started Tour HowTo Effective Go File Structure The go workspace ( ~/go under Linux) contains the following directories: src/ # sources github.com/alinex/go-learn # package source .git/ # version control system ... pkg/ # downloaded packages bin/ # executable commands Editor At first I use the Atom editor with the following plugins: go-plus go-debug go-signature-statusbar formatter-gofmt No need for a style guide here because formatting is done by gofmt on it's own on saving. The above installers go-signature-statusbar will you show a reference using Alt-D with the cursor on a specific symbol. Build Tools All the possible tools are combined in the go cli tool: go build # check if it can be compiled go install # to build and make a runnable command go test # run unit tests <name> # to directly run it, because you have it in your path go get <path> # fetch and install Documenting To show your documentation in the browser start the godoc server: godoc -http=:6060 & # to start it in the background xdg-open http://localhost:6060/pkg/ # to open the package list in the browser And then go in your browser to Learning Go . {!docs/abbreviations.txt!}","title":"Go"},{"location":"lang/go/#go","text":"My first experience with go! In my effort to learn a new language I started to checkout Go and Rust both for some days and do a bit in both to decide which one to choose for a deeper experience. I worked a lot with Java later with NodeJS but both have some problems for me. Java is not after my liking, while NodeJS is great for fast prototyping but get problems if the project grows really big. So in this article I describe the findings of Go as an notebook for myself.","title":"Go"},{"location":"lang/go/#installation","text":"This is done really easy under Linux: # download curl -O https://storage.googleapis.com/golang/go1.8.1.linux-amd64.tar.gz # install sudo tar -C /usr/local -xzf go1.8.1.linux-amd64.tar.gz rm go1.8.1.linux-amd64.tar.gz # set path to go and compiled programs echo \"export GOPATH=$HOME/go\" >> ~/.bashrc echo \"export GOPATH=$HOME/go\" >> ~/.profile echo \"export PATH=$PATH:/usr/local/go/bin:$GOPATH/bin\" >> ~/.bashrc echo \"export PATH=$PATH:/usr/local/go/bin:$GOPATH/bin\" >> ~/.profile export GOPATH=$HOME/go export PATH=$PATH:/usr/local/go/bin:$GOPATH/bin # install additional helpers go get -u github.com/nsf/gocode go get -u github.com/golang/lint/golint Other installation methods also exist see the Project Site . You will find all the go related stuff below a go folder in your home directory.","title":"Installation"},{"location":"lang/go/#learning","text":"To dive into I worked through the following resources: Getting Started Tour HowTo Effective Go","title":"Learning"},{"location":"lang/go/#file-structure","text":"The go workspace ( ~/go under Linux) contains the following directories: src/ # sources github.com/alinex/go-learn # package source .git/ # version control system ... pkg/ # downloaded packages bin/ # executable commands","title":"File Structure"},{"location":"lang/go/#editor","text":"At first I use the Atom editor with the following plugins: go-plus go-debug go-signature-statusbar formatter-gofmt No need for a style guide here because formatting is done by gofmt on it's own on saving. The above installers go-signature-statusbar will you show a reference using Alt-D with the cursor on a specific symbol.","title":"Editor"},{"location":"lang/go/#build-tools","text":"All the possible tools are combined in the go cli tool: go build # check if it can be compiled go install # to build and make a runnable command go test # run unit tests <name> # to directly run it, because you have it in your path go get <path> # fetch and install","title":"Build Tools"},{"location":"lang/go/#documenting","text":"To show your documentation in the browser start the godoc server: godoc -http=:6060 & # to start it in the background xdg-open http://localhost:6060/pkg/ # to open the package list in the browser And then go in your browser to Learning Go . {!docs/abbreviations.txt!}","title":"Documenting"},{"location":"lang/handlebars/","text":"Handlebars Templates Handlebars is a simple web template system with minimal logic support. The templates written in this language will be compiled and executed with specific context to get the result. It's main purpose is for HTML output but can be used for other text based formats, too. An template may look like: {% raw %} <ul> {{#each users}} <li>{{firstname}} {{lastname}}</li> {{/each}} </ul> {% endraw %} Expression Syntax The Handlebars Expressions like seen above are written like (a double stash before, followed with the content to be evaluated, followed by a closing double stash): {% raw %} {{ content goes here }} {% endraw %} Special HTML characters are escaped automatically, but you may prevent this with the following syntax: Escaping The content may be a variable to use, some control methods or special helper functions to include. {% raw %} Triple Stash {{{ }}} For Non-escape HTML {% endraw %} Comments This is how you add comments in a Handlebars template: {% raw %} {{! Whatever is inside this comment expression will not be outputted }} {% endraw %} Or if your comment may contain other handlebars tokens use: {% raw %} {{!-- this may contain {{tokens}} and more --}} {% endraw %} Instead of the regular HTML comments they will not be outputted to the user. Blocks Blocks in Handlebars are expression that has a block, an opening {% raw %}{{# }}{% endraw %} followed by a closing {% raw %}{{/ }}{% endraw %} . See some more examples below. {% raw %} {{#each}}Content goes here.{{/each}} {% endraw %} Here is an if block {% raw %} {{#if someValueIsTrue}}Content goes here{{/if}} {% endraw %} Helpers An helper is an additional method working with some content or variable. They may be used as simple functions or block expressions or both depending on it's implementation. Calls to a helper may also have literal values passed to them either as parameter arguments or hash arguments. See the helper's description how it needs them. Supported literals include numbers, strings, true, false, null and undefined. Inline helpers are used as: {% raw %} {{helperName arg1 arg2 arg3 mapArg=\"val\"}} {% endraw %} Block helpers are used as: {% raw %} {{#helperName arg 1 arg2}} other stuff {{/helperName}} {% endraw %} Paths (with dot notation) A path in Handlebars is a property lookup. If we have a name property that contains an object, such as: {% raw %} objData = {name: {firstName: \"Michael\", lastName:\"Jackson\"}} {% endraw %} We can use nested paths (dot notation) to lookup the property you want, like this: {% raw %} {{name.firstName}} {% endraw %} Parent Path ../ Handlebars also has a parent path ../ to lookup properties on parents of the current context. Thus with a data object such as this: {% raw %} shoesData = {groupName:\"Celebrities\", users:[{name:{firstName:\"Mike\", lastName:\"Alexander\" }}, {name:{firstName:\"John\", lastName:\"Waters\" }} ]}; {% endraw %} We can use the parent path ../ to get the groupName property: {% raw %} \u200b<script id=\"shoe-template\" type=\"x-handlebars-template\">\u200b {{#users}}\u200b <li>{{name.firstName}} {{name.lastName}} is in the {{../groupName}} group.</li>\u200b {{/users}} \u200b</script> {% endraw %} Built-In Helpers if block A block in this condition will only be rendered if the given argument is empty or null. {% raw %} {{#if author}} Author: {{firstName}} {{lastName}} {{/if}} {% endraw %} It can also contain an alternative to use if not: {% raw %} {{#if author}} Author: {{firstName}} {{lastName}} {{else}} No Author defined. {{/if}} {% endraw %} And finally you can also chain multiple conditional blocks together: {% raw %} {{#if isActive}} Is working now... {{else if isInactive}} Is inactive. {{else}} Unknown state. {{/if}} {% endraw %} unless block You can use the unless helper as the inverse of the if helper. His content will be rendered if the expression returns a falsy value. {% raw %} {{#unless author}} No Author defined. {{/if}} {% endraw %} each block You can iterate over a list using this. Inside the block, you can use this to reference the element being iterated over. {% raw %} {{#each people}} - {{this}} {{/each}} {% endraw %} If it contains objects you may use their values: {% raw %} {{#each people}} - {{first}} {{this.last}} {{else}} No people assigned! {{/each}} {% endraw %} As you see you may use the this value explicitly or directly use the object's properties. And like in the if helper an additional else part will be rendered if nothing to iterate is found. When looping through items in each, you can optionally reference the current loop index with '@index': {% raw %} {{#each people}} {{@index}}: {{first}} {{this.last}} {{else}} No people assigned! {{/each}} {% endraw %} Also the first and last steps of iteration are noted via the '@first' and '@last' variables when iterating over an array. When iterating over an object only the '@first' is available. Nested each blocks may access the iteration variables via depth based paths. To access the parent index, for example, '{% raw %}{{@../index}}{% endraw %}' can be used. The each helper also supports block parameters, allowing for named references anywhere in the block: {% raw %} {{#each array as |value key|}} {{#each child as |childValue childKey|}} {{key}} - {{childKey}}. {{childValue}} {{/each}} {{/each}} {% endraw %} It is not usable on objects, only on lists or lists of objects. with block Normally, Handlebars templates are evaluated against the context passed into the compiled method. But you may shift the current context for a section of a template by using the built-in with block helper. {% raw %} {{#with author}} Author: {{firstName}} {{lastName}} {{/with}} {% endraw %} with can also be used with block parameters to define known references in the current block. The example above can be converted to {% raw %} {{#with author as |myAuthor|}} Author: {{myAuthor.firstName}} {{myAuthor.lastName}} {{/with}} {% endraw %} This can also contain an else section which will display only when the passed value is empty. lookup The lookup helper allows for dynamic parameter resolution using Handlebars variables. This is useful for resolving values for array indexes. {% raw %} {{#each bar}} {{lookup ../foo @index}} {{/each}} {% endraw %} raw block Raw blocks are available for templates needing to handle unprocessed mustache blocks. {% raw %} {{{{raw-helper}}}} {{bar}} {{{{/raw-helper}}}} {% endraw %} This keeps the internal handlebars syntax untouched. Alinex Helpers If handlebars are used through the validator a lot more and powerful helpers are available. {!docs/abbreviations.txt!}","title":"Handlebars"},{"location":"lang/handlebars/#handlebars-templates","text":"Handlebars is a simple web template system with minimal logic support. The templates written in this language will be compiled and executed with specific context to get the result. It's main purpose is for HTML output but can be used for other text based formats, too. An template may look like: {% raw %} <ul> {{#each users}} <li>{{firstname}} {{lastname}}</li> {{/each}} </ul> {% endraw %}","title":"Handlebars Templates"},{"location":"lang/handlebars/#expression-syntax","text":"The Handlebars Expressions like seen above are written like (a double stash before, followed with the content to be evaluated, followed by a closing double stash): {% raw %} {{ content goes here }} {% endraw %} Special HTML characters are escaped automatically, but you may prevent this with the following syntax:","title":"Expression Syntax"},{"location":"lang/handlebars/#escaping","text":"The content may be a variable to use, some control methods or special helper functions to include. {% raw %} Triple Stash {{{ }}} For Non-escape HTML {% endraw %}","title":"Escaping"},{"location":"lang/handlebars/#comments","text":"This is how you add comments in a Handlebars template: {% raw %} {{! Whatever is inside this comment expression will not be outputted }} {% endraw %} Or if your comment may contain other handlebars tokens use: {% raw %} {{!-- this may contain {{tokens}} and more --}} {% endraw %} Instead of the regular HTML comments they will not be outputted to the user.","title":"Comments"},{"location":"lang/handlebars/#blocks","text":"Blocks in Handlebars are expression that has a block, an opening {% raw %}{{# }}{% endraw %} followed by a closing {% raw %}{{/ }}{% endraw %} . See some more examples below. {% raw %} {{#each}}Content goes here.{{/each}} {% endraw %} Here is an if block {% raw %} {{#if someValueIsTrue}}Content goes here{{/if}} {% endraw %}","title":"Blocks"},{"location":"lang/handlebars/#helpers","text":"An helper is an additional method working with some content or variable. They may be used as simple functions or block expressions or both depending on it's implementation. Calls to a helper may also have literal values passed to them either as parameter arguments or hash arguments. See the helper's description how it needs them. Supported literals include numbers, strings, true, false, null and undefined. Inline helpers are used as: {% raw %} {{helperName arg1 arg2 arg3 mapArg=\"val\"}} {% endraw %} Block helpers are used as: {% raw %} {{#helperName arg 1 arg2}} other stuff {{/helperName}} {% endraw %}","title":"Helpers"},{"location":"lang/handlebars/#paths-with-dot-notation","text":"A path in Handlebars is a property lookup. If we have a name property that contains an object, such as: {% raw %} objData = {name: {firstName: \"Michael\", lastName:\"Jackson\"}} {% endraw %} We can use nested paths (dot notation) to lookup the property you want, like this: {% raw %} {{name.firstName}} {% endraw %}","title":"Paths (with dot notation)"},{"location":"lang/handlebars/#parent-path","text":"Handlebars also has a parent path ../ to lookup properties on parents of the current context. Thus with a data object such as this: {% raw %} shoesData = {groupName:\"Celebrities\", users:[{name:{firstName:\"Mike\", lastName:\"Alexander\" }}, {name:{firstName:\"John\", lastName:\"Waters\" }} ]}; {% endraw %} We can use the parent path ../ to get the groupName property: {% raw %} \u200b<script id=\"shoe-template\" type=\"x-handlebars-template\">\u200b {{#users}}\u200b <li>{{name.firstName}} {{name.lastName}} is in the {{../groupName}} group.</li>\u200b {{/users}} \u200b</script> {% endraw %}","title":"Parent Path ../"},{"location":"lang/handlebars/#built-in-helpers","text":"","title":"Built-In Helpers"},{"location":"lang/handlebars/#if-block","text":"A block in this condition will only be rendered if the given argument is empty or null. {% raw %} {{#if author}} Author: {{firstName}} {{lastName}} {{/if}} {% endraw %} It can also contain an alternative to use if not: {% raw %} {{#if author}} Author: {{firstName}} {{lastName}} {{else}} No Author defined. {{/if}} {% endraw %} And finally you can also chain multiple conditional blocks together: {% raw %} {{#if isActive}} Is working now... {{else if isInactive}} Is inactive. {{else}} Unknown state. {{/if}} {% endraw %}","title":"if block"},{"location":"lang/handlebars/#unless-block","text":"You can use the unless helper as the inverse of the if helper. His content will be rendered if the expression returns a falsy value. {% raw %} {{#unless author}} No Author defined. {{/if}} {% endraw %}","title":"unless block"},{"location":"lang/handlebars/#each-block","text":"You can iterate over a list using this. Inside the block, you can use this to reference the element being iterated over. {% raw %} {{#each people}} - {{this}} {{/each}} {% endraw %} If it contains objects you may use their values: {% raw %} {{#each people}} - {{first}} {{this.last}} {{else}} No people assigned! {{/each}} {% endraw %} As you see you may use the this value explicitly or directly use the object's properties. And like in the if helper an additional else part will be rendered if nothing to iterate is found. When looping through items in each, you can optionally reference the current loop index with '@index': {% raw %} {{#each people}} {{@index}}: {{first}} {{this.last}} {{else}} No people assigned! {{/each}} {% endraw %} Also the first and last steps of iteration are noted via the '@first' and '@last' variables when iterating over an array. When iterating over an object only the '@first' is available. Nested each blocks may access the iteration variables via depth based paths. To access the parent index, for example, '{% raw %}{{@../index}}{% endraw %}' can be used. The each helper also supports block parameters, allowing for named references anywhere in the block: {% raw %} {{#each array as |value key|}} {{#each child as |childValue childKey|}} {{key}} - {{childKey}}. {{childValue}} {{/each}} {{/each}} {% endraw %} It is not usable on objects, only on lists or lists of objects.","title":"each block"},{"location":"lang/handlebars/#with-block","text":"Normally, Handlebars templates are evaluated against the context passed into the compiled method. But you may shift the current context for a section of a template by using the built-in with block helper. {% raw %} {{#with author}} Author: {{firstName}} {{lastName}} {{/with}} {% endraw %} with can also be used with block parameters to define known references in the current block. The example above can be converted to {% raw %} {{#with author as |myAuthor|}} Author: {{myAuthor.firstName}} {{myAuthor.lastName}} {{/with}} {% endraw %} This can also contain an else section which will display only when the passed value is empty.","title":"with block"},{"location":"lang/handlebars/#lookup","text":"The lookup helper allows for dynamic parameter resolution using Handlebars variables. This is useful for resolving values for array indexes. {% raw %} {{#each bar}} {{lookup ../foo @index}} {{/each}} {% endraw %}","title":"lookup"},{"location":"lang/handlebars/#raw-block","text":"Raw blocks are available for templates needing to handle unprocessed mustache blocks. {% raw %} {{{{raw-helper}}}} {{bar}} {{{{/raw-helper}}}} {% endraw %} This keeps the internal handlebars syntax untouched.","title":"raw block"},{"location":"lang/handlebars/#alinex-helpers","text":"If handlebars are used through the validator a lot more and powerful helpers are available. {!docs/abbreviations.txt!}","title":"Alinex Helpers"},{"location":"lang/markdown/","text":"Markdown Syntax The easiest way to document is to use markdown syntax. You write the code in plain text with some easy formatting rules. This can be transformed into HTML or other formats. But it's not an inherent standard, different flavors exist. For the alinex modules I decided to use this as the base for documentation. The following examples should help to use the right syntax which will be interpreted by GitHub and the integrated document generation of the alinex-builder . Headings Two formats of headers are supported. First you may use double or single underlines: Heading Level 1 ========================================== Heading Level 2 ------------------------------------------ Or you can define the headings with starting hash signs: # Heading Level 1 ## Heading Level 2 ### Heading Level 4 #### Heading Level 3 ##### Heading Level 5 ###### Heading Level 6 Paragraphs Paragraphs are written with an empty line. Newlines are ignored but you may add an <br /> Tag if you want: This text will be in one line <br>And this in a new one. becomes This text will be in one line And this in a new one. Text Format This styles can be used inline and also within a word. To prevent wrong interpretation you may escape the signs with an backslash \\ . Strikethrough Currently not supported in the document generation but in GitHub view! Use a double tilde sign before and after the text: Here is the ~~wrong text~~ right text. becomes Here is the ~~wrong text~~ right text. Emphasis *single asterisks* _single underscores_ becomes single asterisks single underscores Strong **double asterisks** __double underscores__ becomes double asterisks double underscores Code `code in backquotes` becomes `code in backquotes` Links If you give a standard URL in the text an link will be added automatically. You can also surround them with <...> : http://example.com and <http://example.com> info@alinex.de and <info@alinex.de> becomes http://example.com and http://example.com info@alinex.de and info@alinex.de You may also give a special link text by writing the text in square brackets and the link in round brackets behind: This is the alinex [home](http://alinex.github.io/node-alinex). becomes This is the alinex home . And at last you may use referenced links like that This is [an example][id] reference-style link. [id]: http://example.com/ \"Optional Title Here\" becomes This is an example reference-style link. The referenced links will get you the same output but makes the source code more readable. You may also keep the id empty to use the name also as the id. Lists You can write unordered lists like: * red * green * blue which becomes red green blue You can do the exactly same with an * , + or - sign. Ordered lists are written with the numbers following by periods: 1. red 2. green 3. blue will become red green blue If your list entry has multiple paragraph the second paragraph has to be indented by 4 spaces. To avoid interpreting number with dots as list you may escape the dot with an backslash. Blockquotes You may use email like block indenting with lines beginning with one or multiple > . You can also be lazy and put the sign only on the first line of an paragraph. I answered > Hello > > > Is this code written in coffeescript? > > Thats correct. becomes I answered Hello Is this code written in coffeescript? Thats correct. Blockquotes can also contain any other markdown format. Code blocks Standard Markup Syntax is to indent code blocks with four spaces: function test() { console.log('Test succeeded!'); } A better way is to fence the code like below. This also adds the ability to specify the language used (needed for syntax highlighting): ``` javascript function test() { console.log('Test succeeded!'); } ``` ``` coffee test = -> console.log 'Test succeeded!' ``` becomes function test() { console.log(\"Test succeeded!\"); } test = -> console.log 'Test succeeded!' Possible languages are listed in the languages YAML file . Horizontal lines You can add them by writing a paragraph which consists of only three or more * or - signs: ---------------------- becomes Tables Currently not supported in the document generation but in GitHub view! Tables will be created by dividing cells with a pipe | and the header row with hyphens - : | Color | Value | | ----- | ------ | | red | ff0000 | | green | 00ff00 | | blue | 0000ff | becomes Color Value red ff0000 green 00ff00 blue 0000ff The cells don't need the exact same width it may also vary in the same column. And you may put inline markup into the cell like bold , italic ... To align columns you have to add a colon in the divider line between header and body: | Left | Center | Right | | :----------------- | :----------------: | -----------------: | | first entry | first entry | first entry | | another | another | another | | and the last entry | and the last entry | and the last entry | becomes Left Center Right first entry first entry first entry another another another and the last entry and the last entry and the last entry Symbols & HTML You may also use some HTML symbols or tags but only as less as possible: &copy;<br />&frac12; becomes &copy;<br />&frac12; Images Images may be placed like links but start with an exclamation mark ! : ![Alinex Logo](../alinex-logo.png \"ALINEX\") becomes The title at the end is optional but will output as the tooltip. Like for links you may use the reference style here, too: ![Alinex Logo][id] [id]: ../alinex-logo.png \"ALINEX\" becomes Extended Markdown Within the report package an extended version of markdown conversion is used supporting all of the above but also boxes emoji special styles table of contents footnotes acronyms and more {!docs/abbreviations.txt!}","title":"Markdown"},{"location":"lang/markdown/#markdown-syntax","text":"The easiest way to document is to use markdown syntax. You write the code in plain text with some easy formatting rules. This can be transformed into HTML or other formats. But it's not an inherent standard, different flavors exist. For the alinex modules I decided to use this as the base for documentation. The following examples should help to use the right syntax which will be interpreted by GitHub and the integrated document generation of the alinex-builder .","title":"Markdown Syntax"},{"location":"lang/markdown/#headings","text":"Two formats of headers are supported. First you may use double or single underlines: Heading Level 1 ========================================== Heading Level 2 ------------------------------------------ Or you can define the headings with starting hash signs: # Heading Level 1 ## Heading Level 2 ### Heading Level 4 #### Heading Level 3 ##### Heading Level 5 ###### Heading Level 6","title":"Headings"},{"location":"lang/markdown/#paragraphs","text":"Paragraphs are written with an empty line. Newlines are ignored but you may add an <br /> Tag if you want: This text will be in one line <br>And this in a new one. becomes This text will be in one line And this in a new one.","title":"Paragraphs"},{"location":"lang/markdown/#text-format","text":"This styles can be used inline and also within a word. To prevent wrong interpretation you may escape the signs with an backslash \\ .","title":"Text Format"},{"location":"lang/markdown/#strikethrough","text":"Currently not supported in the document generation but in GitHub view! Use a double tilde sign before and after the text: Here is the ~~wrong text~~ right text. becomes Here is the ~~wrong text~~ right text.","title":"Strikethrough"},{"location":"lang/markdown/#emphasis","text":"*single asterisks* _single underscores_ becomes single asterisks single underscores","title":"Emphasis"},{"location":"lang/markdown/#strong","text":"**double asterisks** __double underscores__ becomes double asterisks double underscores","title":"Strong"},{"location":"lang/markdown/#code","text":"`code in backquotes` becomes `code in backquotes`","title":"Code"},{"location":"lang/markdown/#links","text":"If you give a standard URL in the text an link will be added automatically. You can also surround them with <...> : http://example.com and <http://example.com> info@alinex.de and <info@alinex.de> becomes http://example.com and http://example.com info@alinex.de and info@alinex.de You may also give a special link text by writing the text in square brackets and the link in round brackets behind: This is the alinex [home](http://alinex.github.io/node-alinex). becomes This is the alinex home . And at last you may use referenced links like that This is [an example][id] reference-style link. [id]: http://example.com/ \"Optional Title Here\" becomes This is an example reference-style link. The referenced links will get you the same output but makes the source code more readable. You may also keep the id empty to use the name also as the id.","title":"Links"},{"location":"lang/markdown/#lists","text":"You can write unordered lists like: * red * green * blue which becomes red green blue You can do the exactly same with an * , + or - sign. Ordered lists are written with the numbers following by periods: 1. red 2. green 3. blue will become red green blue If your list entry has multiple paragraph the second paragraph has to be indented by 4 spaces. To avoid interpreting number with dots as list you may escape the dot with an backslash.","title":"Lists"},{"location":"lang/markdown/#blockquotes","text":"You may use email like block indenting with lines beginning with one or multiple > . You can also be lazy and put the sign only on the first line of an paragraph. I answered > Hello > > > Is this code written in coffeescript? > > Thats correct. becomes I answered Hello Is this code written in coffeescript? Thats correct. Blockquotes can also contain any other markdown format.","title":"Blockquotes"},{"location":"lang/markdown/#code-blocks","text":"Standard Markup Syntax is to indent code blocks with four spaces: function test() { console.log('Test succeeded!'); } A better way is to fence the code like below. This also adds the ability to specify the language used (needed for syntax highlighting): ``` javascript function test() { console.log('Test succeeded!'); } ``` ``` coffee test = -> console.log 'Test succeeded!' ``` becomes function test() { console.log(\"Test succeeded!\"); } test = -> console.log 'Test succeeded!' Possible languages are listed in the languages YAML file .","title":"Code blocks"},{"location":"lang/markdown/#horizontal-lines","text":"You can add them by writing a paragraph which consists of only three or more * or - signs: ---------------------- becomes","title":"Horizontal lines"},{"location":"lang/markdown/#tables","text":"Currently not supported in the document generation but in GitHub view! Tables will be created by dividing cells with a pipe | and the header row with hyphens - : | Color | Value | | ----- | ------ | | red | ff0000 | | green | 00ff00 | | blue | 0000ff | becomes Color Value red ff0000 green 00ff00 blue 0000ff The cells don't need the exact same width it may also vary in the same column. And you may put inline markup into the cell like bold , italic ... To align columns you have to add a colon in the divider line between header and body: | Left | Center | Right | | :----------------- | :----------------: | -----------------: | | first entry | first entry | first entry | | another | another | another | | and the last entry | and the last entry | and the last entry | becomes Left Center Right first entry first entry first entry another another another and the last entry and the last entry and the last entry","title":"Tables"},{"location":"lang/markdown/#symbols-html","text":"You may also use some HTML symbols or tags but only as less as possible: &copy;<br />&frac12; becomes &copy;<br />&frac12;","title":"Symbols &amp; HTML"},{"location":"lang/markdown/#images","text":"Images may be placed like links but start with an exclamation mark ! : ![Alinex Logo](../alinex-logo.png \"ALINEX\") becomes The title at the end is optional but will output as the tooltip. Like for links you may use the reference style here, too: ![Alinex Logo][id] [id]: ../alinex-logo.png \"ALINEX\" becomes","title":"Images"},{"location":"lang/markdown/#extended-markdown","text":"Within the report package an extended version of markdown conversion is used supporting all of the above but also boxes emoji special styles table of contents footnotes acronyms and more {!docs/abbreviations.txt!}","title":"Extended Markdown"},{"location":"lang/sass/","text":"Sass CSS Preprocessor !!! info Since Version 3 SCSS is the official Syntax, which adds braces to the definition. There,fore only this format is described here. Variables To reuse the same information again, variables may be used. They need a pound sign in front. $font-stack: Helvetica, sans-serif; $primary-color: #333; body { font: 100% $font-stack; color: $primary-color; } Nesting With nesting the CSS will become more organized: div { // will become: div a { .... } a { color: red; // The parent selector helps to further extend the same element. // div a:hover { .... } &:hover { font-weight: bold; } } } .test { // It can also be used to style the outer selector in a certain context, such // as a body set to use a right-to-left language. // [dir=rtl] .test { ... } [dir=rtl] & { margin-left: 0; margin-right: 10px; } // You can even use it as an argument to pseudo-class selectors. // :not(.test) { ... } :not(&) { opacity: 0.8; } // Also a suffix to the existing class can be set. // .test--dark { ... } &--dark { color: white; } Mixins @mixin transform($property) { -webkit-transform: $property; -ms-transform: $property; transform: $property; } .box { @include transform(rotate(30deg)); } Extend You define a block of scripts as Extend block, which may be used anywhere: %message-shared { border: 1px solid #ccc; padding: 10px; color: #333; } .message { @extend %message-shared; } Operators Mathematical operators are possible, too. article[role=\"main\"] { float: left; width: 600px / 960px * 100%; } At-rules @use , @forward , @import loads mixins, functions, and variables. @mixin and @include makes it easy to re-use chunks of styles. @function defines custom functions that can be used in SassScript expressions. @extend allows selectors to inherit styles from one another. @at-root puts styles within it at the root of the CSS document. Flow control rules like @if , @each , @for and @while control whether or how many times styles are emitted. Builtin Modules sass:math module provides functions that operate on numbers. sass:string module makes it easy to combine, search, or split apart strings. sass:color module generates new colors based on existing ones, making it easy to build color themes. sass:list module lets you access and modify values in lists. sass:map module makes it possible to look up the value associated with a key in a map, and much more. sass:selector module provides access to Sass\u2019s powerful selector engine. sass:meta module exposes the details of Sass\u2019s inner workings. More details can be found in Sass Documentation . {!docs/abbreviations.txt!}","title":"Sass"},{"location":"lang/sass/#sass-css-preprocessor","text":"!!! info Since Version 3 SCSS is the official Syntax, which adds braces to the definition. There,fore only this format is described here.","title":"Sass CSS Preprocessor"},{"location":"lang/sass/#variables","text":"To reuse the same information again, variables may be used. They need a pound sign in front. $font-stack: Helvetica, sans-serif; $primary-color: #333; body { font: 100% $font-stack; color: $primary-color; }","title":"Variables"},{"location":"lang/sass/#nesting","text":"With nesting the CSS will become more organized: div { // will become: div a { .... } a { color: red; // The parent selector helps to further extend the same element. // div a:hover { .... } &:hover { font-weight: bold; } } } .test { // It can also be used to style the outer selector in a certain context, such // as a body set to use a right-to-left language. // [dir=rtl] .test { ... } [dir=rtl] & { margin-left: 0; margin-right: 10px; } // You can even use it as an argument to pseudo-class selectors. // :not(.test) { ... } :not(&) { opacity: 0.8; } // Also a suffix to the existing class can be set. // .test--dark { ... } &--dark { color: white; }","title":"Nesting"},{"location":"lang/sass/#mixins","text":"@mixin transform($property) { -webkit-transform: $property; -ms-transform: $property; transform: $property; } .box { @include transform(rotate(30deg)); }","title":"Mixins"},{"location":"lang/sass/#extend","text":"You define a block of scripts as Extend block, which may be used anywhere: %message-shared { border: 1px solid #ccc; padding: 10px; color: #333; } .message { @extend %message-shared; }","title":"Extend"},{"location":"lang/sass/#operators","text":"Mathematical operators are possible, too. article[role=\"main\"] { float: left; width: 600px / 960px * 100%; }","title":"Operators"},{"location":"lang/sass/#at-rules","text":"@use , @forward , @import loads mixins, functions, and variables. @mixin and @include makes it easy to re-use chunks of styles. @function defines custom functions that can be used in SassScript expressions. @extend allows selectors to inherit styles from one another. @at-root puts styles within it at the root of the CSS document. Flow control rules like @if , @each , @for and @while control whether or how many times styles are emitted.","title":"At-rules"},{"location":"lang/sass/#builtin-modules","text":"sass:math module provides functions that operate on numbers. sass:string module makes it easy to combine, search, or split apart strings. sass:color module generates new colors based on existing ones, making it easy to build color themes. sass:list module lets you access and modify values in lists. sass:map module makes it possible to look up the value associated with a key in a map, and much more. sass:selector module provides access to Sass\u2019s powerful selector engine. sass:meta module exposes the details of Sass\u2019s inner workings. More details can be found in Sass Documentation . {!docs/abbreviations.txt!}","title":"Builtin Modules"},{"location":"rust/","text":"Rust Rust is a system programming language sponsored by Mozilla which is a safe, concurrent and practical language supporting functional and imperative-procedural paradigms. The syntax of Rust is similar to C and C++ using blocks of code delimited by curly brackets and control flow keywords such as if, else, while, and for. Rust targets a field where you probably don't want a garbage collector, but of course memory safety is always important. In order to achieve memory safety without garbage collection, they enforce safety at compile time. Memory gets freed when you don\u2019t need it any more, which the compiler knows for certain because of borrowing and ownership rules. That\u2019s the biggest unique part about Rust: the ownership system. Cargo is Rust\u2019s build system and package manager. This tool handles a lot of tasks for you, such as building your code, downloading the libraries your code depends on, and building those libraries. As Rust uses editions to differentiate between major version changes, this documentation tries to keep up with the newest edition but most parts belong to all of them. To learn about Rust and have a deeper look you should consider reading: The Book Rust by Example . The Rust Reference Further reading may be: The Cargo Book The rustc Book The Rustdoc Book Standard Library WebAssembly And if this all doesn't help contact the Community . Rust has a steal learning curve with rules around types, ownership, and lifetimes. Despite the difficulty, it's an interesting learning experience but it is worth it. {!docs/abbreviations.txt!}","title":"Overview"},{"location":"rust/#rust","text":"Rust is a system programming language sponsored by Mozilla which is a safe, concurrent and practical language supporting functional and imperative-procedural paradigms. The syntax of Rust is similar to C and C++ using blocks of code delimited by curly brackets and control flow keywords such as if, else, while, and for. Rust targets a field where you probably don't want a garbage collector, but of course memory safety is always important. In order to achieve memory safety without garbage collection, they enforce safety at compile time. Memory gets freed when you don\u2019t need it any more, which the compiler knows for certain because of borrowing and ownership rules. That\u2019s the biggest unique part about Rust: the ownership system. Cargo is Rust\u2019s build system and package manager. This tool handles a lot of tasks for you, such as building your code, downloading the libraries your code depends on, and building those libraries. As Rust uses editions to differentiate between major version changes, this documentation tries to keep up with the newest edition but most parts belong to all of them. To learn about Rust and have a deeper look you should consider reading: The Book Rust by Example . The Rust Reference Further reading may be: The Cargo Book The rustc Book The Rustdoc Book Standard Library WebAssembly And if this all doesn't help contact the Community . Rust has a steal learning curve with rules around types, ownership, and lifetimes. Despite the difficulty, it's an interesting learning experience but it is worth it. {!docs/abbreviations.txt!}","title":"Rust"},{"location":"rust/core/","text":"Language Core This very long page will go over the whole language specification explaining each part in a short form. As already said, for more detailed information read the documents already mentioned at the start of the Rust part. Local Variables They can be defined using let which will immediately allocate memory but they have to be initialized with a value first to be used. Both can also be done in one step: let x = 5; !!! notice \"Reserved names\" Rust has some [keywords](https://doc.rust-lang.org/reference/keywords.html) which are not possible to be used as variables. If you want to use such, use a raw variable like `fn r#match ...` starting with `r#` on definition and every call. This is mainly used while transferring files to newer editions. It is always better to use another name. Constants All hard coded values, which won't change in any circumstance, should be declared as constant. They are defined using const , have to be initialized on definition and can't be changed later. Mutability To ensure memory safety all variables are immutable by default. So you have to specify if you want to change them later. To set a variable as mutable precede it with the mut keyword. let x = 5; // immutable variable let mut x = 5; // mutable variable Shadowing You can declare a new variable with the same name as a previous variable. Meaning the first variable is shadowed by the second so that the second variable\u2019s value is what appears when the variable is used. You do so by redefining the variable again. This is often used to change the type of the value but reuse the same name which is not possible with mutable variables. Scope The scope is the range within the program for which the variable is valid. It is valid from it's definition till the end of scope. The scope of the variable is the block in which it is defined. A new scope can be created by using curly braces also directly within the function. Copying Data on the stack (primary data types) are copied by value, others on the heap are moved, if you assign one variable to another. That means a moved variable can't be used any longer. let a = 1; let b = a; // copy data on the stack let s1 = String::from(\"hello\"); let s2 = s1; // move data to s2, s1 can no longer be used let s3 = s2.clone(); // clone data on the heap To really copy heap data you need to clone them. The same goes for the copying/moving variables into functions as parameters or by returning them. So they are no longer valid in the parent function after the call is made. Ownership Rust\u2019s central feature is ownership with the following rules: Each value in Rust has a variable that\u2019s called its owner. There can only be one owner at a time. When the owner goes out of scope, the value will be dropped. The owner may borrow the variable to another function. Like shown above ownership is managed by copying/moving variables between functions. Alternatively the variable owned in the parent function can be borrowed to sub functions as reference indicated by & . The owner stays and can further work with the variable. Keep in mind that the reference also have to be mutable if it's value should be changeable. fn main() { let s1 = String::from(\"hello\"); let len = calculate_length(&s1); println!(\"The length of '{}' is {}.\", s1, len); } fn calculate_length(s: &String) -> usize { s.len() } You can have only one mutable reference to a particular piece of data in a particular scope. If you need more you have to create separate scopes to end validity for one mutable reference before the other is created. The same goes for mixed mutable and immutable references to prevent you from references which change while using them. References can't be borrowed out of the definition scope. References Referencing is done by & before the variable and dereferencing with * before the variable containing a reference. Primary Data Types Rust is a statically typed language, which means that it must know the types of all variables at compile time. The compiler can usually infer what type we want to use based on the value and how we use it. But if not we have to annotate it. let value: u32 = \"42\".parse().expect(\"Not a number!\"); Integer Length Type Min Max 8-bit i8 -128 127 8-bit u8 0 255 16-bit i16 -32768 32767 16-bit u16 0 65535 32-bit i32 -2147483648 2147483647 32-bit u32 0 4294967295 64-bit i64 -9223372036854775808 9223372036854775807 64-bit u64 0 18446744073709551615 arch isize ? ? arch usize 0 ? The isize and usize types depend on the kind of computer your program is running on: 64 bits if you\u2019re on a 64-bit architecture and 32 bits if you\u2019re on a 32-bit architecture. Integer values can be written as: Decimal: 98_222 Hex: 0xff Octal: 0o77 Binary: 0b1111_0000 Byte (u8 only): b'A' Integer types default is i32 - this type is generally the fastest, even on 64-bit systems. If you set a variable you can define the type by also adding the type to the numeric value like 5u43 or with an optional underscore for better readability 5_u32 . Float Rust\u2019s floating-point types are f32 and f64 , which are 32 bits and 64 bits in size, respectively. The default type is f64 because on modern CPUs it\u2019s roughly the same speed as f32 but is capable of more precision. Like for integers the _ may be used as visual separator and the type may be appended like 13_f32 . Boolean Boolean type in Rust are defined as bool and have two possible values: true and false . Character Rust\u2019s char type is specified with single quotes and represents a Unicode character. Tuple A tuple is an ordered list of multiple other types. To access parts of it you can destructure it or directly access sub parts. // define let coordinate: (i32, i32) = (25, 40); // destructure let (x, y) = coordinate; println!(\"The value of y is: {}\", y); // direct access println!(\"The value of y is: {}\", coordinate.1); As far as they contain only primary data types they are completely stored on the stack. Array The array gives you a list of values, all with the same type. Arrays in Rust have a fixed length at compile time and cannot grow or shrink. let a = [1, 2, 3, 4, 5]; let second = a[1]; // access specific element As far as they contain primary data types they are completely stored on the stack. Slices Slices let you reference a contiguous sequence of elements in a collection rather than the whole collection. A string slice is a reference to part of a String which is created from a String with a range that begins at start and continues up to, but not including the end position, both are optional. The type is written as &str . let s = String::from(\"hello world\"); let hello = &s[0..5]; String literals are also slices pointing to a position in the binary. Slices are also possible on arrays: let a = [1, 2, 3, 4, 5]; let slice = &a[1..3]; Here the type is &[i32] . Literals Numeric literals can be type annotated by adding the type as a suffix. As an example, to specify that the literal 42 should have the type i32 , write 42i32 . Type Aliasing The main use of aliases is to reduce boilerplate; for example the IoResult<T> type is an alias for the Result<T, IoError> type. This is done by defining the type like: type NanoSecond = u64; Type Conversion Explicit type conversion (casting) can be performed using the as keyword: let decimal = 65.4321_f32; let integer = decimal as u8; let character = integer as char; Conversions between custom types are implemented using the traits From and Into or for Strings ToString and FromString . They are used through the from() and parse() functions like: String::from(my_str) or \"10\".parse::<i32>().unwrap() Functions Functions will be declared using fn before it's name. If parameters are possible, they need to be defined with their types. Within the function you may use statements and expressions. While expressions evaluate to an value, statements only perform some actions. Because assignments are statements, something like x = y = 6 is not allowed. While function calls are expressions, macros are statements and each expression ending with a ; is also turned into a statement. Return values are declared with an arrow -> before the function body: fn add(a: i32, b: i32) -> i32 { a + b } // calling it using let res = add(5, 7); You can return early from a function by using the return keyword and specifying a value, but most functions return the last expression implicitly. That's why no ; is set after the addition expression to keep it an expression and return it's result. Closures Sometimes you need anonymous functions given as a closure to another function or variable. They can capture values from the defining scope. The arguments here are given between pipes |err| , instead of round brackets used in normal functions, before a code block: let add = |a, b| { let x = add(5, 7); x }); // calling it using let res = add(5, 7); Closures are usually short and relevant only within a narrow context they mostly don't need type annotations. It can also consist of a single expression without the curly braces: let add_one_v4 = |x| x + 1 ; Each variable in the definition scope of the closure is directly accessible: let x = 4; let equal_to_x = |z| z == x; let res = equal_to_x(4); // that's true Using the move keyword before the parameters, the closure will take ownership of the used variables. A struct can be made to hold the closure and it's result value. This allows to call it multiple times but only execute it once. When taking a closure as an input parameter, the closure's complete type must be annotated using one of a few traits. In order of decreasing restriction, they are: Fn : the closure captures by reference ( &T ) FnMut : the closure captures by mutable reference ( &mut T ) FnOnce : the closure captures by value ( T ) See the description of structs and traits later in this book. See the Caching using Struct as an example of how this is used. Diverging functions Functions which will never return are called diverging functions. They are marked using !, which is an empty type. fn foo() -> ! { panic!(\"This call never returns.\"); } Comments As in most C-like languages you can have line and block comments: // this is a line comment which goes to the end of this line /* this is a block comment ending on the first closing characters */ Additionally there are document comments to add text to the API documentation. /// Generate library docs for the following item. //! Generate library docs for the enclosing item. Documentation Comments Documentation comments use three slashes, /// , instead of two and support Markdown notation for formatting the text. They have to be placed just before the item they\u2019re documenting. The documentation should explain the element. But don't describe the concrete API because this is added automatically by Rust. Useful sections may be: Examples - short code parts to explain typical use Panics - scenarios in which the function could panic Errors - the kinds of errors that might occur within the Result Safety - for unsafe functions there should be an explanation why the function is unsafe and covering the invariants that the function expects callers to uphold Running cargo test will run the code examples in your documentation as tests this will keep your examples always functional and up to date. To document the file itself you can use the alternative marker //! which is used to document the element this comment is contained in. Attention: Documentation in main.rs will not be exported. Only files which could be loaded as module can be documented, so put the main documentation in lib.rs . Control Flow if An if expression allows you to branch your code depending on conditions. If the condition is met the following block is executed but if not the optional else block is executed. The result of the condition has to be a bool . let number = 3; if number < 5 { println!(\"a small number\"); } else if number < 10 == 0 { println!(\"a little bigger number\"); } else { println!(\"big number\"); } As shown multiple alternative conditions may be joined. But if you need multiple alternatives consider using match . if is an expression so it can be used in a statement to set the returning value of the evaluated block to a variable. This means the values that have the potential to be results from each arm of the if must be the same type. let condition = true; let number = if condition { 5 } else { 6 }; loop The loop keyword tells Rust to execute a block of code over and over again forever or until you explicitly tell it to stop using break . With continue the current loop iteration is stopped and the next iteration will follow. Also the loop can be named to break or continue out of a specific loop: fn main() { 'outer: loop { println!(\"Entered the outer loop\"); 'inner: loop { println!(\"Entered the inner loop\"); break 'outer; } println!(\"This point will never be reached\"); } println!(\"Exited the outer loop\"); } You have to use a label like 'outer: before the loop statement and also behind the break or continue , Also you might return a value to the rest of the code by putting it after the break , and it will be returned by the loop expression. while If you want to check a condition to decide if the loop should continue you may use the while loop. Before each round of the loop the condition is evaluated and while it is true the loop goes on. let mut x = 5; while x < 10000 { println!(\"{}\", x); x = x * x } Alternatively you can do the same using loop and an if check with break . for To iterate a defined number of times use for n in 1..101 to go from 1 to 100. To also include the second value use for n in 1..=100 which is the same. A for loop is also used to execute some code for each item in a collection: for element in a.iter() { println!(\"the value is: {}\", element); } Alternatively the following iterators may be used in collections: iter() - borrows each element of the collection through each iteration, leaving the collection untouched and available for reuse after the loop into_iter() - consumes the collection and moves each element into the loop, they are no longer available in the collection iter_mut() - mutably borrow each element of the collection, allowing for the collection to be modified in place Structs Structs are one possibility to create objects. They are similar to tuples but with named elements, so that it is clear, what each element is. As a result the order of the elements isn't of matter. struct User { name: String, email: String, online: bool, } let mut user1 = User { email: String::from(\"someone@example.com\"), name: String::from(\"someone\"), online: true, }; user1.email = String::from(\"anotheremail@example.com\"); // if variable name and struct field is the same the creation can be simplified fn build_user(email: String, name: String) -> User { User { email, name, online: true, } } To create a new instance partly of the old one the .. before the variable name specifies that the undefined fields should have the same field as the specified object. let user2 = User { email: String::from(\"another@example.com\"), username: String::from(\"anotherusername567\"), ..user1 }; Also you can define structs like tuples without naming the data pieces. struct Color(i32, i32, i32); let black = Color(0, 0, 0); Methods Methods are functions contained within a struct, which are bound to the struct object data. An implementation block impl holds method functions for a struct. The method always has a reference to the struct object itself as &self as first parameter: struct Rectangle { width: u32, height: u32, } impl Rectangle { fn area(&self) -> u32 { self.width * self.height } } fn main() { let rect1 = Rectangle { width: 30, height: 50 }; let area = rect1.area(); } The self reference can also be made mutable. By calling such methods Rust will do automatic dereferencing so there is no need to make references in the calling. As with functions additional parameters are possible. Associated Functions This is like static methods in object oriented languages. They don't get a self reference and are called using :: #[derive(Debug)] struct Rectangle { width: u32, height: u32, } impl Rectangle { fn square(size: u32) -> Rectangle { Rectangle { width: size, height: size } } fn area(&self) -> u32 { self.width * self.height } } fn main() { let sq = Rectangle::square(3); let area = sq.area(); } You can also put the methods and associated functions in multiple implementation blocks. Enum The enum can be used to present different exclusive states which may also contain an associated value. enum IpAddrKind { V4, V6 } enum IpAddr { V4(u8, u8, u8, u8), V6(String), } let localhost = IpAddr::V4(127, 0, 0, 1); As shown the second example defines an enum with assigned values. Similar to struct an enum may contain methods. Rust didn't have the null value but the Option enum will be used. It is so common that you may use its values Some and None directly without the Option:: prefix. Pattern Matching Patterns are a special syntax in Rust for matching against the structure of types, both complex and simple. Within the patterns you may use: value - only this concrete value 1 | 2 - multiple matches are possible with or 1 ... 5 - ranges (a, b) - destructruring vector { x: a, y: b } - destructuring structs { x, y } - destructuring structs, using property name as variable names _ - is used to ignore values .. - ignoring remaining or previous parts ref or ref mut - creating a reference @ - create variable (before) and also test it (after the @) Match The match command allows you to compare a value against a series of patterns and then execute code based on which pattern matches. match x { None => None, Some(i) => Some(i + 1), } All possible values have to be checked. But you can use the _ placeholder as pattern to match the rest of the possibilities. If it gets more complex use match guards with an additional if : let num = Some(4); match num { Some(x) if x < 5 => println!(\"less than five: {}\", x), Some(x) => println!(\"{}\", x), None => (), } If Let The if let syntax lets you write a singular pattern match in an easier form: if let Some(3) = some_u8_value { println!(\"three\"); } Here the pattern comes before the equal sign and the value behind. As with a normal if you can also use an else part which is executed if the pattern doesn't match. It is used like if and can also be combined with it using else if let and else . While let Similar to the if let the while let will run a loop while the match succeeds. Error Handling Rust knows two types of errors: unrecoverable errors - will force the code to stop recoverable errors - will be handled within the code Use the unrecoverable errors sparsely. If a library use a recoverable error the caller always has the option to make it unrecoverable but not the other way around. Unrecoverable errors Then some bad situations occur which should not be there the panic! macro will print a failure message, unwind and clean up the stack and quit the program. The debug build will have included symbols, so that also a backtrace is possible which shows you the files and lines which brought you to the problem. If the program is called with the RUST_BACKTRACE=1 environment setting this backtrace will be displayed. Use such errors in prototyping to be later replaced or in tests. Or use it than it can't really fail. Recoverable errors Functions which may have an error are returning a Result enum which is defined as having two variants, Ok with an associated value and Err with an associated message. This can be checked like: let f = File::open(\"hello.txt\"); let f = match f { Ok(file) => file, Err(error) => { panic!(\"There was a problem opening the file: {:?}\", error) }, }; At first this is an recoverable error in the file::open method but then it was decided to make it in this situation unrecoverable using the panic! macro. The type of error within the Err handler here is io::Error , which is a struct provided by the standard library. This struct has a method kind that holds an io::ErrorKind value. The enum io::ErrorKind is also from the standard library and has variants representing the different kinds of errors that might result from an IO operation. The relevant variant in this example may be ErrorKind::NotFound , which indicates the file doesn\u2019t exist yet. It can be checked like: if error.kind() == ErrorKind::NotFound { ... } The Result<T, E> has some useful shortcut helpers: unwrap - if OK return the value and if Err then call panic! unwrap_or_else - like unwrap but on Err call the code in the given closure expect - do the same but use the given error message in panic message Propagation This will let the error bubble up through it's call stack. The following example will check for errors and immediately propagate them: let mut f = match f { Ok(file) => file, Err(e) => return Err(e), }; As a shortcut here the ? operator may be used: let mut f = File::open(\"hello.txt\")?; // immediately return on Result::Err with it File::open(\"hello.txt\")?.read_to_string(&mut s)?; // also possibly in between calls This will also automatically convert the error type to the function's defined returning type. But it can only be used in functions returning a Result . Generics Generics are abstract stand-ins to allow different concrete types to be used. Therefore the generics has to be defined in the function signature: fn largest<T>(list: &[T]) -> T { ... } This defines that the same type which is given ad reference will be returned. So this can be used with \u00ec32 or float ... Generics can also be used in struct or enum like: enum Result<T, E> { Ok(T), Err(E), } On implementations you have to define the generics directly after imp<T> enum<T>{ ... } . The performance of generics is not slower because the compiler will transform the generics to different concrete functions which are used. WHERE clauses A bound can also be expressed using a where clause immediately before the opening { , rather than at the type's first mention. Instead of: fn apply<F: FnOnce()>(f: F) { f(); } It can be written as: fn apply<F>(f: F) where F: FnOnce() { f(); } Traits Traits are similar to interfaces. They define behavior for multiple types. A trait contains the function signatures which has to be met: pub trait Summary { fn summarize(&self) -> String; } This can be used to ensure that types marked with this trait are only allowed on types with the defined signature. To implement a trait on a type: impl Summary for NewsArticle { fn summarize(&self) -> String { format!(\"{}, by {} ({})\", self.headline, self.author, self.location) } } Traits can only be implemented on local types. If a trait should have a default behavior this is defined as functions with body to the trait definition. Boundaries A trait can be used to limit generics to only some types. pub fn notify<T: Summary>(item: T) { println!(\"Breaking news! {}\", item.summarize()); } Multiple trait bounds can also be set like T: Summary + Display but to make this better readable it can also put as where condition before the function body: fn some_function<T, U>(t: T, u: U) -> i32 where T: Display + Clone, U: Clone + Debug { You can also use traits to conditional implement methods: impl<T: Display + PartialOrd> Pair<T> { fn cmp_display(&self) { ... } } Or directly implement a trait on types matching another trait: impl<T: Display> ToString for T { ... } Phantom Types A phantom type parameter is one that doesn't show up at runtime, but is checked statically (and only) at compile time. In combination with generics this can help to ensure type safety. Lifetime References have lifetimes, which are the scope for which this references are valid and help to prevent dangling references. The default lifetime can be adjusted using annotations. Problems occur if the reference of an inner variable is borrowed to an outer variable with longer lifetime. To solve this references are annotated with relations to lifetime spaces: &i32 // a reference &'a i32 // a reference with an explicit lifetime &'a mut i32 // a mutable reference with an explicit lifetime Each lifetime for it's own didn't give any help but the relation of different references with the same lifetime defined in the function signature: fn longest<'a>(x: &'a str, y: &'a str) -> &'a str { .. } This defines that all references in the parameters and the return value must have the same lifetime. Because we\u2019ve annotated the returned reference with the same lifetime parameter `a , the returned reference will also be valid for the length of the smaller of the lifetimes of x and y . Alternatively you can return an owned data type rather than a reference so the calling function is then responsible for cleaning up the value. Lifetimes are also possible in method definitions or in a struct that holds references But you won't need to add lifetime annotations anywhere because the compiler will add them automatically for known patterns. Static The 'static lifetime is special, it keeps the references for the whole lifetime of the program. This is used on string literals which are stored in the binary. Iterator You can create an iterator by calling iter , into_iter , or iter_mut on a vector. You can create iterators from the other collection types in the standard library, such as hash map. You can also create iterators that do anything you want by implementing the Iterator trait on your own types. On iterators you can also use high level operations like: filter - to only select elements where the given closure is true map - call a closure on each element and replace it with the closure`s result zip - combine two iterators together as tuples collect - return a vector of values count - count the number of elements And some more. Own Iterator Each iterator has to implement the next method: impl Iterator for Counter { type Item = u32; fn next(&mut self) -> Option<Self::Item> { self.count += 1; if self.count < 6 { Some(self.count) } else { None } } } Object Orientation Rust is not fully object oriented but you can implement object oriented patterns with it. Structs and enums are containing data and the procedures working on them, so they are objects. Also the implementation can partly be hide by encapsulation with modules and making only some parts public. But inheritance is not really possible. Although polymorphism is possible using the traits. Attributes An attribute is metadata applied to some module, crate or item. This metadata can be used to/for: conditional compilation of code set crate name, version and type (binary or library) disable lints (warnings) enable compiler features (macros, glob imports, etc.) link to a foreign library mark functions as unit tests mark functions that will be part of a benchmark They are written as #[attribute] if they apply for the current module or following item and #![attribute] if they apply for the whole crate. Just like with doc comments. They also may get values... Compiler Warnings #[allow(dead_code)] - to don't warn about unused functions Conditional Compiling This allows to compile parts only on specific settings like OS: #[cfg(target_os = \"linux\")] fn are_you_on_linux() { println!(\"You are running linux!\"); } #[cfg(not(target_os = \"linux\"))] fn are_you_on_linux() { println!(\"You are *not* running linux!\"); } The same may be achieved using the cfg! macro. The condition may be build using nested parts using the following helper: any like #[cfg(any(unix, windows))] all like #[cfg(all(unix, target_pointer_width = \"32\"))] not like #[cfg(not(foo))] You can also set another attribute based on a cfg variable with #[cfg_attr(a, b)] . This will set attribute b , but only if attribute a is set. Custom Conditions Using Cargo, they get set in the [features] section of your Cargo.toml: [features] # no features by default default = [] # Add feature \"foo\" here, then you can use it. # Our \"foo\" feature depends on nothing else. foo = [] #[cfg(feature = \"foo\")] mod foo { } Compile this using cargo build , no additional flag will be send to the rustc compiler (default). But using cargo build --features \"foo\" it will send the foo flag to rustc and the output will have the mod foo in it. {!docs/abbreviations.txt!}","title":"Language Core"},{"location":"rust/core/#language-core","text":"This very long page will go over the whole language specification explaining each part in a short form. As already said, for more detailed information read the documents already mentioned at the start of the Rust part.","title":"Language Core"},{"location":"rust/core/#local-variables","text":"They can be defined using let which will immediately allocate memory but they have to be initialized with a value first to be used. Both can also be done in one step: let x = 5; !!! notice \"Reserved names\" Rust has some [keywords](https://doc.rust-lang.org/reference/keywords.html) which are not possible to be used as variables. If you want to use such, use a raw variable like `fn r#match ...` starting with `r#` on definition and every call. This is mainly used while transferring files to newer editions. It is always better to use another name.","title":"Local Variables"},{"location":"rust/core/#constants","text":"All hard coded values, which won't change in any circumstance, should be declared as constant. They are defined using const , have to be initialized on definition and can't be changed later.","title":"Constants"},{"location":"rust/core/#mutability","text":"To ensure memory safety all variables are immutable by default. So you have to specify if you want to change them later. To set a variable as mutable precede it with the mut keyword. let x = 5; // immutable variable let mut x = 5; // mutable variable","title":"Mutability"},{"location":"rust/core/#shadowing","text":"You can declare a new variable with the same name as a previous variable. Meaning the first variable is shadowed by the second so that the second variable\u2019s value is what appears when the variable is used. You do so by redefining the variable again. This is often used to change the type of the value but reuse the same name which is not possible with mutable variables.","title":"Shadowing"},{"location":"rust/core/#scope","text":"The scope is the range within the program for which the variable is valid. It is valid from it's definition till the end of scope. The scope of the variable is the block in which it is defined. A new scope can be created by using curly braces also directly within the function.","title":"Scope"},{"location":"rust/core/#copying","text":"Data on the stack (primary data types) are copied by value, others on the heap are moved, if you assign one variable to another. That means a moved variable can't be used any longer. let a = 1; let b = a; // copy data on the stack let s1 = String::from(\"hello\"); let s2 = s1; // move data to s2, s1 can no longer be used let s3 = s2.clone(); // clone data on the heap To really copy heap data you need to clone them. The same goes for the copying/moving variables into functions as parameters or by returning them. So they are no longer valid in the parent function after the call is made.","title":"Copying"},{"location":"rust/core/#ownership","text":"Rust\u2019s central feature is ownership with the following rules: Each value in Rust has a variable that\u2019s called its owner. There can only be one owner at a time. When the owner goes out of scope, the value will be dropped. The owner may borrow the variable to another function. Like shown above ownership is managed by copying/moving variables between functions. Alternatively the variable owned in the parent function can be borrowed to sub functions as reference indicated by & . The owner stays and can further work with the variable. Keep in mind that the reference also have to be mutable if it's value should be changeable. fn main() { let s1 = String::from(\"hello\"); let len = calculate_length(&s1); println!(\"The length of '{}' is {}.\", s1, len); } fn calculate_length(s: &String) -> usize { s.len() } You can have only one mutable reference to a particular piece of data in a particular scope. If you need more you have to create separate scopes to end validity for one mutable reference before the other is created. The same goes for mixed mutable and immutable references to prevent you from references which change while using them. References can't be borrowed out of the definition scope.","title":"Ownership"},{"location":"rust/core/#references","text":"Referencing is done by & before the variable and dereferencing with * before the variable containing a reference.","title":"References"},{"location":"rust/core/#primary-data-types","text":"Rust is a statically typed language, which means that it must know the types of all variables at compile time. The compiler can usually infer what type we want to use based on the value and how we use it. But if not we have to annotate it. let value: u32 = \"42\".parse().expect(\"Not a number!\");","title":"Primary Data Types"},{"location":"rust/core/#integer","text":"Length Type Min Max 8-bit i8 -128 127 8-bit u8 0 255 16-bit i16 -32768 32767 16-bit u16 0 65535 32-bit i32 -2147483648 2147483647 32-bit u32 0 4294967295 64-bit i64 -9223372036854775808 9223372036854775807 64-bit u64 0 18446744073709551615 arch isize ? ? arch usize 0 ? The isize and usize types depend on the kind of computer your program is running on: 64 bits if you\u2019re on a 64-bit architecture and 32 bits if you\u2019re on a 32-bit architecture. Integer values can be written as: Decimal: 98_222 Hex: 0xff Octal: 0o77 Binary: 0b1111_0000 Byte (u8 only): b'A' Integer types default is i32 - this type is generally the fastest, even on 64-bit systems. If you set a variable you can define the type by also adding the type to the numeric value like 5u43 or with an optional underscore for better readability 5_u32 .","title":"Integer"},{"location":"rust/core/#float","text":"Rust\u2019s floating-point types are f32 and f64 , which are 32 bits and 64 bits in size, respectively. The default type is f64 because on modern CPUs it\u2019s roughly the same speed as f32 but is capable of more precision. Like for integers the _ may be used as visual separator and the type may be appended like 13_f32 .","title":"Float"},{"location":"rust/core/#boolean","text":"Boolean type in Rust are defined as bool and have two possible values: true and false .","title":"Boolean"},{"location":"rust/core/#character","text":"Rust\u2019s char type is specified with single quotes and represents a Unicode character.","title":"Character"},{"location":"rust/core/#tuple","text":"A tuple is an ordered list of multiple other types. To access parts of it you can destructure it or directly access sub parts. // define let coordinate: (i32, i32) = (25, 40); // destructure let (x, y) = coordinate; println!(\"The value of y is: {}\", y); // direct access println!(\"The value of y is: {}\", coordinate.1); As far as they contain only primary data types they are completely stored on the stack.","title":"Tuple"},{"location":"rust/core/#array","text":"The array gives you a list of values, all with the same type. Arrays in Rust have a fixed length at compile time and cannot grow or shrink. let a = [1, 2, 3, 4, 5]; let second = a[1]; // access specific element As far as they contain primary data types they are completely stored on the stack.","title":"Array"},{"location":"rust/core/#slices","text":"Slices let you reference a contiguous sequence of elements in a collection rather than the whole collection. A string slice is a reference to part of a String which is created from a String with a range that begins at start and continues up to, but not including the end position, both are optional. The type is written as &str . let s = String::from(\"hello world\"); let hello = &s[0..5]; String literals are also slices pointing to a position in the binary. Slices are also possible on arrays: let a = [1, 2, 3, 4, 5]; let slice = &a[1..3]; Here the type is &[i32] .","title":"Slices"},{"location":"rust/core/#literals","text":"Numeric literals can be type annotated by adding the type as a suffix. As an example, to specify that the literal 42 should have the type i32 , write 42i32 .","title":"Literals"},{"location":"rust/core/#type-aliasing","text":"The main use of aliases is to reduce boilerplate; for example the IoResult<T> type is an alias for the Result<T, IoError> type. This is done by defining the type like: type NanoSecond = u64;","title":"Type Aliasing"},{"location":"rust/core/#type-conversion","text":"Explicit type conversion (casting) can be performed using the as keyword: let decimal = 65.4321_f32; let integer = decimal as u8; let character = integer as char; Conversions between custom types are implemented using the traits From and Into or for Strings ToString and FromString . They are used through the from() and parse() functions like: String::from(my_str) or \"10\".parse::<i32>().unwrap()","title":"Type Conversion"},{"location":"rust/core/#functions","text":"Functions will be declared using fn before it's name. If parameters are possible, they need to be defined with their types. Within the function you may use statements and expressions. While expressions evaluate to an value, statements only perform some actions. Because assignments are statements, something like x = y = 6 is not allowed. While function calls are expressions, macros are statements and each expression ending with a ; is also turned into a statement. Return values are declared with an arrow -> before the function body: fn add(a: i32, b: i32) -> i32 { a + b } // calling it using let res = add(5, 7); You can return early from a function by using the return keyword and specifying a value, but most functions return the last expression implicitly. That's why no ; is set after the addition expression to keep it an expression and return it's result.","title":"Functions"},{"location":"rust/core/#closures","text":"Sometimes you need anonymous functions given as a closure to another function or variable. They can capture values from the defining scope. The arguments here are given between pipes |err| , instead of round brackets used in normal functions, before a code block: let add = |a, b| { let x = add(5, 7); x }); // calling it using let res = add(5, 7); Closures are usually short and relevant only within a narrow context they mostly don't need type annotations. It can also consist of a single expression without the curly braces: let add_one_v4 = |x| x + 1 ; Each variable in the definition scope of the closure is directly accessible: let x = 4; let equal_to_x = |z| z == x; let res = equal_to_x(4); // that's true Using the move keyword before the parameters, the closure will take ownership of the used variables. A struct can be made to hold the closure and it's result value. This allows to call it multiple times but only execute it once. When taking a closure as an input parameter, the closure's complete type must be annotated using one of a few traits. In order of decreasing restriction, they are: Fn : the closure captures by reference ( &T ) FnMut : the closure captures by mutable reference ( &mut T ) FnOnce : the closure captures by value ( T ) See the description of structs and traits later in this book. See the Caching using Struct as an example of how this is used.","title":"Closures"},{"location":"rust/core/#diverging-functions","text":"Functions which will never return are called diverging functions. They are marked using !, which is an empty type. fn foo() -> ! { panic!(\"This call never returns.\"); }","title":"Diverging functions"},{"location":"rust/core/#comments","text":"As in most C-like languages you can have line and block comments: // this is a line comment which goes to the end of this line /* this is a block comment ending on the first closing characters */ Additionally there are document comments to add text to the API documentation. /// Generate library docs for the following item. //! Generate library docs for the enclosing item.","title":"Comments"},{"location":"rust/core/#documentation-comments","text":"Documentation comments use three slashes, /// , instead of two and support Markdown notation for formatting the text. They have to be placed just before the item they\u2019re documenting. The documentation should explain the element. But don't describe the concrete API because this is added automatically by Rust. Useful sections may be: Examples - short code parts to explain typical use Panics - scenarios in which the function could panic Errors - the kinds of errors that might occur within the Result Safety - for unsafe functions there should be an explanation why the function is unsafe and covering the invariants that the function expects callers to uphold Running cargo test will run the code examples in your documentation as tests this will keep your examples always functional and up to date. To document the file itself you can use the alternative marker //! which is used to document the element this comment is contained in. Attention: Documentation in main.rs will not be exported. Only files which could be loaded as module can be documented, so put the main documentation in lib.rs .","title":"Documentation Comments"},{"location":"rust/core/#control-flow","text":"","title":"Control Flow"},{"location":"rust/core/#if","text":"An if expression allows you to branch your code depending on conditions. If the condition is met the following block is executed but if not the optional else block is executed. The result of the condition has to be a bool . let number = 3; if number < 5 { println!(\"a small number\"); } else if number < 10 == 0 { println!(\"a little bigger number\"); } else { println!(\"big number\"); } As shown multiple alternative conditions may be joined. But if you need multiple alternatives consider using match . if is an expression so it can be used in a statement to set the returning value of the evaluated block to a variable. This means the values that have the potential to be results from each arm of the if must be the same type. let condition = true; let number = if condition { 5 } else { 6 };","title":"if"},{"location":"rust/core/#loop","text":"The loop keyword tells Rust to execute a block of code over and over again forever or until you explicitly tell it to stop using break . With continue the current loop iteration is stopped and the next iteration will follow. Also the loop can be named to break or continue out of a specific loop: fn main() { 'outer: loop { println!(\"Entered the outer loop\"); 'inner: loop { println!(\"Entered the inner loop\"); break 'outer; } println!(\"This point will never be reached\"); } println!(\"Exited the outer loop\"); } You have to use a label like 'outer: before the loop statement and also behind the break or continue , Also you might return a value to the rest of the code by putting it after the break , and it will be returned by the loop expression.","title":"loop"},{"location":"rust/core/#while","text":"If you want to check a condition to decide if the loop should continue you may use the while loop. Before each round of the loop the condition is evaluated and while it is true the loop goes on. let mut x = 5; while x < 10000 { println!(\"{}\", x); x = x * x } Alternatively you can do the same using loop and an if check with break .","title":"while"},{"location":"rust/core/#for","text":"To iterate a defined number of times use for n in 1..101 to go from 1 to 100. To also include the second value use for n in 1..=100 which is the same. A for loop is also used to execute some code for each item in a collection: for element in a.iter() { println!(\"the value is: {}\", element); } Alternatively the following iterators may be used in collections: iter() - borrows each element of the collection through each iteration, leaving the collection untouched and available for reuse after the loop into_iter() - consumes the collection and moves each element into the loop, they are no longer available in the collection iter_mut() - mutably borrow each element of the collection, allowing for the collection to be modified in place","title":"for"},{"location":"rust/core/#structs","text":"Structs are one possibility to create objects. They are similar to tuples but with named elements, so that it is clear, what each element is. As a result the order of the elements isn't of matter. struct User { name: String, email: String, online: bool, } let mut user1 = User { email: String::from(\"someone@example.com\"), name: String::from(\"someone\"), online: true, }; user1.email = String::from(\"anotheremail@example.com\"); // if variable name and struct field is the same the creation can be simplified fn build_user(email: String, name: String) -> User { User { email, name, online: true, } } To create a new instance partly of the old one the .. before the variable name specifies that the undefined fields should have the same field as the specified object. let user2 = User { email: String::from(\"another@example.com\"), username: String::from(\"anotherusername567\"), ..user1 }; Also you can define structs like tuples without naming the data pieces. struct Color(i32, i32, i32); let black = Color(0, 0, 0);","title":"Structs"},{"location":"rust/core/#methods","text":"Methods are functions contained within a struct, which are bound to the struct object data. An implementation block impl holds method functions for a struct. The method always has a reference to the struct object itself as &self as first parameter: struct Rectangle { width: u32, height: u32, } impl Rectangle { fn area(&self) -> u32 { self.width * self.height } } fn main() { let rect1 = Rectangle { width: 30, height: 50 }; let area = rect1.area(); } The self reference can also be made mutable. By calling such methods Rust will do automatic dereferencing so there is no need to make references in the calling. As with functions additional parameters are possible.","title":"Methods"},{"location":"rust/core/#associated-functions","text":"This is like static methods in object oriented languages. They don't get a self reference and are called using :: #[derive(Debug)] struct Rectangle { width: u32, height: u32, } impl Rectangle { fn square(size: u32) -> Rectangle { Rectangle { width: size, height: size } } fn area(&self) -> u32 { self.width * self.height } } fn main() { let sq = Rectangle::square(3); let area = sq.area(); } You can also put the methods and associated functions in multiple implementation blocks.","title":"Associated Functions"},{"location":"rust/core/#enum","text":"The enum can be used to present different exclusive states which may also contain an associated value. enum IpAddrKind { V4, V6 } enum IpAddr { V4(u8, u8, u8, u8), V6(String), } let localhost = IpAddr::V4(127, 0, 0, 1); As shown the second example defines an enum with assigned values. Similar to struct an enum may contain methods. Rust didn't have the null value but the Option enum will be used. It is so common that you may use its values Some and None directly without the Option:: prefix.","title":"Enum"},{"location":"rust/core/#pattern-matching","text":"Patterns are a special syntax in Rust for matching against the structure of types, both complex and simple. Within the patterns you may use: value - only this concrete value 1 | 2 - multiple matches are possible with or 1 ... 5 - ranges (a, b) - destructruring vector { x: a, y: b } - destructuring structs { x, y } - destructuring structs, using property name as variable names _ - is used to ignore values .. - ignoring remaining or previous parts ref or ref mut - creating a reference @ - create variable (before) and also test it (after the @)","title":"Pattern Matching"},{"location":"rust/core/#match","text":"The match command allows you to compare a value against a series of patterns and then execute code based on which pattern matches. match x { None => None, Some(i) => Some(i + 1), } All possible values have to be checked. But you can use the _ placeholder as pattern to match the rest of the possibilities. If it gets more complex use match guards with an additional if : let num = Some(4); match num { Some(x) if x < 5 => println!(\"less than five: {}\", x), Some(x) => println!(\"{}\", x), None => (), }","title":"Match"},{"location":"rust/core/#if-let","text":"The if let syntax lets you write a singular pattern match in an easier form: if let Some(3) = some_u8_value { println!(\"three\"); } Here the pattern comes before the equal sign and the value behind. As with a normal if you can also use an else part which is executed if the pattern doesn't match. It is used like if and can also be combined with it using else if let and else .","title":"If Let"},{"location":"rust/core/#while-let","text":"Similar to the if let the while let will run a loop while the match succeeds.","title":"While let"},{"location":"rust/core/#error-handling","text":"Rust knows two types of errors: unrecoverable errors - will force the code to stop recoverable errors - will be handled within the code Use the unrecoverable errors sparsely. If a library use a recoverable error the caller always has the option to make it unrecoverable but not the other way around.","title":"Error Handling"},{"location":"rust/core/#unrecoverable-errors","text":"Then some bad situations occur which should not be there the panic! macro will print a failure message, unwind and clean up the stack and quit the program. The debug build will have included symbols, so that also a backtrace is possible which shows you the files and lines which brought you to the problem. If the program is called with the RUST_BACKTRACE=1 environment setting this backtrace will be displayed. Use such errors in prototyping to be later replaced or in tests. Or use it than it can't really fail.","title":"Unrecoverable errors"},{"location":"rust/core/#recoverable-errors","text":"Functions which may have an error are returning a Result enum which is defined as having two variants, Ok with an associated value and Err with an associated message. This can be checked like: let f = File::open(\"hello.txt\"); let f = match f { Ok(file) => file, Err(error) => { panic!(\"There was a problem opening the file: {:?}\", error) }, }; At first this is an recoverable error in the file::open method but then it was decided to make it in this situation unrecoverable using the panic! macro. The type of error within the Err handler here is io::Error , which is a struct provided by the standard library. This struct has a method kind that holds an io::ErrorKind value. The enum io::ErrorKind is also from the standard library and has variants representing the different kinds of errors that might result from an IO operation. The relevant variant in this example may be ErrorKind::NotFound , which indicates the file doesn\u2019t exist yet. It can be checked like: if error.kind() == ErrorKind::NotFound { ... } The Result<T, E> has some useful shortcut helpers: unwrap - if OK return the value and if Err then call panic! unwrap_or_else - like unwrap but on Err call the code in the given closure expect - do the same but use the given error message in panic message","title":"Recoverable errors"},{"location":"rust/core/#propagation","text":"This will let the error bubble up through it's call stack. The following example will check for errors and immediately propagate them: let mut f = match f { Ok(file) => file, Err(e) => return Err(e), }; As a shortcut here the ? operator may be used: let mut f = File::open(\"hello.txt\")?; // immediately return on Result::Err with it File::open(\"hello.txt\")?.read_to_string(&mut s)?; // also possibly in between calls This will also automatically convert the error type to the function's defined returning type. But it can only be used in functions returning a Result .","title":"Propagation"},{"location":"rust/core/#generics","text":"Generics are abstract stand-ins to allow different concrete types to be used. Therefore the generics has to be defined in the function signature: fn largest<T>(list: &[T]) -> T { ... } This defines that the same type which is given ad reference will be returned. So this can be used with \u00ec32 or float ... Generics can also be used in struct or enum like: enum Result<T, E> { Ok(T), Err(E), } On implementations you have to define the generics directly after imp<T> enum<T>{ ... } . The performance of generics is not slower because the compiler will transform the generics to different concrete functions which are used.","title":"Generics"},{"location":"rust/core/#where-clauses","text":"A bound can also be expressed using a where clause immediately before the opening { , rather than at the type's first mention. Instead of: fn apply<F: FnOnce()>(f: F) { f(); } It can be written as: fn apply<F>(f: F) where F: FnOnce() { f(); }","title":"WHERE clauses"},{"location":"rust/core/#traits","text":"Traits are similar to interfaces. They define behavior for multiple types. A trait contains the function signatures which has to be met: pub trait Summary { fn summarize(&self) -> String; } This can be used to ensure that types marked with this trait are only allowed on types with the defined signature. To implement a trait on a type: impl Summary for NewsArticle { fn summarize(&self) -> String { format!(\"{}, by {} ({})\", self.headline, self.author, self.location) } } Traits can only be implemented on local types. If a trait should have a default behavior this is defined as functions with body to the trait definition.","title":"Traits"},{"location":"rust/core/#boundaries","text":"A trait can be used to limit generics to only some types. pub fn notify<T: Summary>(item: T) { println!(\"Breaking news! {}\", item.summarize()); } Multiple trait bounds can also be set like T: Summary + Display but to make this better readable it can also put as where condition before the function body: fn some_function<T, U>(t: T, u: U) -> i32 where T: Display + Clone, U: Clone + Debug { You can also use traits to conditional implement methods: impl<T: Display + PartialOrd> Pair<T> { fn cmp_display(&self) { ... } } Or directly implement a trait on types matching another trait: impl<T: Display> ToString for T { ... }","title":"Boundaries"},{"location":"rust/core/#phantom-types","text":"A phantom type parameter is one that doesn't show up at runtime, but is checked statically (and only) at compile time. In combination with generics this can help to ensure type safety.","title":"Phantom Types"},{"location":"rust/core/#lifetime","text":"References have lifetimes, which are the scope for which this references are valid and help to prevent dangling references. The default lifetime can be adjusted using annotations. Problems occur if the reference of an inner variable is borrowed to an outer variable with longer lifetime. To solve this references are annotated with relations to lifetime spaces: &i32 // a reference &'a i32 // a reference with an explicit lifetime &'a mut i32 // a mutable reference with an explicit lifetime Each lifetime for it's own didn't give any help but the relation of different references with the same lifetime defined in the function signature: fn longest<'a>(x: &'a str, y: &'a str) -> &'a str { .. } This defines that all references in the parameters and the return value must have the same lifetime. Because we\u2019ve annotated the returned reference with the same lifetime parameter `a , the returned reference will also be valid for the length of the smaller of the lifetimes of x and y . Alternatively you can return an owned data type rather than a reference so the calling function is then responsible for cleaning up the value. Lifetimes are also possible in method definitions or in a struct that holds references But you won't need to add lifetime annotations anywhere because the compiler will add them automatically for known patterns.","title":"Lifetime"},{"location":"rust/core/#static","text":"The 'static lifetime is special, it keeps the references for the whole lifetime of the program. This is used on string literals which are stored in the binary.","title":"Static"},{"location":"rust/core/#iterator","text":"You can create an iterator by calling iter , into_iter , or iter_mut on a vector. You can create iterators from the other collection types in the standard library, such as hash map. You can also create iterators that do anything you want by implementing the Iterator trait on your own types. On iterators you can also use high level operations like: filter - to only select elements where the given closure is true map - call a closure on each element and replace it with the closure`s result zip - combine two iterators together as tuples collect - return a vector of values count - count the number of elements And some more.","title":"Iterator"},{"location":"rust/core/#own-iterator","text":"Each iterator has to implement the next method: impl Iterator for Counter { type Item = u32; fn next(&mut self) -> Option<Self::Item> { self.count += 1; if self.count < 6 { Some(self.count) } else { None } } }","title":"Own Iterator"},{"location":"rust/core/#object-orientation","text":"Rust is not fully object oriented but you can implement object oriented patterns with it. Structs and enums are containing data and the procedures working on them, so they are objects. Also the implementation can partly be hide by encapsulation with modules and making only some parts public. But inheritance is not really possible. Although polymorphism is possible using the traits.","title":"Object Orientation"},{"location":"rust/core/#attributes","text":"An attribute is metadata applied to some module, crate or item. This metadata can be used to/for: conditional compilation of code set crate name, version and type (binary or library) disable lints (warnings) enable compiler features (macros, glob imports, etc.) link to a foreign library mark functions as unit tests mark functions that will be part of a benchmark They are written as #[attribute] if they apply for the current module or following item and #![attribute] if they apply for the whole crate. Just like with doc comments. They also may get values...","title":"Attributes"},{"location":"rust/core/#compiler-warnings","text":"#[allow(dead_code)] - to don't warn about unused functions","title":"Compiler Warnings"},{"location":"rust/core/#conditional-compiling","text":"This allows to compile parts only on specific settings like OS: #[cfg(target_os = \"linux\")] fn are_you_on_linux() { println!(\"You are running linux!\"); } #[cfg(not(target_os = \"linux\"))] fn are_you_on_linux() { println!(\"You are *not* running linux!\"); } The same may be achieved using the cfg! macro. The condition may be build using nested parts using the following helper: any like #[cfg(any(unix, windows))] all like #[cfg(all(unix, target_pointer_width = \"32\"))] not like #[cfg(not(foo))] You can also set another attribute based on a cfg variable with #[cfg_attr(a, b)] . This will set attribute b , but only if attribute a is set.","title":"Conditional Compiling"},{"location":"rust/core/#custom-conditions","text":"Using Cargo, they get set in the [features] section of your Cargo.toml: [features] # no features by default default = [] # Add feature \"foo\" here, then you can use it. # Our \"foo\" feature depends on nothing else. foo = [] #[cfg(feature = \"foo\")] mod foo { } Compile this using cargo build , no additional flag will be send to the rustc compiler (default). But using cargo build --features \"foo\" it will send the foo flag to rustc and the output will have the mod foo in it. {!docs/abbreviations.txt!}","title":"Custom Conditions"},{"location":"rust/directory/","text":"Directory Structure We follow the best practice structure here which is also made by cargo . The structure can be a workspace with multiple packages (in sub directories) or a single package. Cargo.toml / Cargo.lock This contains some metadata and is stored in the root of the workspace and package. The Manifest Format src The main directory for all sources of the library or binary in the package. main.rs - the start point of a binary lib.rs - the start point of a library Modules with submodules will have sub directories. Here the mod.rs contains the parent module and the child modules are there as separate files or directories with their files. tests Containing integration tests of each package. targets Everything created while building, debugging and testing will be made here. It is also the folder you find your created binaries only in the top level workspace or package. {!docs/abbreviations.txt!}","title":"Directory Structure"},{"location":"rust/directory/#directory-structure","text":"We follow the best practice structure here which is also made by cargo . The structure can be a workspace with multiple packages (in sub directories) or a single package.","title":"Directory Structure"},{"location":"rust/directory/#cargotoml-cargolock","text":"This contains some metadata and is stored in the root of the workspace and package. The Manifest Format","title":"Cargo.toml / Cargo.lock"},{"location":"rust/directory/#src","text":"The main directory for all sources of the library or binary in the package. main.rs - the start point of a binary lib.rs - the start point of a library Modules with submodules will have sub directories. Here the mod.rs contains the parent module and the child modules are there as separate files or directories with their files.","title":"src"},{"location":"rust/directory/#tests","text":"Containing integration tests of each package.","title":"tests"},{"location":"rust/directory/#targets","text":"Everything created while building, debugging and testing will be made here. It is also the folder you find your created binaries only in the top level workspace or package. {!docs/abbreviations.txt!}","title":"targets"},{"location":"rust/modules/","text":"Modules To split your code into organized and reusable parts you can extract functions and other code into modules. Definitions within the module can be public or private. By default everything in a module has private visibility. The module itself is created using mod with the name off the module and the contents as code block: mod network { fn connect() { } } Visibility Only the public items of a module can be accessed from outside the module scope. You mark functions as public using the pub modifier before: mod network { pub fn connect() { } } The same goes for structs, their fields and methods. Files Modules can be moved into separate files to have better overview over the code. As an example you place a client module's code into client.rs file and include it in the main like this: mod client; Modules can also be set within other modules. Modules with submodules will be made as directories and the mod.rs contains the parent module and the child modules are there as separate files or directories with their files. Namespaces To use other namespaces you can always specify them completely. But to make it easier you may import the namespace with use into the current file. After that you can directly use them without the full name. use a::series::of; Within modules you can also use relative paths from the current module, or type it with self:: the current crate with crate:: the parent module using super:: as prefix the root using :: as prefix Using asterisk you can also import all names of a namespace but this may be problematic. Libraries To make it more reusable also for other projects also make a library using cargo new <name> --lib . This will create a library which can be included in different other binaries or libraries. The main file of the library is lib.rs which will look like the following for a start: #[cfg(test)] mod tests { #[test] fn it_works() { assert_eq!(2 + 2, 4); } } Usage To use a library it have to be added in Cargo.toml . It can also be imported using extern crate xxxx; within the main.rs or lib.rs but this mostly is not necessary. After this the namespace can be imported using use . Reexports To make elements deep in your structure easily selectable for the libraries uses then can be reexported by pub use <fullpath> . After that they can directly be used under the library. Workspace A workspace is a set of related packages that are developed in tandem. Within the workspace directory there should be a Cargo.toml file that will configure the entire workspace. This file will have a [workspace] section that will allow us to add members to the workspace by specifying the path to them: [workspace] members = [ \"adder\", \"add_lib\", ] Within this directory the packages will be created using cargo new . The workspace has one target directory at the top level for the compiled artifacts of all packages to be placed into. Build and tests from the top level directory will always do it for all. Within tests you may select the package to test with cargo test -p add-one . Within the packages you have to define the dependencies between the workspace packages with the relative path: [dependencies] add_lib = { path = \"../add_lib\" } {!docs/abbreviations.txt!}","title":"Modules"},{"location":"rust/modules/#modules","text":"To split your code into organized and reusable parts you can extract functions and other code into modules. Definitions within the module can be public or private. By default everything in a module has private visibility. The module itself is created using mod with the name off the module and the contents as code block: mod network { fn connect() { } }","title":"Modules"},{"location":"rust/modules/#visibility","text":"Only the public items of a module can be accessed from outside the module scope. You mark functions as public using the pub modifier before: mod network { pub fn connect() { } } The same goes for structs, their fields and methods.","title":"Visibility"},{"location":"rust/modules/#files","text":"Modules can be moved into separate files to have better overview over the code. As an example you place a client module's code into client.rs file and include it in the main like this: mod client; Modules can also be set within other modules. Modules with submodules will be made as directories and the mod.rs contains the parent module and the child modules are there as separate files or directories with their files.","title":"Files"},{"location":"rust/modules/#namespaces","text":"To use other namespaces you can always specify them completely. But to make it easier you may import the namespace with use into the current file. After that you can directly use them without the full name. use a::series::of; Within modules you can also use relative paths from the current module, or type it with self:: the current crate with crate:: the parent module using super:: as prefix the root using :: as prefix Using asterisk you can also import all names of a namespace but this may be problematic.","title":"Namespaces"},{"location":"rust/modules/#libraries","text":"To make it more reusable also for other projects also make a library using cargo new <name> --lib . This will create a library which can be included in different other binaries or libraries. The main file of the library is lib.rs which will look like the following for a start: #[cfg(test)] mod tests { #[test] fn it_works() { assert_eq!(2 + 2, 4); } }","title":"Libraries"},{"location":"rust/modules/#usage","text":"To use a library it have to be added in Cargo.toml . It can also be imported using extern crate xxxx; within the main.rs or lib.rs but this mostly is not necessary. After this the namespace can be imported using use .","title":"Usage"},{"location":"rust/modules/#reexports","text":"To make elements deep in your structure easily selectable for the libraries uses then can be reexported by pub use <fullpath> . After that they can directly be used under the library.","title":"Reexports"},{"location":"rust/modules/#workspace","text":"A workspace is a set of related packages that are developed in tandem. Within the workspace directory there should be a Cargo.toml file that will configure the entire workspace. This file will have a [workspace] section that will allow us to add members to the workspace by specifying the path to them: [workspace] members = [ \"adder\", \"add_lib\", ] Within this directory the packages will be created using cargo new . The workspace has one target directory at the top level for the compiled artifacts of all packages to be placed into. Build and tests from the top level directory will always do it for all. Within tests you may select the package to test with cargo test -p add-one . Within the packages you have to define the dependencies between the workspace packages with the relative path: [dependencies] add_lib = { path = \"../add_lib\" } {!docs/abbreviations.txt!}","title":"Workspace"},{"location":"rust/phantom/","text":"Phantom Types DSL type checking Rust supports generics. A phantom type is a type parameter that isn't used in the definition of the type. Imagine a datatype that is used to represent types in a simple DSL: enum T { TI(int), TS(Box<str>) } The type T has two constructors. A TI representing an integer and a TS representing a string. The Box type is used to get an owned pointer to a string allocated on the heap. We have two functions that operate on this type. One to add two integers and the other to concatenate two strings: fn plus (lhs: T, rhs: T) -> T fn concat (lhs: T, rhs: T) -> T Using these functions looks like: let d1 = TI(1); let d2 = TI(2); let x = plus(d1, d2); display(&x); let d1 = TS(Box::new(\"Hello, \")); let d2 = TS(Box::new(\"World\")); let y = concat (d1,d2); display(&y); Unfortunately with the type definition of T it is possible to add an integer and a string: let d1 = TI(1); let d2 = TS(Box::new(\"Hello, \")); let x = plus(d1, d2); display(&x); This won't be caught at compile time but, assuming the plus function aborts on incorrect types, will fail at runtime. A full example that demonstrates this available in test1.rs. Compiling with rustc test.rs and running the resulting test produces: int: 3 str: Hello, World task <unnamed> failed at 'error', test1.rs:12 We can detect this issue at compile time by using phantom types. This involves changing the T type to be: enum T<A> { TI(int), TS(Box<str>) } Notice that the type index A does not appear anywhere in the type definition on the right hand side. The definitions of plus and concat are changed to have a different type for the type index A: fn plus (lhs: T<int>, rhs: T<int>) -> T<int> fn concat (lhs: T<~str>, rhs: T<~str>) -> T<~str> We also have to change the code that creates the TI and TS types. Code like the following will successfully type check: let d1 = TI(1); let d2 = TS(Box::new(\"hello\")); let x = plus(d1, d2); This is because d1 and d2 get their types inferred as T<int> due to plus requiring two T<int> types. What's needed is to constrain the types of d1 and d2 to T<int> and T<Box<str>> at the point of definition. This can be done with: let d1: T<int> = TI(1); let d2: T<~str> = TS(Box::new(\"foo\")); let x = plus(d1, d2); Unfortunately this will still work: let d1: T<int> = TI(1); let d2: T<int> = TS(Box::new(\"foo\")); let x = plus(d1, d2); A better approach is to not use the constructors for the T<A> enum and instead create our own: fn make_int (i: int) -> T<int> { TI(i) } fn make_str (s: Box<str>>) -> T<Box<str>> { TS(s) } Using these to create our values will ensure that they have the type of the argument passed to the function. Passing the wrong type is now an error: let d1 = make_int(1); let d2 = make_str(Box::new(\"foo\")); let x = plus(d1, d2); In real code it's best to ensure that the constructors for T<A> aren't public to ensure that callers from outside the module can't create them and subvert our attempts at better type checking. This code also ensures that the result of a plus cannot be used in a concat and vice versa. The following code produces a compile error: fn main() { let d1 = TI(1); let d2 = TI(2); let x = plus(d1, d2); display(&x); let d1 = TS(Box::new(\"Hello, \")); let d2 = TS(Box::new(\"World\")); let y = concat (d1,d2); display(&y); // Compile error here let z = concat(x, y); display(&z); } The full example is in test2.rs and compiling it results in: test2.rs:45:17: 45:18 error: mismatched types: expected `T<~str>` but found `T<int>` (expected ~str but found int) test2.rs:45 let z = concat(x, y); ^ Commenting out the offending code allows the program to compile and run without errors. There is no runtime overhead or additional code generated when using phantom types. The types are erased at compile time and the resulting program is as if they hadn't been used at all - except that you know the code won't have the runtime failure as it passed type checking. Safer strings Another use of phantom types is for providing a taint to string types so they can't be used in an unsafe manner. An example of this is generating HTML template pages as the response to an HTTP request. The page is generated by concatenating various strings together. A string that comes from user input must be properly escaped before it is allowed to be embedded in an HTML page. In this example we have a type representing an HTML fragment used to compose an HTML page to be generated: enum Fragment { Str(Box<str>) } It only has one constructor, Str, which holds the unique string containing the page data. A render_page function displays a complete page: fn get_str(s: Fragment) -> Box<str> { match s { Str(s) => { s } } } fn render_page(s: Fragment) { println(get_str(s)); } Functions exist to generate the head and body of the page with the latter including some form of data obtained elsewhere. Perhaps user input or the output of some other process: fn get_head() -> Fragment { Str(Box::new(\"<head></head>\")) } fn get_body(s: Fragment) -> Fragment { Str(\"<body>\" + get_str(s) + \"</body>\") } Data from untrusted sources needs to be 'blessed'. This should perform any escaping necessary to make the data safe to embed in the web page. The following is a main program that generates and displays the page: fn main() { let user_input = get_user_input(~\"<script>alert('oops')</script>\"); let page = generate_page(get_head(), get_body(user_input)); render_page(page); } This compiles and runs. The full example for trying it out is in test3.rs. The output is: <html><head></head><body><script>alert('oops')</script></body></html> Unfortunately we forgot to call bless and the user input was added unescaped. Ideally this should be a compile time error. If we add a phantom type to the Fragment class we can differentiate between safe and unsafe strings with no runtime overhead: struct Safe; struct Unsafe; enum Fragment<T> { Str(Box<str>) } All the generation functions now require safe fragments: fn generate_page(head: Fragment<Safe>, body: Fragment<Safe>) -> Fragment<Safe> fn get_head() -> Fragment<Safe> fn get_body(s: Fragment<Safe>) -> Fragment<Safe> fn render_page(s: Fragment<Safe>) fn bless(s: Fragment<Unsafe>) -> Fragment<Safe> The exception is the function that obtains untrusted data. It returns an unsafe fragment: fn get_user_input(s: Box<str>) -> Fragment<Unsafe> Now the code that forgets to call bless (in test4.rs) results in a compile error: test4.rs:41:36: 41:46 error: mismatched types: expected `Fragment<Safe>` but found `Fragment<Unsafe>` (expected struct Safe but found struct Unsafe) test4.rs:41 get_body(user_input)); ^~~~~~~~~~ This approach can be used to help wherever untrusted data shouldn't be mixed with trusted data. Constructing SQL queries and generating URL's are other examples. Servo One of the uses of phantom types in Servo is in the Node type: // A phantom type representing the script task's view of this node. Script is able to mutate /// nodes but may not access layout data. pub struct ScriptView; /// A phantom type representing the layout task's view of the node. Layout is not allowed to mutate /// nodes but may access layout data. pub struct LayoutView; /// An HTML node. /// /// `View` describes extra data associated with this node that this task has access to. For /// the script task, this is the unit type `()`. For the layout task, this is /// `layout::aux::LayoutData`. pub struct Node<View> { ... } A node has fields that should only be accessed or mutated on certain tasks. The script task can mutate nodes but cannot access data related to layout. The layout task is not allowed to mutate the node but can access the layout data. The prevents data races amongst the fields in the node. The is implemented using phantom types. The node type is indexed over a View which can be ScriptView or LayoutView . There are methods implemented for Node<ScriptView> which makes them only accessible to the script task: impl Node<ScriptView> { pub fn .... } An example of where the layout task gets the LayoutView version of a node is in handle_reflow . This takes a Reflow structure: pub struct Reflow { /// The document node. document_root: AbstractNode<ScriptView>, /// The style changes that need to be done. damage: DocumentDamage, /// The goal of reflow: either to render to the screen or to flush layout info for script. goal: ReflowGoal, /// The URL of the page. url: Url, /// The channel through which messages can be sent back to the script task. script_chan: ScriptChan, /// The current window size. window_size: Size2D<uint>, /// The channel that we send a notification to. script_join_chan: Chan<()>, } Note the document_root is an AbstractNode<ScriptView> . Inside the handle_reflow function in the layout task it does: /// The high-level routine that performs layout tasks. fn handle_reflow(&mut self, data: &Reflow) { // FIXME: Isolate this transmutation into a \"bridge\" module. let node: &AbstractNode<LayoutView> = unsafe { transmute(&data.document_root) }; ... } The function transmute is a cast. In this case it is converting the AbstractNode<ScriptView> to an AbstractNode<LayoutView> . The rest of the reflow deals with this so it can be proven to not access the script portions of the node and therefore should not have data races in those parts. Node simplified I've attempted to do a simplified example in Rust to test this type of phantom type usage. In layout.rs I have a basic Node class that is indexed over the views: pub struct ScriptView; pub struct LayoutView; pub struct LayoutData { field3: ~str, } impl LayoutData { pub fn new() -> LayoutData { LayoutData { field3: ~\"layoutdata\" } } } pub struct Node<View> { field1: ~str, field2: ~str, priv layout_data: Option<@mut LayoutData> } Note that layout_data is private. A method implemented on Node<LayoutView> provides public access to it: impl Node<LayoutView> { pub fn layout_data(&self) -> Option<@mut LayoutData> { self.layout_data } } Only the layout task has the LayoutView of Node so only it can get access to the layout_data method. A LayoutView is obtained by casting (using transmute): impl<View> Node<View> { pub fn change_view<View>(&self) -> &Node<View> { unsafe { transmute(self) } } } The file main.rc contains the main function that tests these layout types and functions: mod layout; pub fn main() { use layout::\\*; let a = ~Node::new(); std::io::println (a.field1); std::io::println (a.field2); let b: &Node<LayoutView> = a.change_view(); match (b.layout_data()) { None => { std::io::println(\"no layout data\"); } Some (data) => { std::io::println(data.field3); } } } This shows accessing the fields from the ScriptView view of the Node then switching to the LayoutView to get access to the layout_data. Trying to modify the non-layout fields with the LayoutView should fail. Build the example with rustc main.rc and make sure main.rc and layout.rs are in the current directory. The Servo code is quite a bit more complex than this but hopefully it helps with understanding the idiom of phantom type usage in nodes. If I've made any mistakes in my understanding of how things work, please let me know! {!docs/abbreviations.txt!}","title":"Phantom Types"},{"location":"rust/phantom/#phantom-types","text":"","title":"Phantom Types"},{"location":"rust/phantom/#dsl-type-checking","text":"Rust supports generics. A phantom type is a type parameter that isn't used in the definition of the type. Imagine a datatype that is used to represent types in a simple DSL: enum T { TI(int), TS(Box<str>) } The type T has two constructors. A TI representing an integer and a TS representing a string. The Box type is used to get an owned pointer to a string allocated on the heap. We have two functions that operate on this type. One to add two integers and the other to concatenate two strings: fn plus (lhs: T, rhs: T) -> T fn concat (lhs: T, rhs: T) -> T Using these functions looks like: let d1 = TI(1); let d2 = TI(2); let x = plus(d1, d2); display(&x); let d1 = TS(Box::new(\"Hello, \")); let d2 = TS(Box::new(\"World\")); let y = concat (d1,d2); display(&y); Unfortunately with the type definition of T it is possible to add an integer and a string: let d1 = TI(1); let d2 = TS(Box::new(\"Hello, \")); let x = plus(d1, d2); display(&x); This won't be caught at compile time but, assuming the plus function aborts on incorrect types, will fail at runtime. A full example that demonstrates this available in test1.rs. Compiling with rustc test.rs and running the resulting test produces: int: 3 str: Hello, World task <unnamed> failed at 'error', test1.rs:12 We can detect this issue at compile time by using phantom types. This involves changing the T type to be: enum T<A> { TI(int), TS(Box<str>) } Notice that the type index A does not appear anywhere in the type definition on the right hand side. The definitions of plus and concat are changed to have a different type for the type index A: fn plus (lhs: T<int>, rhs: T<int>) -> T<int> fn concat (lhs: T<~str>, rhs: T<~str>) -> T<~str> We also have to change the code that creates the TI and TS types. Code like the following will successfully type check: let d1 = TI(1); let d2 = TS(Box::new(\"hello\")); let x = plus(d1, d2); This is because d1 and d2 get their types inferred as T<int> due to plus requiring two T<int> types. What's needed is to constrain the types of d1 and d2 to T<int> and T<Box<str>> at the point of definition. This can be done with: let d1: T<int> = TI(1); let d2: T<~str> = TS(Box::new(\"foo\")); let x = plus(d1, d2); Unfortunately this will still work: let d1: T<int> = TI(1); let d2: T<int> = TS(Box::new(\"foo\")); let x = plus(d1, d2); A better approach is to not use the constructors for the T<A> enum and instead create our own: fn make_int (i: int) -> T<int> { TI(i) } fn make_str (s: Box<str>>) -> T<Box<str>> { TS(s) } Using these to create our values will ensure that they have the type of the argument passed to the function. Passing the wrong type is now an error: let d1 = make_int(1); let d2 = make_str(Box::new(\"foo\")); let x = plus(d1, d2); In real code it's best to ensure that the constructors for T<A> aren't public to ensure that callers from outside the module can't create them and subvert our attempts at better type checking. This code also ensures that the result of a plus cannot be used in a concat and vice versa. The following code produces a compile error: fn main() { let d1 = TI(1); let d2 = TI(2); let x = plus(d1, d2); display(&x); let d1 = TS(Box::new(\"Hello, \")); let d2 = TS(Box::new(\"World\")); let y = concat (d1,d2); display(&y); // Compile error here let z = concat(x, y); display(&z); } The full example is in test2.rs and compiling it results in: test2.rs:45:17: 45:18 error: mismatched types: expected `T<~str>` but found `T<int>` (expected ~str but found int) test2.rs:45 let z = concat(x, y); ^ Commenting out the offending code allows the program to compile and run without errors. There is no runtime overhead or additional code generated when using phantom types. The types are erased at compile time and the resulting program is as if they hadn't been used at all - except that you know the code won't have the runtime failure as it passed type checking.","title":"DSL type checking"},{"location":"rust/phantom/#safer-strings","text":"Another use of phantom types is for providing a taint to string types so they can't be used in an unsafe manner. An example of this is generating HTML template pages as the response to an HTTP request. The page is generated by concatenating various strings together. A string that comes from user input must be properly escaped before it is allowed to be embedded in an HTML page. In this example we have a type representing an HTML fragment used to compose an HTML page to be generated: enum Fragment { Str(Box<str>) } It only has one constructor, Str, which holds the unique string containing the page data. A render_page function displays a complete page: fn get_str(s: Fragment) -> Box<str> { match s { Str(s) => { s } } } fn render_page(s: Fragment) { println(get_str(s)); } Functions exist to generate the head and body of the page with the latter including some form of data obtained elsewhere. Perhaps user input or the output of some other process: fn get_head() -> Fragment { Str(Box::new(\"<head></head>\")) } fn get_body(s: Fragment) -> Fragment { Str(\"<body>\" + get_str(s) + \"</body>\") } Data from untrusted sources needs to be 'blessed'. This should perform any escaping necessary to make the data safe to embed in the web page. The following is a main program that generates and displays the page: fn main() { let user_input = get_user_input(~\"<script>alert('oops')</script>\"); let page = generate_page(get_head(), get_body(user_input)); render_page(page); } This compiles and runs. The full example for trying it out is in test3.rs. The output is: <html><head></head><body><script>alert('oops')</script></body></html> Unfortunately we forgot to call bless and the user input was added unescaped. Ideally this should be a compile time error. If we add a phantom type to the Fragment class we can differentiate between safe and unsafe strings with no runtime overhead: struct Safe; struct Unsafe; enum Fragment<T> { Str(Box<str>) } All the generation functions now require safe fragments: fn generate_page(head: Fragment<Safe>, body: Fragment<Safe>) -> Fragment<Safe> fn get_head() -> Fragment<Safe> fn get_body(s: Fragment<Safe>) -> Fragment<Safe> fn render_page(s: Fragment<Safe>) fn bless(s: Fragment<Unsafe>) -> Fragment<Safe> The exception is the function that obtains untrusted data. It returns an unsafe fragment: fn get_user_input(s: Box<str>) -> Fragment<Unsafe> Now the code that forgets to call bless (in test4.rs) results in a compile error: test4.rs:41:36: 41:46 error: mismatched types: expected `Fragment<Safe>` but found `Fragment<Unsafe>` (expected struct Safe but found struct Unsafe) test4.rs:41 get_body(user_input)); ^~~~~~~~~~ This approach can be used to help wherever untrusted data shouldn't be mixed with trusted data. Constructing SQL queries and generating URL's are other examples.","title":"Safer strings"},{"location":"rust/phantom/#servo","text":"One of the uses of phantom types in Servo is in the Node type: // A phantom type representing the script task's view of this node. Script is able to mutate /// nodes but may not access layout data. pub struct ScriptView; /// A phantom type representing the layout task's view of the node. Layout is not allowed to mutate /// nodes but may access layout data. pub struct LayoutView; /// An HTML node. /// /// `View` describes extra data associated with this node that this task has access to. For /// the script task, this is the unit type `()`. For the layout task, this is /// `layout::aux::LayoutData`. pub struct Node<View> { ... } A node has fields that should only be accessed or mutated on certain tasks. The script task can mutate nodes but cannot access data related to layout. The layout task is not allowed to mutate the node but can access the layout data. The prevents data races amongst the fields in the node. The is implemented using phantom types. The node type is indexed over a View which can be ScriptView or LayoutView . There are methods implemented for Node<ScriptView> which makes them only accessible to the script task: impl Node<ScriptView> { pub fn .... } An example of where the layout task gets the LayoutView version of a node is in handle_reflow . This takes a Reflow structure: pub struct Reflow { /// The document node. document_root: AbstractNode<ScriptView>, /// The style changes that need to be done. damage: DocumentDamage, /// The goal of reflow: either to render to the screen or to flush layout info for script. goal: ReflowGoal, /// The URL of the page. url: Url, /// The channel through which messages can be sent back to the script task. script_chan: ScriptChan, /// The current window size. window_size: Size2D<uint>, /// The channel that we send a notification to. script_join_chan: Chan<()>, } Note the document_root is an AbstractNode<ScriptView> . Inside the handle_reflow function in the layout task it does: /// The high-level routine that performs layout tasks. fn handle_reflow(&mut self, data: &Reflow) { // FIXME: Isolate this transmutation into a \"bridge\" module. let node: &AbstractNode<LayoutView> = unsafe { transmute(&data.document_root) }; ... } The function transmute is a cast. In this case it is converting the AbstractNode<ScriptView> to an AbstractNode<LayoutView> . The rest of the reflow deals with this so it can be proven to not access the script portions of the node and therefore should not have data races in those parts. Node simplified I've attempted to do a simplified example in Rust to test this type of phantom type usage. In layout.rs I have a basic Node class that is indexed over the views: pub struct ScriptView; pub struct LayoutView; pub struct LayoutData { field3: ~str, } impl LayoutData { pub fn new() -> LayoutData { LayoutData { field3: ~\"layoutdata\" } } } pub struct Node<View> { field1: ~str, field2: ~str, priv layout_data: Option<@mut LayoutData> } Note that layout_data is private. A method implemented on Node<LayoutView> provides public access to it: impl Node<LayoutView> { pub fn layout_data(&self) -> Option<@mut LayoutData> { self.layout_data } } Only the layout task has the LayoutView of Node so only it can get access to the layout_data method. A LayoutView is obtained by casting (using transmute): impl<View> Node<View> { pub fn change_view<View>(&self) -> &Node<View> { unsafe { transmute(self) } } } The file main.rc contains the main function that tests these layout types and functions: mod layout; pub fn main() { use layout::\\*; let a = ~Node::new(); std::io::println (a.field1); std::io::println (a.field2); let b: &Node<LayoutView> = a.change_view(); match (b.layout_data()) { None => { std::io::println(\"no layout data\"); } Some (data) => { std::io::println(data.field3); } } } This shows accessing the fields from the ScriptView view of the Node then switching to the LayoutView to get access to the layout_data. Trying to modify the non-layout fields with the LayoutView should fail. Build the example with rustc main.rc and make sure main.rc and layout.rs are in the current directory. The Servo code is quite a bit more complex than this but hopefully it helps with understanding the idiom of phantom type usage in nodes. If I've made any mistakes in my understanding of how things work, please let me know! {!docs/abbreviations.txt!}","title":"Servo"},{"location":"rust/standard/","text":"Standard Library Formatted Output Some macros help easily write output: format! - write formatted text to String println! - write line to standard output print! - write formatted text to standard output without additional newline eprintln! - write line to error output eprint! - write formatted text to error output without additional newline They all allow to use placeholder: {} - for text output using Display trait {:?} - for debugging using Debug trait Within the curly braces a number or identifier can be added as prefix to specify the element number or variable name. And for the Debug trait a format specifier can be added after the colon separator: fill character with alignment < - left-aligned ^ - center-aligned > - right-aligned sign + - always print the sign #? - use pretty print as alternate format #x - precede with '0x' #X - precede with '0X' #b - precede with '0b' #o - precede with '0o' 0 - pad with 0 behind the sign width in characters (can also be given as parameter reference like '0$' or ' $') precision count gives the truncate length .N - precision as integer .N$ - parameter reference .* - precision defined by next parameter type ? - default format x - lower-case hexadecimal X - upper-case hexadecimal If curly braces should be used as normal character you have to double them as {{ or }} . Vector Vectors allow you to store more than one value in a single data structure that puts all the values next to each other in memory. Vectors can only store values of the same type. let v: Vec<i32> = Vec::new(); The vec! macro is another alternative to create a new vector that holds the values you give it. let v = vec![1, 2, 3]; To read data from a vector you have two possibilities: let v = vec![1, 2, 3, 4, 5]; let third = &v[2]; // &i32 let third = v.get(2); // Option<&i32> Line 2 gives you an reference to the value while line 3 gives you an Option enum with the value. This last access is more secure because you will get the None value if the index is out of the current bounds of the vector. Update To update the values of a mutable vector you can use the push method: let v: Vec<i32> = Vec::new(); v.push(5); Iterating A vector is iterateable so you can just use the for loop to step over each element: let v = vec![100, 32, 57]; for i in &v { println!(\"{}\", i); } Store different Types With the use of enum you can trick vectors to store different types as enum can hold different types for you: enum SpreadsheetCell { Int(i32), Float(f64), Text(String), } let row = vec![ SpreadsheetCell::Int(3), SpreadsheetCell::Text(String::from(\"blue\")), SpreadsheetCell::Float(10.12), ]; String This is a special collection of UTF-8 characters stored on the heap as growable and mutable data object. The string slice str in the core is a fixed string which cannot be changed. A String is creating through the new method or with some initial data using the to_string or String::from methods: let mut s = String::new(); let s = \"initial data\".to_string(); let s = String::from(\"initial contents\"); Manipulation Using the push_str method you can concat another string to the first one: let mut s = String::from(\"foo\"); s.push_str(\"bar\"); The push methods works identical, but only adds a single character to the string. The + operator can easily be used to concat two strings. let s1 = String::from(\"Hello, \"); let s2 = String::from(\"world!\"); let s3 = s1 + &s2; // Note s1 has been moved here and can no longer be used An alternative is to use the format! macro: let s = format!(\"{}-{}-{}\", s1, s2, s3); This won't take ownership of any of the variables. Index Access using index numbers is critical because the UTF-8 strings may have characters with more than 1 byte. Better solution is to work with the chars method: for c in \"\u0928\u092e\u0938\u094d\u0924\u0947\".chars() { println!(\"{}\", c); } Hash Map The HashMap stores a mapping of keys and values. But as this is not in the prelude you have to include it first. You can create an empty hash map with new and add elements with insert and like in vectors all keys have to have the same type and all values have to have the same type: use std::collections::HashMap; let mut scores = HashMap::new(); scores.insert(String::from(\"Blue\"), 10); scores.insert(String::from(\"Yellow\"), 50); A hash map can be also created from an vector using the collect method. In the example the insert is used to add new entries but it can also be used to update an already existing value. Access Similar to vectors you can use the get method to retrieve a specific value as Option : let score = scores.get(&team_name); To insert a value if it doesn't exist this is possible using entry : scores.entry(String::from(\"Yellow\")).or_insert(50); And as entry and or_insert returns a mutable reference you can work with it: let count = scores.entry(String::from(\"Yellow\")).or_insert(0); *count += 1; Smart Pointer Box To hold needed memory on heap low you can use Box<T> in any structure to store a pointer to the data structure instead of the whole data structure. This is needed in recursive data structures or there the size of the containing element can vary much. This also implements the Deref trait so you can use normal dereferencing with it. Reference Counted When a single value might have multiple owners, Rust has a type called Rc<T> , which is an abbreviation for reference counting. Such a value shouldn\u2019t be cleaned up unless it doesn\u2019t have any pointers to it. It keeps track of the number of references to a value which determines whether or not a value is still in use. To make another instance of an Rc<T> instance the call to Rc::clone only increments the reference count, which doesn\u2019t take much time. By dropping the variables, the counter will be decreased and by reaching 0 the value will be dropped. Reference cycles can lead to memory leaks to prevent this you can use Weak<T> instead if a back-link in the structure is set. Also there are Arc<T> which is an atomic reference counted which is needed for thread safety. Interior mutability This is a design pattern in Rust that allows you to mutate data even when there are immutable references to that data. To mutate data, the pattern uses unsafe code inside a data structure to bend Rust\u2019s usual rules that govern mutation and borrowing but wrapped in a safe API, and the outer type is still immutable. This is implemented in RefCell<T> . Deref Trait For own types like the MyBox here, you have to implement the Deref trait to allow type referencing. use std::ops::Deref; impl<T> Deref for MyBox<T> { type Target = T; fn deref(&self) -> &T { &self.0 // MyBox is a tuple with one element } } To work on mutable references you have to implement the DerefMut trait. Drop Trait With the Drop trait you define what to do when a value is about to go out of scope. This can be implemented on any type to release resources like files or network connections. Rust doesn\u2019t let you call the Drop trait\u2019s drop method manually so you have to call the drop function if you want to force a value to be dropped before the end of its scope. Concurrency / Parallelism Concurrent programming, where different parts of a program execute independently, and parallel programming, where different parts of a program execute at the same time and take advantage of their multiple processors. Rust standard library only provides an implementation of 1:1 threading there each programming thread belongs to one OS thread but there are crates which allow also M:N threading. This is only a short overview, so read more about that in the standard library and have a look at the existing crates dealing with concurrency. Spawning To create a new thread, we call the thread::spawn function and pass it a closure containing the code we want to run in the new thread. use std::thread; use std::time::Duration; fn main() { let handle = thread::spawn(|| { // do something in thread }); // do something in main handle.join().unwrap(); } The join method on the returned spawn handle will wait till it has finished. The main and thread parts between will run in parallel. With move before the closure's parameter list the ownership will be transfered to the thread. Channels A channel is used to communicate between threads, contains of a transmitter and a receiver and is created using mpsc::channel . The way Rust implements channels means a channel can have multiple sending ends that produce values but only one receiving end that consumes those values. use std::sync::mpsc; fn main() { let (tx, rx) = mpsc::channel(); thread::spawn(move || { let val = String::from(\"hi\"); tx.send(val).unwrap(); }); let received = rx.recv().unwrap(); } By sending a value it is moved, so the variable is no longer allowed after that. Multiple transmitters are possible by cloning through let tx2 = mpsc::Sender::clone(&tx); . The receiving end of a channel has two useful methods recv to wait for a message and try_recv which won't block. You can also use the receiver as an iterator to work on multiple messages, which will end if the channel is closed: for received in rx { ... } Mutex In Rust also shared memory can be done safely. Therefore Mutex<T> (is an abbreviation for mutual exclusion) allows only one thread to access some data at any given time. To access the data in a mutex, a thread must first signal that it wants access by asking to acquire the mutex's lock. Using lock() you get a mutable reference back which is automatically unlocked on drop of the variable. File management The prelude is loaded to import some common file management methods like read_to_string in the following example: use std::fs::File; use std::io::prelude::*; ... let mut f = File::open(filename).expect(\"file not found\"); let mut contents = String::new(); f.read_to_string(&mut contents).expect(\"something went wrong reading the file\"); CLI Interaction Argument parsing You can use different crates to access parameters but the low level method is to use env::args : use std::env; ... let args: Vec<String> = env::args().collect(); Exit To stop the program without given debugging information like panic! will do you can use process::exit with the code given to the caller: use std::process; ... process::exit(1); {!docs/abbreviations.txt!}","title":"Standard Library"},{"location":"rust/standard/#standard-library","text":"","title":"Standard Library"},{"location":"rust/standard/#formatted-output","text":"Some macros help easily write output: format! - write formatted text to String println! - write line to standard output print! - write formatted text to standard output without additional newline eprintln! - write line to error output eprint! - write formatted text to error output without additional newline They all allow to use placeholder: {} - for text output using Display trait {:?} - for debugging using Debug trait Within the curly braces a number or identifier can be added as prefix to specify the element number or variable name. And for the Debug trait a format specifier can be added after the colon separator: fill character with alignment < - left-aligned ^ - center-aligned > - right-aligned sign + - always print the sign #? - use pretty print as alternate format #x - precede with '0x' #X - precede with '0X' #b - precede with '0b' #o - precede with '0o' 0 - pad with 0 behind the sign width in characters (can also be given as parameter reference like '0$' or ' $') precision count gives the truncate length .N - precision as integer .N$ - parameter reference .* - precision defined by next parameter type ? - default format x - lower-case hexadecimal X - upper-case hexadecimal If curly braces should be used as normal character you have to double them as {{ or }} .","title":"Formatted Output"},{"location":"rust/standard/#vector","text":"Vectors allow you to store more than one value in a single data structure that puts all the values next to each other in memory. Vectors can only store values of the same type. let v: Vec<i32> = Vec::new(); The vec! macro is another alternative to create a new vector that holds the values you give it. let v = vec![1, 2, 3]; To read data from a vector you have two possibilities: let v = vec![1, 2, 3, 4, 5]; let third = &v[2]; // &i32 let third = v.get(2); // Option<&i32> Line 2 gives you an reference to the value while line 3 gives you an Option enum with the value. This last access is more secure because you will get the None value if the index is out of the current bounds of the vector.","title":"Vector"},{"location":"rust/standard/#update","text":"To update the values of a mutable vector you can use the push method: let v: Vec<i32> = Vec::new(); v.push(5);","title":"Update"},{"location":"rust/standard/#iterating","text":"A vector is iterateable so you can just use the for loop to step over each element: let v = vec![100, 32, 57]; for i in &v { println!(\"{}\", i); }","title":"Iterating"},{"location":"rust/standard/#store-different-types","text":"With the use of enum you can trick vectors to store different types as enum can hold different types for you: enum SpreadsheetCell { Int(i32), Float(f64), Text(String), } let row = vec![ SpreadsheetCell::Int(3), SpreadsheetCell::Text(String::from(\"blue\")), SpreadsheetCell::Float(10.12), ];","title":"Store different Types"},{"location":"rust/standard/#string","text":"This is a special collection of UTF-8 characters stored on the heap as growable and mutable data object. The string slice str in the core is a fixed string which cannot be changed. A String is creating through the new method or with some initial data using the to_string or String::from methods: let mut s = String::new(); let s = \"initial data\".to_string(); let s = String::from(\"initial contents\");","title":"String"},{"location":"rust/standard/#manipulation","text":"Using the push_str method you can concat another string to the first one: let mut s = String::from(\"foo\"); s.push_str(\"bar\"); The push methods works identical, but only adds a single character to the string. The + operator can easily be used to concat two strings. let s1 = String::from(\"Hello, \"); let s2 = String::from(\"world!\"); let s3 = s1 + &s2; // Note s1 has been moved here and can no longer be used An alternative is to use the format! macro: let s = format!(\"{}-{}-{}\", s1, s2, s3); This won't take ownership of any of the variables.","title":"Manipulation"},{"location":"rust/standard/#index","text":"Access using index numbers is critical because the UTF-8 strings may have characters with more than 1 byte. Better solution is to work with the chars method: for c in \"\u0928\u092e\u0938\u094d\u0924\u0947\".chars() { println!(\"{}\", c); }","title":"Index"},{"location":"rust/standard/#hash-map","text":"The HashMap stores a mapping of keys and values. But as this is not in the prelude you have to include it first. You can create an empty hash map with new and add elements with insert and like in vectors all keys have to have the same type and all values have to have the same type: use std::collections::HashMap; let mut scores = HashMap::new(); scores.insert(String::from(\"Blue\"), 10); scores.insert(String::from(\"Yellow\"), 50); A hash map can be also created from an vector using the collect method. In the example the insert is used to add new entries but it can also be used to update an already existing value.","title":"Hash Map"},{"location":"rust/standard/#access","text":"Similar to vectors you can use the get method to retrieve a specific value as Option : let score = scores.get(&team_name); To insert a value if it doesn't exist this is possible using entry : scores.entry(String::from(\"Yellow\")).or_insert(50); And as entry and or_insert returns a mutable reference you can work with it: let count = scores.entry(String::from(\"Yellow\")).or_insert(0); *count += 1;","title":"Access"},{"location":"rust/standard/#smart-pointer","text":"","title":"Smart Pointer"},{"location":"rust/standard/#box","text":"To hold needed memory on heap low you can use Box<T> in any structure to store a pointer to the data structure instead of the whole data structure. This is needed in recursive data structures or there the size of the containing element can vary much. This also implements the Deref trait so you can use normal dereferencing with it.","title":"Box"},{"location":"rust/standard/#reference-counted","text":"When a single value might have multiple owners, Rust has a type called Rc<T> , which is an abbreviation for reference counting. Such a value shouldn\u2019t be cleaned up unless it doesn\u2019t have any pointers to it. It keeps track of the number of references to a value which determines whether or not a value is still in use. To make another instance of an Rc<T> instance the call to Rc::clone only increments the reference count, which doesn\u2019t take much time. By dropping the variables, the counter will be decreased and by reaching 0 the value will be dropped. Reference cycles can lead to memory leaks to prevent this you can use Weak<T> instead if a back-link in the structure is set. Also there are Arc<T> which is an atomic reference counted which is needed for thread safety.","title":"Reference Counted"},{"location":"rust/standard/#interior-mutability","text":"This is a design pattern in Rust that allows you to mutate data even when there are immutable references to that data. To mutate data, the pattern uses unsafe code inside a data structure to bend Rust\u2019s usual rules that govern mutation and borrowing but wrapped in a safe API, and the outer type is still immutable. This is implemented in RefCell<T> .","title":"Interior mutability"},{"location":"rust/standard/#deref-trait","text":"For own types like the MyBox here, you have to implement the Deref trait to allow type referencing. use std::ops::Deref; impl<T> Deref for MyBox<T> { type Target = T; fn deref(&self) -> &T { &self.0 // MyBox is a tuple with one element } } To work on mutable references you have to implement the DerefMut trait.","title":"Deref Trait"},{"location":"rust/standard/#drop-trait","text":"With the Drop trait you define what to do when a value is about to go out of scope. This can be implemented on any type to release resources like files or network connections. Rust doesn\u2019t let you call the Drop trait\u2019s drop method manually so you have to call the drop function if you want to force a value to be dropped before the end of its scope.","title":"Drop Trait"},{"location":"rust/standard/#concurrency-parallelism","text":"Concurrent programming, where different parts of a program execute independently, and parallel programming, where different parts of a program execute at the same time and take advantage of their multiple processors. Rust standard library only provides an implementation of 1:1 threading there each programming thread belongs to one OS thread but there are crates which allow also M:N threading. This is only a short overview, so read more about that in the standard library and have a look at the existing crates dealing with concurrency.","title":"Concurrency / Parallelism"},{"location":"rust/standard/#spawning","text":"To create a new thread, we call the thread::spawn function and pass it a closure containing the code we want to run in the new thread. use std::thread; use std::time::Duration; fn main() { let handle = thread::spawn(|| { // do something in thread }); // do something in main handle.join().unwrap(); } The join method on the returned spawn handle will wait till it has finished. The main and thread parts between will run in parallel. With move before the closure's parameter list the ownership will be transfered to the thread.","title":"Spawning"},{"location":"rust/standard/#channels","text":"A channel is used to communicate between threads, contains of a transmitter and a receiver and is created using mpsc::channel . The way Rust implements channels means a channel can have multiple sending ends that produce values but only one receiving end that consumes those values. use std::sync::mpsc; fn main() { let (tx, rx) = mpsc::channel(); thread::spawn(move || { let val = String::from(\"hi\"); tx.send(val).unwrap(); }); let received = rx.recv().unwrap(); } By sending a value it is moved, so the variable is no longer allowed after that. Multiple transmitters are possible by cloning through let tx2 = mpsc::Sender::clone(&tx); . The receiving end of a channel has two useful methods recv to wait for a message and try_recv which won't block. You can also use the receiver as an iterator to work on multiple messages, which will end if the channel is closed: for received in rx { ... }","title":"Channels"},{"location":"rust/standard/#mutex","text":"In Rust also shared memory can be done safely. Therefore Mutex<T> (is an abbreviation for mutual exclusion) allows only one thread to access some data at any given time. To access the data in a mutex, a thread must first signal that it wants access by asking to acquire the mutex's lock. Using lock() you get a mutable reference back which is automatically unlocked on drop of the variable.","title":"Mutex"},{"location":"rust/standard/#file-management","text":"The prelude is loaded to import some common file management methods like read_to_string in the following example: use std::fs::File; use std::io::prelude::*; ... let mut f = File::open(filename).expect(\"file not found\"); let mut contents = String::new(); f.read_to_string(&mut contents).expect(\"something went wrong reading the file\");","title":"File management"},{"location":"rust/standard/#cli-interaction","text":"","title":"CLI Interaction"},{"location":"rust/standard/#argument-parsing","text":"You can use different crates to access parameters but the low level method is to use env::args : use std::env; ... let args: Vec<String> = env::args().collect();","title":"Argument parsing"},{"location":"rust/standard/#exit","text":"To stop the program without given debugging information like panic! will do you can use process::exit with the code given to the caller: use std::process; ... process::exit(1); {!docs/abbreviations.txt!}","title":"Exit"},{"location":"rust/start/","text":"Getting started Start with the setup and installation of your development system to begin learning or coding in Rust. Install Rust On Linux or macOS you only need the installation script: $ curl https://sh.rustup.rs -sSf | sh && source $HOME/.cargo/env info: downloading installer Welcome to Rust! This will download and install the official compiler for the Rust programming language, and its package manager, Cargo. It will add the cargo, rustc, rustup and other commands to Cargo's bin directory, located at: /home/alex/.cargo/bin This path will then be added to your PATH environment variable by modifying the profile file located at: /home/alex/.profile You can uninstall at any time with rustup self uninstall and these changes will be reverted. Current installation options: default host triple: x86_64-unknown-linux-gnu default toolchain: stable modify PATH variable: yes 1) Proceed with installation (default) 2) Customize installation 3) Cancel installation >1 info: syncing channel updates for 'stable-x86_64-unknown-linux-gnu' info: latest update on 2018-07-10, rust version 1.27.1 (5f2b325f6 2018-07-07) info: downloading component 'rustc' 72.1 MiB / 72.1 MiB (100 %) 10.0 MiB/s ETA: 0 s info: downloading component 'rust-std' 56.2 MiB / 56.2 MiB (100 %) 10.1 MiB/s ETA: 0 s info: downloading component 'cargo' info: downloading component 'rust-docs' info: installing component 'rustc' info: installing component 'rust-std' info: installing component 'cargo' info: installing component 'rust-docs' info: default toolchain set to 'stable' stable installed - rustc 1.27.1 (5f2b325f6 2018-07-07) Rust is installed now. Great! To get started you need Cargo's bin directory ($HOME/.cargo/bin) in your PATH environment variable. Next time you log in this will be done automatically. To configure your current shell run source $HOME/.cargo/env Like shown, this will download and install everything for you. Once the Rust installation is complete, Cargo\u2019s bin directory (~/.cargo/bin \u2013 where all tools are installed) will be added in your PATH environment variable, in ~/.profile. This step only needs to be done one time, updates are done using the tool: $ rustup update info: syncing channel updates for 'stable-x86_64-unknown-linux-gnu' info: checking for self-updates stable-x86_64-unknown-linux-gnu unchanged - rustc 1.27.1 (5f2b325f6 2018-07-07) Additional Tools Some of the additional tools and other common modules needs some of the following to compile: sudo apt install pkg-config libssl-dev cmake After that you can now add the following tools are needed also for IDE support: rustup toolchain install nightly rustup component add rls-preview rust-analysis rust-src clippy rustfmt-preview cargo +nightly install racer cargo install rustsym cargo install cargo-update cargo install cargo-outdated cargo install cargo-watch See the description of all this tools . Editor As I currently do for all languages I use VS Code also for rust. Like a lot of other editors it comes with some plugins to make it a IDE for Rust. Updating code Every few years a new edition will follow and you may upgrade your code to use the new features that are incompatible with the older edition. In order to help transition Cargo can help: cargo fix --edition This will check your code, and automatically fix any issues that it can. If cargo fix can't fix something, it will print the warning that it cannot fix to the console. If you see one of these warnings, you'll have to update your code manually. If cargo fix --edition succeeds without warnings you can now switch to the new edition. To do this add the new edition key/value pair to cargo.toml .\ufffc [package] name = \"foo\" version = \"0.1.0\" authors = [\"Your Name <you@example.com>\"] edition = \"2018\" If there's no edition key, Cargo will default to Rust 2015. But in this case, we've chosen 2018, and so our code is compiling with Rust 2018! {!docs/abbreviations.txt!}","title":"Getting started"},{"location":"rust/start/#getting-started","text":"Start with the setup and installation of your development system to begin learning or coding in Rust.","title":"Getting started"},{"location":"rust/start/#install-rust","text":"On Linux or macOS you only need the installation script: $ curl https://sh.rustup.rs -sSf | sh && source $HOME/.cargo/env info: downloading installer Welcome to Rust! This will download and install the official compiler for the Rust programming language, and its package manager, Cargo. It will add the cargo, rustc, rustup and other commands to Cargo's bin directory, located at: /home/alex/.cargo/bin This path will then be added to your PATH environment variable by modifying the profile file located at: /home/alex/.profile You can uninstall at any time with rustup self uninstall and these changes will be reverted. Current installation options: default host triple: x86_64-unknown-linux-gnu default toolchain: stable modify PATH variable: yes 1) Proceed with installation (default) 2) Customize installation 3) Cancel installation >1 info: syncing channel updates for 'stable-x86_64-unknown-linux-gnu' info: latest update on 2018-07-10, rust version 1.27.1 (5f2b325f6 2018-07-07) info: downloading component 'rustc' 72.1 MiB / 72.1 MiB (100 %) 10.0 MiB/s ETA: 0 s info: downloading component 'rust-std' 56.2 MiB / 56.2 MiB (100 %) 10.1 MiB/s ETA: 0 s info: downloading component 'cargo' info: downloading component 'rust-docs' info: installing component 'rustc' info: installing component 'rust-std' info: installing component 'cargo' info: installing component 'rust-docs' info: default toolchain set to 'stable' stable installed - rustc 1.27.1 (5f2b325f6 2018-07-07) Rust is installed now. Great! To get started you need Cargo's bin directory ($HOME/.cargo/bin) in your PATH environment variable. Next time you log in this will be done automatically. To configure your current shell run source $HOME/.cargo/env Like shown, this will download and install everything for you. Once the Rust installation is complete, Cargo\u2019s bin directory (~/.cargo/bin \u2013 where all tools are installed) will be added in your PATH environment variable, in ~/.profile. This step only needs to be done one time, updates are done using the tool: $ rustup update info: syncing channel updates for 'stable-x86_64-unknown-linux-gnu' info: checking for self-updates stable-x86_64-unknown-linux-gnu unchanged - rustc 1.27.1 (5f2b325f6 2018-07-07)","title":"Install Rust"},{"location":"rust/start/#additional-tools","text":"Some of the additional tools and other common modules needs some of the following to compile: sudo apt install pkg-config libssl-dev cmake After that you can now add the following tools are needed also for IDE support: rustup toolchain install nightly rustup component add rls-preview rust-analysis rust-src clippy rustfmt-preview cargo +nightly install racer cargo install rustsym cargo install cargo-update cargo install cargo-outdated cargo install cargo-watch See the description of all this tools .","title":"Additional Tools"},{"location":"rust/start/#editor","text":"As I currently do for all languages I use VS Code also for rust. Like a lot of other editors it comes with some plugins to make it a IDE for Rust.","title":"Editor"},{"location":"rust/start/#updating-code","text":"Every few years a new edition will follow and you may upgrade your code to use the new features that are incompatible with the older edition. In order to help transition Cargo can help: cargo fix --edition This will check your code, and automatically fix any issues that it can. If cargo fix can't fix something, it will print the warning that it cannot fix to the console. If you see one of these warnings, you'll have to update your code manually. If cargo fix --edition succeeds without warnings you can now switch to the new edition. To do this add the new edition key/value pair to cargo.toml .\ufffc [package] name = \"foo\" version = \"0.1.0\" authors = [\"Your Name <you@example.com>\"] edition = \"2018\" If there's no edition key, Cargo will default to Rust 2015. But in this case, we've chosen 2018, and so our code is compiling with Rust 2018! {!docs/abbreviations.txt!}","title":"Updating code"},{"location":"rust/test/","text":"Unit Testing At its simplest, a test in Rust is a function that\u2019s annotated with the test attribute. #[cfg(test)] mod tests { use super::*; #[test] fn it_works() { assert_eq!(2 + 2, 4); } } Cargo has an test runner to execute them by calling: $ cargo test Compiling xxxx v0.1.0 (file:///projects/xxxx) Finished dev [unoptimized + debuginfo] target(s) in 0.22 secs Running target/debug/deps/xxxx-ce99bcc2479f4607 running 1 test test tests::it_works ... ok test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out Doc-tests xxxx running 0 tests test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out As seen above this will run all tests and give a summary line. The second part of the output is for documentation tests , checking that code and documentation is in sync. Structure Rust divides tests into unit tests and integration tests. Unit tests are small and more focused, testing one module in isolation at a time, and can test private interfaces. Integration tests are entirely external to your library and use your code in the same way any other external code would, using only the public interface and potentially exercising multiple modules per test. Unit Tests The convention is to create a module named tests in each file to contain the test functions and to annotate the module with cfg(test) . This tells Rust to only build this code in test mode. Integration Tests They are used to test the library's external API. They are written in a separate tests directory. Each file here will be compiled separately and looks like: extern crate adder; #[test] fn it_adds_two() { assert_eq!(4, adder::add_two(2)); } If submodules are needed they have to be moved into <name>/mod.rs to not be interpreted as tests and we include it normally using mod <name>; . To add integration tests to binary crates you have to move most code into src/lib.rs which can be tested and have a small wrapper src/main.rs which only calls the library. Documentation Tests If you use examples within your API Documentation they will be also tested to ensure they are up to date with your code. Value Checking Assertions To check that the code works correctly some assertion macros will help: assert! - check that the given expression is true assert_eq! - check that the two given values are equal assert_ne! - check that the two given values are not equal Additional to the required one or two parameters all macros will also allow an additional format string and values for a custom failure message. This additional parameters will be given to the format! macro for output. assert!( result.contains(\"Carol\"), \"Greeting did not contain name, value was `{}`\", result ); Panic To check if a panic was raised the #[shoould_panic] attribute is used. It have to be after the test attribute and can have an additional expect message which have to be a substring of the panic message: #[test] #[should_panic(expected = \"Guess value must be less than or equal to 100\")] fn greater_than_100() { Guess::new(200); } Ignoring A test can be specified with attribute #[ignore] to normally be ignored. This may be useful for expensive tests. They will only run if directly specified or with the --ignored flag. #[test] #[ignore] fn expensive_test() { // code that takes an hour to run } {!docs/abbreviations.txt!}","title":"Unit Testing"},{"location":"rust/test/#unit-testing","text":"At its simplest, a test in Rust is a function that\u2019s annotated with the test attribute. #[cfg(test)] mod tests { use super::*; #[test] fn it_works() { assert_eq!(2 + 2, 4); } } Cargo has an test runner to execute them by calling: $ cargo test Compiling xxxx v0.1.0 (file:///projects/xxxx) Finished dev [unoptimized + debuginfo] target(s) in 0.22 secs Running target/debug/deps/xxxx-ce99bcc2479f4607 running 1 test test tests::it_works ... ok test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out Doc-tests xxxx running 0 tests test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out As seen above this will run all tests and give a summary line. The second part of the output is for documentation tests , checking that code and documentation is in sync.","title":"Unit Testing"},{"location":"rust/test/#structure","text":"Rust divides tests into unit tests and integration tests. Unit tests are small and more focused, testing one module in isolation at a time, and can test private interfaces. Integration tests are entirely external to your library and use your code in the same way any other external code would, using only the public interface and potentially exercising multiple modules per test.","title":"Structure"},{"location":"rust/test/#unit-tests","text":"The convention is to create a module named tests in each file to contain the test functions and to annotate the module with cfg(test) . This tells Rust to only build this code in test mode.","title":"Unit Tests"},{"location":"rust/test/#integration-tests","text":"They are used to test the library's external API. They are written in a separate tests directory. Each file here will be compiled separately and looks like: extern crate adder; #[test] fn it_adds_two() { assert_eq!(4, adder::add_two(2)); } If submodules are needed they have to be moved into <name>/mod.rs to not be interpreted as tests and we include it normally using mod <name>; . To add integration tests to binary crates you have to move most code into src/lib.rs which can be tested and have a small wrapper src/main.rs which only calls the library.","title":"Integration Tests"},{"location":"rust/test/#documentation-tests","text":"If you use examples within your API Documentation they will be also tested to ensure they are up to date with your code.","title":"Documentation Tests"},{"location":"rust/test/#value-checking","text":"","title":"Value Checking"},{"location":"rust/test/#assertions","text":"To check that the code works correctly some assertion macros will help: assert! - check that the given expression is true assert_eq! - check that the two given values are equal assert_ne! - check that the two given values are not equal Additional to the required one or two parameters all macros will also allow an additional format string and values for a custom failure message. This additional parameters will be given to the format! macro for output. assert!( result.contains(\"Carol\"), \"Greeting did not contain name, value was `{}`\", result );","title":"Assertions"},{"location":"rust/test/#panic","text":"To check if a panic was raised the #[shoould_panic] attribute is used. It have to be after the test attribute and can have an additional expect message which have to be a substring of the panic message: #[test] #[should_panic(expected = \"Guess value must be less than or equal to 100\")] fn greater_than_100() { Guess::new(200); }","title":"Panic"},{"location":"rust/test/#ignoring","text":"A test can be specified with attribute #[ignore] to normally be ignored. This may be useful for expensive tests. They will only run if directly specified or with the --ignored flag. #[test] #[ignore] fn expensive_test() { // code that takes an hour to run } {!docs/abbreviations.txt!}","title":"Ignoring"},{"location":"rust/tools/","text":"Tooling Rustup This is the version manager of rust which will install the Rust language tools for you. Rust is distributed on three different release channels: stable, beta, and nightly. rustup is configured to use the stable channel by default. Switch between stable and nightly To switch between the versions use: rustup default nightly rustup default stable Or you can call only a single command with nightly: rustup run nightly rustc --version Update Rust is released every six weeks, to be up to date you have to call: rustup update This will download and install the newest updates of rust and its core components. Components Components can be removed or added (also using the nightly toolchain): rustup component list rustup component remove clippy rustup component remove --toolchain nightly clippy rustup component add clippy rustup component add --toolchain nightly clippy Cross Compiling To use cross compiling the standard libraries for all target platforms. The following examples show how to compile on Ubuntu for Windows: rustup target add x86_64-pc-windows-gnu To list possibilities use rustup target add list . Now you need the corresponding linker: sudo apt install mingw-w64 And configure it in ~/.cargo/config and write: [target.x86_64-pc-windows-gnu] linker = \"x86_64-w64-mingw32-gcc\" ar = \"x86_64-w64-mingw32-gcc-ar\" Now to compile for another target: cargo build --release --target=x86_64-pc-windows-gnu --verbose Cargo Cargo is the package manager and build tool. You can do something like: cargo new <name> to create a new binary project cargo new <name> --lib to create a new library project cargo check to check if code can be compiled cargo build to compile the current project cargo test run the contained tests cargo run to build and start the current project cargo doc --open to show documentation cargo build --release to build it for release cargo install <pack> to install a package cargo install-update -a to update all installed modules Installing You can install binary crates by using: cargo +nightly install --force racer cargo install --force rustsym cargo install --force cargo-outdated The --force parameter allows to reinstall (update) if it is already installed. Dependencies Within Cargo.toml you specify which dependencies you need: [dependencies] console = \"^0.6.1\" actix-web = \"~0.7\" clap = \"^2.31.2\" You specify the versions like: direct version number ^ prefix - the new version number does not modify the left-most non-zero digit ~ prefix - the specified number has to met but patch-level changes are allowed * allows wildcard at the end < > = - to manually specify To show which dependencies are too old: cargo outdated And to update according the defined versions: cargo update Profiles Cargo has two predefined and customizable profiles: the dev profile Cargo uses when you run cargo build and the release profile Cargo uses when you run cargo build --release . To change them define a section [profile.dev] or [profile.release] in Cargo.toml . Test cargo test compiles your code in test mode and runs the resulting test binary. If the binary needs some parameters they have to be given after the test parameters separated by -- . By default the tests run in parallel so they should not depend on each other or on any shared state, including a shared environment, such as the current working directory or environment variables. Run it synchronously call it with cargo test -- --test-threads=1 . If a test passes no output other than the success message is made. If you want to see any output do this using cargo test -- --test-threads=1 --nocapture . Synchronous processing is needed else the different test outputs will be mixed up. If you give specific name to the test runner only these tests containing this substring in its name will be run: cargo test my_test To run also the tests marked as ignore you can run cargo test -- --ignored . Documentation Using cargo doc --open will create the HTML documentation and open it in the browser. But you may also generate the full documentation only using cargo doc --release which will generate the documentation under target/doc . You may serve this with any webserver. Release To get a better optimized build which is smaller in size add the following to Cargo.toml : [profile.release] lto = true # default is false panic = 'abort' # default is 'unwind' This will enable the link time optimization and only includes the parts of the libraries we really used. Publishing Packages can be published as source code crate to crates.io . To get an API token you have to log in to crates.io using a GitHub account and retrieve your API key from your Account Settings. cargo login ThiS0sHoUld0bE0YouR0SeCReT0TOkEN This stores the API token locally in ~/.cargo/credentials . Before publishing you have to set at least name , description and license in the [package] section of Cargo.toml . [package] name = \"guessing_game\" version = \"0.1.0\" authors = [\"Your Name <you@example.com>\"] description = \"A fun game where you guess what number the computer has chosen.\" license = \"MIT OR Apache-2.0\" One major goal of crates.io is to act as a permanent archive of code so that builds of all projects that depend on crates from crates.io will continue to work. This means the version can never be overwritten, and the code cannot be deleted. To finally publish run: cargo publish Although you can\u2019t remove previous versions of a crate, you can prevent any future projects from adding them as a new dependency. This is useful when a crate version is broken for one reason or another. In such situations, Cargo supports yanking a crate version using cargo yank --vers 1.0.1 Extenstion Watch Watches over your project's source for changes, and runs Cargo commands when they occur. cargo install cargo-watch # install cargo watch -x test # call test on each change cargo watch -x 'run -- --some-arg' # run with some arguments Update A cargo subcommand for checking and applying updates to installed executables. cargo install cargo-update # install cargo install-update -a # update all installed Outdated A cargo subcommand for displaying when Rust dependencies are out of date. cargo install cargo-outdated # install cargo-outdated # list outdated Crates A crate is a compilation unit in Rust, mostly a binary. But it may also be a library. Rust packages can be found in crates.io and all packages which are released there have their API documentation under docs.rs . Some popular and useful packages are: clap - cli interface and argument parser iron - HTTP server Clippy A linter Run it using: cargo clippy If you use linting through clippy you may ignore a warning at one point. To do this use the following syntax which is only read on the cargo clippy call which is also run using just link in the alinex projects: #[cfg_attr(feature = \"cargo-clippy\", allow(needless_pass_by_value))] fn is_u32(v: String) -> Result<(), String> { if v.parse::<u32>().is_ok() { return Ok(()); } Err(format!(\"{} isn't a positive integer number\", &*v)) } Rustfmt A tool for formatting Rust code according to style guidelines. Mostly your editor will use it to make formatting, but you can run Rustfmt by just typing rustfmt <filename> . This runs rustfmt on the given file and related modules. So to run on a whole module or crate, you just need to run on the root file (usually mod.rs or lib.rs ). Racer An auto completion utility RLS A language server implementation for Rust. It provides things like code completion, goto definitions, rich refactoring, and some other nice features. It enables a much richer development experience and extends on the capabilities of racer . Optimize Binary Strips debug symbols if you don't need backtrace: strip target/release/hello Valgrind Valgrind is a tool that can automatically detect many memory management and threading bugs, and profile your programs in detail. Install using apt-get -y install valgrind . $ valgrind --leak-check=yes ../target/debug/fibonacci ==28440== Memcheck, a memory error detector ==28440== Copyright (C) 2002-2015, and GNU GPL'd, by Julian Seward et al. ==28440== Using Valgrind-3.11.0 and LibVEX; rerun with -h for copyright info ==28440== Command: ../target/debug/fibonacci ==28440== Which fibonacci position to calculate? 40 The 40th fibonacci number is 102334155 ==28440== ==28440== HEAP SUMMARY: ==28440== in use at exit: 0 bytes in 0 blocks ==28440== total heap usage: 0 allocs, 0 frees, 0 bytes allocated ==28440== ==28440== All heap blocks were freed -- no leaks are possible ==28440== ==28440== For counts of detected and suppressed errors, rerun with: -v ==28440== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0) $ valgrind --tool=callgrind ../target/debug/fibonacci ==28492== Callgrind, a call-graph generating cache profiler ==28492== Copyright (C) 2002-2015, and GNU GPL'd, by Josef Weidendorfer et al. ==28492== Using Valgrind-3.11.0 and LibVEX; rerun with -h for copyright info ==28492== Command: ../target/debug/fibonacci ==28492== ==28492== For interactive control, run 'callgrind_control -h'. Which fibonacci position to calculate? 40 The 40th fibonacci number is 102334155 ==28492== ==28492== Events : Ir ==28492== Collected : 563309 ==28492== ==28492== I refs: 563,309 {!docs/abbreviations.txt!}","title":"Tooling"},{"location":"rust/tools/#tooling","text":"","title":"Tooling"},{"location":"rust/tools/#rustup","text":"This is the version manager of rust which will install the Rust language tools for you. Rust is distributed on three different release channels: stable, beta, and nightly. rustup is configured to use the stable channel by default.","title":"Rustup"},{"location":"rust/tools/#switch-between-stable-and-nightly","text":"To switch between the versions use: rustup default nightly rustup default stable Or you can call only a single command with nightly: rustup run nightly rustc --version","title":"Switch between stable and nightly"},{"location":"rust/tools/#update","text":"Rust is released every six weeks, to be up to date you have to call: rustup update This will download and install the newest updates of rust and its core components.","title":"Update"},{"location":"rust/tools/#components","text":"Components can be removed or added (also using the nightly toolchain): rustup component list rustup component remove clippy rustup component remove --toolchain nightly clippy rustup component add clippy rustup component add --toolchain nightly clippy","title":"Components"},{"location":"rust/tools/#cross-compiling","text":"To use cross compiling the standard libraries for all target platforms. The following examples show how to compile on Ubuntu for Windows: rustup target add x86_64-pc-windows-gnu To list possibilities use rustup target add list . Now you need the corresponding linker: sudo apt install mingw-w64 And configure it in ~/.cargo/config and write: [target.x86_64-pc-windows-gnu] linker = \"x86_64-w64-mingw32-gcc\" ar = \"x86_64-w64-mingw32-gcc-ar\" Now to compile for another target: cargo build --release --target=x86_64-pc-windows-gnu --verbose","title":"Cross Compiling"},{"location":"rust/tools/#cargo","text":"Cargo is the package manager and build tool. You can do something like: cargo new <name> to create a new binary project cargo new <name> --lib to create a new library project cargo check to check if code can be compiled cargo build to compile the current project cargo test run the contained tests cargo run to build and start the current project cargo doc --open to show documentation cargo build --release to build it for release cargo install <pack> to install a package cargo install-update -a to update all installed modules","title":"Cargo"},{"location":"rust/tools/#installing","text":"You can install binary crates by using: cargo +nightly install --force racer cargo install --force rustsym cargo install --force cargo-outdated The --force parameter allows to reinstall (update) if it is already installed.","title":"Installing"},{"location":"rust/tools/#dependencies","text":"Within Cargo.toml you specify which dependencies you need: [dependencies] console = \"^0.6.1\" actix-web = \"~0.7\" clap = \"^2.31.2\" You specify the versions like: direct version number ^ prefix - the new version number does not modify the left-most non-zero digit ~ prefix - the specified number has to met but patch-level changes are allowed * allows wildcard at the end < > = - to manually specify To show which dependencies are too old: cargo outdated And to update according the defined versions: cargo update","title":"Dependencies"},{"location":"rust/tools/#profiles","text":"Cargo has two predefined and customizable profiles: the dev profile Cargo uses when you run cargo build and the release profile Cargo uses when you run cargo build --release . To change them define a section [profile.dev] or [profile.release] in Cargo.toml .","title":"Profiles"},{"location":"rust/tools/#test","text":"cargo test compiles your code in test mode and runs the resulting test binary. If the binary needs some parameters they have to be given after the test parameters separated by -- . By default the tests run in parallel so they should not depend on each other or on any shared state, including a shared environment, such as the current working directory or environment variables. Run it synchronously call it with cargo test -- --test-threads=1 . If a test passes no output other than the success message is made. If you want to see any output do this using cargo test -- --test-threads=1 --nocapture . Synchronous processing is needed else the different test outputs will be mixed up. If you give specific name to the test runner only these tests containing this substring in its name will be run: cargo test my_test To run also the tests marked as ignore you can run cargo test -- --ignored .","title":"Test"},{"location":"rust/tools/#documentation","text":"Using cargo doc --open will create the HTML documentation and open it in the browser. But you may also generate the full documentation only using cargo doc --release which will generate the documentation under target/doc . You may serve this with any webserver.","title":"Documentation"},{"location":"rust/tools/#release","text":"To get a better optimized build which is smaller in size add the following to Cargo.toml : [profile.release] lto = true # default is false panic = 'abort' # default is 'unwind' This will enable the link time optimization and only includes the parts of the libraries we really used.","title":"Release"},{"location":"rust/tools/#publishing","text":"Packages can be published as source code crate to crates.io . To get an API token you have to log in to crates.io using a GitHub account and retrieve your API key from your Account Settings. cargo login ThiS0sHoUld0bE0YouR0SeCReT0TOkEN This stores the API token locally in ~/.cargo/credentials . Before publishing you have to set at least name , description and license in the [package] section of Cargo.toml . [package] name = \"guessing_game\" version = \"0.1.0\" authors = [\"Your Name <you@example.com>\"] description = \"A fun game where you guess what number the computer has chosen.\" license = \"MIT OR Apache-2.0\" One major goal of crates.io is to act as a permanent archive of code so that builds of all projects that depend on crates from crates.io will continue to work. This means the version can never be overwritten, and the code cannot be deleted. To finally publish run: cargo publish Although you can\u2019t remove previous versions of a crate, you can prevent any future projects from adding them as a new dependency. This is useful when a crate version is broken for one reason or another. In such situations, Cargo supports yanking a crate version using cargo yank --vers 1.0.1","title":"Publishing"},{"location":"rust/tools/#extenstion","text":"","title":"Extenstion"},{"location":"rust/tools/#watch","text":"Watches over your project's source for changes, and runs Cargo commands when they occur. cargo install cargo-watch # install cargo watch -x test # call test on each change cargo watch -x 'run -- --some-arg' # run with some arguments","title":"Watch"},{"location":"rust/tools/#update_1","text":"A cargo subcommand for checking and applying updates to installed executables. cargo install cargo-update # install cargo install-update -a # update all installed","title":"Update"},{"location":"rust/tools/#outdated","text":"A cargo subcommand for displaying when Rust dependencies are out of date. cargo install cargo-outdated # install cargo-outdated # list outdated","title":"Outdated"},{"location":"rust/tools/#crates","text":"A crate is a compilation unit in Rust, mostly a binary. But it may also be a library. Rust packages can be found in crates.io and all packages which are released there have their API documentation under docs.rs . Some popular and useful packages are: clap - cli interface and argument parser iron - HTTP server","title":"Crates"},{"location":"rust/tools/#clippy","text":"A linter Run it using: cargo clippy If you use linting through clippy you may ignore a warning at one point. To do this use the following syntax which is only read on the cargo clippy call which is also run using just link in the alinex projects: #[cfg_attr(feature = \"cargo-clippy\", allow(needless_pass_by_value))] fn is_u32(v: String) -> Result<(), String> { if v.parse::<u32>().is_ok() { return Ok(()); } Err(format!(\"{} isn't a positive integer number\", &*v)) }","title":"Clippy"},{"location":"rust/tools/#rustfmt","text":"A tool for formatting Rust code according to style guidelines. Mostly your editor will use it to make formatting, but you can run Rustfmt by just typing rustfmt <filename> . This runs rustfmt on the given file and related modules. So to run on a whole module or crate, you just need to run on the root file (usually mod.rs or lib.rs ).","title":"Rustfmt"},{"location":"rust/tools/#racer","text":"An auto completion utility","title":"Racer"},{"location":"rust/tools/#rls","text":"A language server implementation for Rust. It provides things like code completion, goto definitions, rich refactoring, and some other nice features. It enables a much richer development experience and extends on the capabilities of racer .","title":"RLS"},{"location":"rust/tools/#optimize-binary","text":"Strips debug symbols if you don't need backtrace: strip target/release/hello","title":"Optimize Binary"},{"location":"rust/tools/#valgrind","text":"Valgrind is a tool that can automatically detect many memory management and threading bugs, and profile your programs in detail. Install using apt-get -y install valgrind . $ valgrind --leak-check=yes ../target/debug/fibonacci ==28440== Memcheck, a memory error detector ==28440== Copyright (C) 2002-2015, and GNU GPL'd, by Julian Seward et al. ==28440== Using Valgrind-3.11.0 and LibVEX; rerun with -h for copyright info ==28440== Command: ../target/debug/fibonacci ==28440== Which fibonacci position to calculate? 40 The 40th fibonacci number is 102334155 ==28440== ==28440== HEAP SUMMARY: ==28440== in use at exit: 0 bytes in 0 blocks ==28440== total heap usage: 0 allocs, 0 frees, 0 bytes allocated ==28440== ==28440== All heap blocks were freed -- no leaks are possible ==28440== ==28440== For counts of detected and suppressed errors, rerun with: -v ==28440== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0) $ valgrind --tool=callgrind ../target/debug/fibonacci ==28492== Callgrind, a call-graph generating cache profiler ==28492== Copyright (C) 2002-2015, and GNU GPL'd, by Josef Weidendorfer et al. ==28492== Using Valgrind-3.11.0 and LibVEX; rerun with -h for copyright info ==28492== Command: ../target/debug/fibonacci ==28492== ==28492== For interactive control, run 'callgrind_control -h'. Which fibonacci position to calculate? 40 The 40th fibonacci number is 102334155 ==28492== ==28492== Events : Ir ==28492== Collected : 563309 ==28492== ==28492== I refs: 563,309 {!docs/abbreviations.txt!}","title":"Valgrind"},{"location":"rust/crates/","text":"Rust Development Outdated cargo-outdated Update cargo-update Watch cargo-watch Utilities Logger flexi_logger with log Errors failure Regular Expressions regex Storages OR-Mapper Diesel Postgres rust-postgres Formats JSON Serialization serde_json Configuration config Date and Time chrono Command line Colors console Argument parser clap Progress bar indicatif Interactive user input dialoguer Network HTTP Requests reqwest FTP ftp HTTP Server hyper , iron , rocket , actix-web REST Server rustless Hardware Number of CPUs num_cpus {!docs/abbreviations.txt!}","title":"Rust"},{"location":"rust/crates/#rust","text":"","title":"Rust"},{"location":"rust/crates/#development","text":"Outdated cargo-outdated Update cargo-update Watch cargo-watch","title":"Development"},{"location":"rust/crates/#utilities","text":"Logger flexi_logger with log Errors failure Regular Expressions regex","title":"Utilities"},{"location":"rust/crates/#storages","text":"OR-Mapper Diesel Postgres rust-postgres","title":"Storages"},{"location":"rust/crates/#formats","text":"JSON Serialization serde_json Configuration config Date and Time chrono","title":"Formats"},{"location":"rust/crates/#command-line","text":"Colors console Argument parser clap Progress bar indicatif Interactive user input dialoguer","title":"Command line"},{"location":"rust/crates/#network","text":"HTTP Requests reqwest FTP ftp HTTP Server hyper , iron , rocket , actix-web REST Server rustless","title":"Network"},{"location":"rust/crates/#hardware","text":"Number of CPUs num_cpus {!docs/abbreviations.txt!}","title":"Hardware"},{"location":"rust/crates/actix-web/","text":"Actix Web - Web Server Framework This is a web framework layered on actix as very fast development framework for web based services and brings the actor model to the web. An application developed with actix-web will expose an HTTP server contained within a native executable. You can either put this behind another HTTP server like Nginx or serve it up as-is. Even in the complete absence of another HTTP server actix-web is powerful enough to provide HTTP 1 and HTTP 2 support as well as SSL/TLS. Also WebSockets, steaming responses, graceful shutdown, cookie support, static file serving, and good testing infrastructure are readily available out of the box. The key facts are: request handler functions which gets a request and returns a response application instance which registers the request handler with a route main function which starts the server Most part of this abstract comes from the Actix Web documentation. Application The application instance is the central element. It is used for registering routes for resources and middleware. It also stores application state shared across all handlers within same application. The server can be run with multiple applications. fn index(req: &HttpRequest) -> impl Responder { \"Hello world!\" } let app = App::new() .prefix(\"/app\") .resource(\"/index.html\", |r| r.method(Method::GET).f(index)) .finish() The prefix is used for the whole application if set. So in the above the index function is invoked using /app/index.html on the server. {!docs/abbreviations.txt!}","title":"actix_web"},{"location":"rust/crates/actix-web/#actix-web-web-server-framework","text":"This is a web framework layered on actix as very fast development framework for web based services and brings the actor model to the web. An application developed with actix-web will expose an HTTP server contained within a native executable. You can either put this behind another HTTP server like Nginx or serve it up as-is. Even in the complete absence of another HTTP server actix-web is powerful enough to provide HTTP 1 and HTTP 2 support as well as SSL/TLS. Also WebSockets, steaming responses, graceful shutdown, cookie support, static file serving, and good testing infrastructure are readily available out of the box. The key facts are: request handler functions which gets a request and returns a response application instance which registers the request handler with a route main function which starts the server Most part of this abstract comes from the Actix Web documentation.","title":"Actix Web - Web Server Framework"},{"location":"rust/crates/actix-web/#application","text":"The application instance is the central element. It is used for registering routes for resources and middleware. It also stores application state shared across all handlers within same application. The server can be run with multiple applications. fn index(req: &HttpRequest) -> impl Responder { \"Hello world!\" } let app = App::new() .prefix(\"/app\") .resource(\"/index.html\", |r| r.method(Method::GET).f(index)) .finish() The prefix is used for the whole application if set. So in the above the index function is invoked using /app/index.html on the server. {!docs/abbreviations.txt!}","title":"Application"},{"location":"rust/crates/actix/","text":"Actix - Microframework using Actor Pattern Actix is a concept of an actor system for Rust to be also used for web systems with actix-web . It's a microframework which is easy to use. Actix is similar to what you might see in a language like Erlang, except that it adds another degree of robustness and speed by making heavy use of Rust's sophisticated type and concurrency systems. For example, it's not possible for an actor to receive a message that it can't handle at runtime because it would have been disallowed at compile-time. The following description is mostly an abstract of the crates Actix QuickStart documentation. Actor Model In the actor model everything is an actor. This is similar to the everything is an object philosophy used by some object-oriented programming languages. An actor is a computational entity that, in response to a message it receives, can concurrently: make local decisions change internal state create new actors send messages to other actors There is no assumed sequence to the above actions and they could be carried out in parallel. A fundamental advance of the Actor model is the decoupling between sender and receiver enabling asynchronous communication and control structures as patterns of passing messages. Recipients are identified by address, sometimes called \"mailing address\". Thus an actor can only communicate with actors whose addresses it has. It can obtain those from a message it receives, or if the address is for an actor it has itself created. It is a way to design concurrent systems. Rust purposefully does not enforce a design on concurrency as much as other languages might, but instead provides the necessary data structures and zero-cost abstractions, upon which people can build higher-level abstractions. The actor model is one of those higher-level abstractions, and as we have seen with other languages, it is a good model for designing some concurrent systems. Actor Any rust type can be an actor, it only needs to implement the Actor trait. To be able to handle a specific message the actor has to provide a Handler implementation for this message. Lifecycle The actor can be in one of multiple states: Started This is the initial state. The started() hook method is being called to start more actors or initialize something. Running This state comes directly after the started() hook is done and will stay. Stopping This state will be set if: Context::stop is called by the actor itself all addresses to the actor get dropped. i.e. no other actor references it no event objects are registered in the context Incomming messages are no longer processed and the stopping() hook is being called. The actor may go back to running state by creating a new address or adding an event object, and by returning Running::Continue . If the actor does not restore back to the running state, all unprocessed messages are dropped. By default this method returns Running::Stop which confirms the stop operation. Stopped After stopping this state is considered final and at this point the actor is dropped. Address Actors communicate exclusively by exchanging messages. To reference them their address has to be used. !!! example \"Async actor in same thread\" ```rust struct MyActor; impl Actor for MyActor { type Context = Context<Self>; fn started(&mut self, ctx: &mut Context<Self>) { let addr = ctx.address(); // get self address } } let addr = MyActor.start(); ``` Message Messages are used to communicate with and between actors. For actix each message has to implement the Message trait. Result defines the return type. struct PlusOne; impl Message for PlusOne { type Result = u32; } All messages go to the actor's mailbox first, then the actor's execution context calls specific message handlers. To be able to handle a specific message the actor has to provide a Handler<M> implementation for this message. The message can be handled in an asynchronous fashion. The actor can spawn other actors or add futures or streams to the execution context. To send a message to an actor, the Addr object needs to be used. It provides several ways to send a message: do_send(M) - ignores the actor's mailbox capacity and puts the message to a mailbox unconditionally, it does not return the result of message handling and fails silently if the actor is gone try_send(M) - try to send the message immediately, if the mailbox is full or closed (actor is dead), this method returns a SendError send(M) - return a future object that resolves to a result of a message handling process Recipient The recipient address can be used in case the message needs to be sent to a different type of actor. A recipient object can be created from an address with Addr::recipient() . For example recipient can be used for a subscription system there actors can subscribe to get signal messages on specific events. Actix Web This is a web framework layered on actix as very fast development framework for web based services and brings the actor model to the web. See more at actix-web . {!docs/abbreviations.txt!}","title":"actix"},{"location":"rust/crates/actix/#actix-microframework-using-actor-pattern","text":"Actix is a concept of an actor system for Rust to be also used for web systems with actix-web . It's a microframework which is easy to use. Actix is similar to what you might see in a language like Erlang, except that it adds another degree of robustness and speed by making heavy use of Rust's sophisticated type and concurrency systems. For example, it's not possible for an actor to receive a message that it can't handle at runtime because it would have been disallowed at compile-time. The following description is mostly an abstract of the crates Actix QuickStart documentation.","title":"Actix - Microframework using Actor Pattern"},{"location":"rust/crates/actix/#actor-model","text":"In the actor model everything is an actor. This is similar to the everything is an object philosophy used by some object-oriented programming languages. An actor is a computational entity that, in response to a message it receives, can concurrently: make local decisions change internal state create new actors send messages to other actors There is no assumed sequence to the above actions and they could be carried out in parallel. A fundamental advance of the Actor model is the decoupling between sender and receiver enabling asynchronous communication and control structures as patterns of passing messages. Recipients are identified by address, sometimes called \"mailing address\". Thus an actor can only communicate with actors whose addresses it has. It can obtain those from a message it receives, or if the address is for an actor it has itself created. It is a way to design concurrent systems. Rust purposefully does not enforce a design on concurrency as much as other languages might, but instead provides the necessary data structures and zero-cost abstractions, upon which people can build higher-level abstractions. The actor model is one of those higher-level abstractions, and as we have seen with other languages, it is a good model for designing some concurrent systems.","title":"Actor Model"},{"location":"rust/crates/actix/#actor","text":"Any rust type can be an actor, it only needs to implement the Actor trait. To be able to handle a specific message the actor has to provide a Handler implementation for this message.","title":"Actor"},{"location":"rust/crates/actix/#lifecycle","text":"The actor can be in one of multiple states: Started This is the initial state. The started() hook method is being called to start more actors or initialize something. Running This state comes directly after the started() hook is done and will stay. Stopping This state will be set if: Context::stop is called by the actor itself all addresses to the actor get dropped. i.e. no other actor references it no event objects are registered in the context Incomming messages are no longer processed and the stopping() hook is being called. The actor may go back to running state by creating a new address or adding an event object, and by returning Running::Continue . If the actor does not restore back to the running state, all unprocessed messages are dropped. By default this method returns Running::Stop which confirms the stop operation. Stopped After stopping this state is considered final and at this point the actor is dropped.","title":"Lifecycle"},{"location":"rust/crates/actix/#address","text":"Actors communicate exclusively by exchanging messages. To reference them their address has to be used. !!! example \"Async actor in same thread\" ```rust struct MyActor; impl Actor for MyActor { type Context = Context<Self>; fn started(&mut self, ctx: &mut Context<Self>) { let addr = ctx.address(); // get self address } } let addr = MyActor.start(); ```","title":"Address"},{"location":"rust/crates/actix/#message","text":"Messages are used to communicate with and between actors. For actix each message has to implement the Message trait. Result defines the return type. struct PlusOne; impl Message for PlusOne { type Result = u32; } All messages go to the actor's mailbox first, then the actor's execution context calls specific message handlers. To be able to handle a specific message the actor has to provide a Handler<M> implementation for this message. The message can be handled in an asynchronous fashion. The actor can spawn other actors or add futures or streams to the execution context. To send a message to an actor, the Addr object needs to be used. It provides several ways to send a message: do_send(M) - ignores the actor's mailbox capacity and puts the message to a mailbox unconditionally, it does not return the result of message handling and fails silently if the actor is gone try_send(M) - try to send the message immediately, if the mailbox is full or closed (actor is dead), this method returns a SendError send(M) - return a future object that resolves to a result of a message handling process","title":"Message"},{"location":"rust/crates/actix/#recipient","text":"The recipient address can be used in case the message needs to be sent to a different type of actor. A recipient object can be created from an address with Addr::recipient() . For example recipient can be used for a subscription system there actors can subscribe to get signal messages on specific events.","title":"Recipient"},{"location":"rust/crates/actix/#actix-web","text":"This is a web framework layered on actix as very fast development framework for web based services and brings the actor model to the web. See more at actix-web . {!docs/abbreviations.txt!}","title":"Actix Web"},{"location":"rust/crates/clap/","text":"clap - Command Line Argument Parser This is an efficient and configurable library for parsing command line arguments. It supports advanced features like argument relationships, subcommands and mmuch more. It will not only parse but also validate values and display the help page. The following is only a short introduction with some examples. Read more about in the API Docs . Configuration You have different possibilities to setup your application. At first the builder style is the most enhanced system. But you can also load the arguments definition from a YAML file or use the macro to do it from a compressed text form inline. To be really clear and easy readable I use the builder pattern like: #[macro_use] extern crate clap; fn main() { let args = clap::App::new(crate_name!()) .version(crate_version!()) .about(crate_description!()) .author(crate_authors!()) .get_matches(); } The macros are enabled for clap to use meta data from the Cargo.toml file like: crate_name!() crate_version!() crate_description!() crate_authors!() Setup Application Basic setting methods: name - Program's name to be displayed in help or version information. version - Version number to be displayed in version or help information. about - String describing what the program does for the help information. author - Author(s) that will be displayed to the user when they request the help information. Advanced setting methods: usage - Overwrite auto generated usage line. help - Replaces the entire help message. Arguments Use arg method to add an argument like described below to be added to the list of valid possibilities. Argument definition: clap::Arg::with_name(\"config\") .short(\"c\") .long(\"config\") .takes_value(true) .value_name(\"FILE\") .help(\"Provides a config file to myprog\"); Methods for arguments: short - Short version of the argument without the preceding - as option. long - Long version of the argument without the preceding -- as option. alias - Alternative option name, which is \"hidden\" from help information. visible_alias - Alias option name, which is visible in help information. help - Short one line help text. required - Set to true to make this option a required one. takes_value - Set to true to specify that this option can have a value. multiple - If set to true the option may be used multiple times. global - This argument is possible for all subcommands. empty_values - Set to false to disallow empty values. hidden - Set to true to hide this argument from help page. possible_values - Only the given list of values is possible. case_insensitive - This option can be called case insensitive. group - Combine parameters within named groups. number_of_values - Specify how many values to be given to this argument. max_values - Maximum values to be given. min_values - Minimum values to be given. validator - Add a custom validator method. default_value - Value used for this argument if nothing is given. default_value_if - If given option name is equal to value the given value is used as default. env - Specifies that if the value is not passed in as an argument, that it should be retrieved from the environment, if available. value_name - Name used for value in help information. index - Set a index position for ordered arguments Relational methods: required_unless - Given an option name which makes this option required if not set. required_unless_all - All given option names (as list) have to be set or this option is required. required_unless_one - At least one option of the list has to be set or this option is required. conflicts_with - If this argument is set the given option name is not allowed. conflicts_with_all - Set multiple conflicting options. overrides_with - If the defined option name is present this argument will be removed. overrides_with_all - Set multiple overriding options. requires - The named option is required if this argument is given. requires_if - The named option is required if this argument equals the given value. required_if - This argument is required if the named option is equal the given value. Sub Command Using subcomman method you can add such with it's own arguments, subcommands, version, usage, etc. They also function just like the main app. clap::SubCommand::with_name(\"config\") .about(\"Used for configuration\") .arg(Arg::with_name(\"config_file\") .help(\"The configuration file to use\") .index(1))) The methods here are the same as for the main app. Working with Values Using the method get_matches you get information about the arguments that where supplied to the program at runtime by the user. let args = clap::App::new(crate_name!()) .version(crate_version!()) .about(crate_description!()) .author(crate_authors!()) .arg(clap::Arg::with_name(\"config\") .short(\"c\") .long(\"config\") .takes_value(true) .value_name(\"FILE\") .help(\"Provides a config file to myprog\") ) .get_matches(); The matches structure, here stored in args can now be asked for. All methods work with the name of the options given in the arguments definition. value_of - Gets the value of a specific option or positional argument. If the option wasn't present at runtime it returns None . values_of - Does the same but for options which may occur multiple times and returns an Iterator . is_present - Returns true if an argument was present at runtime, otherwise false. occurences_of - Returns the number of times that argument is given. index_of - Gets the starting index of the argument in respect to all other arguments. indices_of - Does the same for multiple arguments. subcommand_name - Returns the name of the subcommand that was used at runtime, or None if one wasn't. subcommand_matches - Get the arguments specified for the subcommand. Examples Check for present argument: if args.is_present(\"list\") { println!(\"Printing testing lists...\"); } else { println!(\"Not printing testing lists...\"); } Check how many times an argument is given: match args.occurrences_of(\"v\") { 0 => println!(\"Verbose mode is off\"), 1 => println!(\"Print only some information\"), 2 => println!(\"Print a lot of information\"), 3 | _ => println!(\"Going crazy\"), } Check for the existence of subcommands, and if found use their matches just as you would the top level app: if let Some(matches) = args.subcommand_matches(\"test\") { // \"$ myapp test\" was run if matches.is_present(\"list\") { // \"$ myapp test -l\" was run println!(\"Printing testing lists...\"); } else { println!(\"Not printing testing lists...\"); } } Shell Completion Completion scripts can be created by clap on call. Therefore a subcommand to do this is needed: .subcommand(SubCommand::with_name(\"completions\") .about(\"Generates completion scripts for your shell\") .arg(Arg::with_name(\"SHELL\") .required(true) .possible_values(&[\"bash\", \"fish\", \"zsh\"]) .help(\"The shell to generate the script for\"))) And the following code will generate it: match args.subcommand() { (\"completions\", Some(sub_args)) => { let shell = sub_args.value_of(\"SHELL\").unwrap(); cli().gen_completions_to( crate_name!(), shell.parse::<clap::Shell>().unwrap(), &mut io::stdout(), ); } (_, _) => unimplemented!(), // for brevity }","title":"clap"},{"location":"rust/crates/clap/#clap-command-line-argument-parser","text":"This is an efficient and configurable library for parsing command line arguments. It supports advanced features like argument relationships, subcommands and mmuch more. It will not only parse but also validate values and display the help page. The following is only a short introduction with some examples. Read more about in the API Docs .","title":"clap - Command Line Argument Parser"},{"location":"rust/crates/clap/#configuration","text":"You have different possibilities to setup your application. At first the builder style is the most enhanced system. But you can also load the arguments definition from a YAML file or use the macro to do it from a compressed text form inline. To be really clear and easy readable I use the builder pattern like: #[macro_use] extern crate clap; fn main() { let args = clap::App::new(crate_name!()) .version(crate_version!()) .about(crate_description!()) .author(crate_authors!()) .get_matches(); } The macros are enabled for clap to use meta data from the Cargo.toml file like: crate_name!() crate_version!() crate_description!() crate_authors!()","title":"Configuration"},{"location":"rust/crates/clap/#setup-application","text":"Basic setting methods: name - Program's name to be displayed in help or version information. version - Version number to be displayed in version or help information. about - String describing what the program does for the help information. author - Author(s) that will be displayed to the user when they request the help information. Advanced setting methods: usage - Overwrite auto generated usage line. help - Replaces the entire help message.","title":"Setup Application"},{"location":"rust/crates/clap/#arguments","text":"Use arg method to add an argument like described below to be added to the list of valid possibilities. Argument definition: clap::Arg::with_name(\"config\") .short(\"c\") .long(\"config\") .takes_value(true) .value_name(\"FILE\") .help(\"Provides a config file to myprog\"); Methods for arguments: short - Short version of the argument without the preceding - as option. long - Long version of the argument without the preceding -- as option. alias - Alternative option name, which is \"hidden\" from help information. visible_alias - Alias option name, which is visible in help information. help - Short one line help text. required - Set to true to make this option a required one. takes_value - Set to true to specify that this option can have a value. multiple - If set to true the option may be used multiple times. global - This argument is possible for all subcommands. empty_values - Set to false to disallow empty values. hidden - Set to true to hide this argument from help page. possible_values - Only the given list of values is possible. case_insensitive - This option can be called case insensitive. group - Combine parameters within named groups. number_of_values - Specify how many values to be given to this argument. max_values - Maximum values to be given. min_values - Minimum values to be given. validator - Add a custom validator method. default_value - Value used for this argument if nothing is given. default_value_if - If given option name is equal to value the given value is used as default. env - Specifies that if the value is not passed in as an argument, that it should be retrieved from the environment, if available. value_name - Name used for value in help information. index - Set a index position for ordered arguments Relational methods: required_unless - Given an option name which makes this option required if not set. required_unless_all - All given option names (as list) have to be set or this option is required. required_unless_one - At least one option of the list has to be set or this option is required. conflicts_with - If this argument is set the given option name is not allowed. conflicts_with_all - Set multiple conflicting options. overrides_with - If the defined option name is present this argument will be removed. overrides_with_all - Set multiple overriding options. requires - The named option is required if this argument is given. requires_if - The named option is required if this argument equals the given value. required_if - This argument is required if the named option is equal the given value.","title":"Arguments"},{"location":"rust/crates/clap/#sub-command","text":"Using subcomman method you can add such with it's own arguments, subcommands, version, usage, etc. They also function just like the main app. clap::SubCommand::with_name(\"config\") .about(\"Used for configuration\") .arg(Arg::with_name(\"config_file\") .help(\"The configuration file to use\") .index(1))) The methods here are the same as for the main app.","title":"Sub Command"},{"location":"rust/crates/clap/#working-with-values","text":"Using the method get_matches you get information about the arguments that where supplied to the program at runtime by the user. let args = clap::App::new(crate_name!()) .version(crate_version!()) .about(crate_description!()) .author(crate_authors!()) .arg(clap::Arg::with_name(\"config\") .short(\"c\") .long(\"config\") .takes_value(true) .value_name(\"FILE\") .help(\"Provides a config file to myprog\") ) .get_matches(); The matches structure, here stored in args can now be asked for. All methods work with the name of the options given in the arguments definition. value_of - Gets the value of a specific option or positional argument. If the option wasn't present at runtime it returns None . values_of - Does the same but for options which may occur multiple times and returns an Iterator . is_present - Returns true if an argument was present at runtime, otherwise false. occurences_of - Returns the number of times that argument is given. index_of - Gets the starting index of the argument in respect to all other arguments. indices_of - Does the same for multiple arguments. subcommand_name - Returns the name of the subcommand that was used at runtime, or None if one wasn't. subcommand_matches - Get the arguments specified for the subcommand.","title":"Working with Values"},{"location":"rust/crates/clap/#examples","text":"Check for present argument: if args.is_present(\"list\") { println!(\"Printing testing lists...\"); } else { println!(\"Not printing testing lists...\"); } Check how many times an argument is given: match args.occurrences_of(\"v\") { 0 => println!(\"Verbose mode is off\"), 1 => println!(\"Print only some information\"), 2 => println!(\"Print a lot of information\"), 3 | _ => println!(\"Going crazy\"), } Check for the existence of subcommands, and if found use their matches just as you would the top level app: if let Some(matches) = args.subcommand_matches(\"test\") { // \"$ myapp test\" was run if matches.is_present(\"list\") { // \"$ myapp test -l\" was run println!(\"Printing testing lists...\"); } else { println!(\"Not printing testing lists...\"); } }","title":"Examples"},{"location":"rust/crates/clap/#shell-completion","text":"Completion scripts can be created by clap on call. Therefore a subcommand to do this is needed: .subcommand(SubCommand::with_name(\"completions\") .about(\"Generates completion scripts for your shell\") .arg(Arg::with_name(\"SHELL\") .required(true) .possible_values(&[\"bash\", \"fish\", \"zsh\"]) .help(\"The shell to generate the script for\"))) And the following code will generate it: match args.subcommand() { (\"completions\", Some(sub_args)) => { let shell = sub_args.value_of(\"SHELL\").unwrap(); cli().gen_completions_to( crate_name!(), shell.parse::<clap::Shell>().unwrap(), &mut io::stdout(), ); } (_, _) => unimplemented!(), // for brevity }","title":"Shell Completion"},{"location":"rust/crates/diesel/","text":"Diesel Object Relation Mapper Diesel provides a separate CLI tool to help manage your project. Since it's a standalone binary, and doesn't affect your project's code directly, it should be installed on your system separately. sudo apt install libpq-dev cargo install diesel_cli --no-default-features --features postgres To setup the database for the Diesel CLI create a environment file called .env : DATABASE_URL=postgres://username:password@localhost/diesel_demo After that run diesel setup which will setup everything. It will create the database if not already there and creates a migrations directory for you. {!docs/abbreviations.txt!}","title":"Diesel Object Relation Mapper"},{"location":"rust/crates/diesel/#diesel-object-relation-mapper","text":"Diesel provides a separate CLI tool to help manage your project. Since it's a standalone binary, and doesn't affect your project's code directly, it should be installed on your system separately. sudo apt install libpq-dev cargo install diesel_cli --no-default-features --features postgres To setup the database for the Diesel CLI create a environment file called .env : DATABASE_URL=postgres://username:password@localhost/diesel_demo After that run diesel setup which will setup everything. It will create the database if not already there and creates a migrations directory for you. {!docs/abbreviations.txt!}","title":"Diesel Object Relation Mapper"},{"location":"rust/crates/failure/","text":"Error Management Best way to work with errors is to propagate them within the library and let the application code handle it. This is easy to code but in the core misses some information like you will get an 'Permission denied' but don't know on which file. The following examples will use the failure crate to make it easier to work with errors. At the moment this is not really final and development is stagnating but it looks like a new team will come up soon and this will become more popular. To use it you have to add failure = \"0.1.1\" to your Cargo.toml and include it in your main.rs and lib.rs : extern crate failure; Context As described before, often the libraries you are using will present error messages that don't provide very helpful information about what exactly has gone wrong. You need to know the circumstances under which it evolves. If at any processing a Result is returned you can add context to it before checking with the ? operator: use failure::{Error, ResultExt}; pub fn run() -> Result<(), Error> { let address = \"127.0.0.1:80\"; server::new(|| { App::new() .resource(\"/\", |r| r.f(greet)) .resource(\"/{name}\", |r| r.f(greet)) }).bind(address).context(format!(\"Could not bind {}\", address))? .start(); Ok(()) } As seen after the bind method you can inject additional context to be carried with this error value, providing semantic information about the nature of the error appropriate to the level of abstraction that the code you are writing operates at. The context method on Fail takes any displayable value (such as a string) to act as context for this error. Using the ResultExt trait, you can also get context as a convenient method on Result directly. In the example above you will see that the \"Permission denied\" message is not coming from a file but from binding to port 80 as not root user. Display This failures which may be a context information with underlying real error can be best displayed using a short formatter method: fn main() { if let Err(ref e) = run() { eprintln!(\"Error: {}\", pretty_error(e)); std::process::exit(1); } } fn pretty_error(err: &failure::Error) -> String { let mut pretty = err.to_string(); let mut prev = err.as_fail(); while let Some(next) = prev.cause() { pretty.push_str(\": \"); pretty.push_str(&next.to_string()); prev = next; } pretty } // Error: Could not bind 127.0.0.1:80: Permission denied (os error 13) In the main method the occurring error is processed, printed and the program will stop. The pretty_error function will generate a concatenated String using the Error with all its causes. In the example above this will be the context and the error itself. Backtrace To also support backtrace on errors using the environment setting RUST_BACKTRACE=1 : if let Some(bt) = e.as_fail().backtrace() { eprintln!(\"{}\", bt) } This will only generate and output the backtrace on demand. {!docs/abbreviations.txt!}","title":"Error Management"},{"location":"rust/crates/failure/#error-management","text":"Best way to work with errors is to propagate them within the library and let the application code handle it. This is easy to code but in the core misses some information like you will get an 'Permission denied' but don't know on which file. The following examples will use the failure crate to make it easier to work with errors. At the moment this is not really final and development is stagnating but it looks like a new team will come up soon and this will become more popular. To use it you have to add failure = \"0.1.1\" to your Cargo.toml and include it in your main.rs and lib.rs : extern crate failure;","title":"Error Management"},{"location":"rust/crates/failure/#context","text":"As described before, often the libraries you are using will present error messages that don't provide very helpful information about what exactly has gone wrong. You need to know the circumstances under which it evolves. If at any processing a Result is returned you can add context to it before checking with the ? operator: use failure::{Error, ResultExt}; pub fn run() -> Result<(), Error> { let address = \"127.0.0.1:80\"; server::new(|| { App::new() .resource(\"/\", |r| r.f(greet)) .resource(\"/{name}\", |r| r.f(greet)) }).bind(address).context(format!(\"Could not bind {}\", address))? .start(); Ok(()) } As seen after the bind method you can inject additional context to be carried with this error value, providing semantic information about the nature of the error appropriate to the level of abstraction that the code you are writing operates at. The context method on Fail takes any displayable value (such as a string) to act as context for this error. Using the ResultExt trait, you can also get context as a convenient method on Result directly. In the example above you will see that the \"Permission denied\" message is not coming from a file but from binding to port 80 as not root user.","title":"Context"},{"location":"rust/crates/failure/#display","text":"This failures which may be a context information with underlying real error can be best displayed using a short formatter method: fn main() { if let Err(ref e) = run() { eprintln!(\"Error: {}\", pretty_error(e)); std::process::exit(1); } } fn pretty_error(err: &failure::Error) -> String { let mut pretty = err.to_string(); let mut prev = err.as_fail(); while let Some(next) = prev.cause() { pretty.push_str(\": \"); pretty.push_str(&next.to_string()); prev = next; } pretty } // Error: Could not bind 127.0.0.1:80: Permission denied (os error 13) In the main method the occurring error is processed, printed and the program will stop. The pretty_error function will generate a concatenated String using the Error with all its causes. In the example above this will be the context and the error itself.","title":"Display"},{"location":"rust/crates/failure/#backtrace","text":"To also support backtrace on errors using the environment setting RUST_BACKTRACE=1 : if let Some(bt) = e.as_fail().backtrace() { eprintln!(\"{}\", bt) } This will only generate and output the backtrace on demand. {!docs/abbreviations.txt!}","title":"Backtrace"},{"location":"rust/solutions/","text":"Solutions Here are some language specific solutions for the Rust language. Some language independent concepts or ready to use modules and programs are also described under general solutions . {!docs/abbreviations.txt!}","title":"Overview"},{"location":"rust/solutions/#solutions","text":"Here are some language specific solutions for the Rust language. Some language independent concepts or ready to use modules and programs are also described under general solutions . {!docs/abbreviations.txt!}","title":"Solutions"},{"location":"rust/solutions/cache/","text":"Caching using Struct You create a struct that will hold the closure and the resulting value of calling the closure. The struct will execute the closure only if we need the resulting value, and it will cache the resulting value so the rest of our code doesn\u2019t have to be responsible for saving and reusing the result. Simple implementation struct Cacher<T> where T: Fn(u32) -> u32 { calculation: T, last: Option<u32>, value: Option<u32>, } Now you add methods to initialize this cache and to retrieve the value: impl<T> Cacher<T> where T: Fn(u32) -> u32 { fn new(calculation: T) -> Cacher<T> { Cacher { calculation, last: 0, // value at start didn't matter value: None, } } fn value(&mut self, arg: u32) -> u32 { // check if argument changed and use only store if last call was the same if self.last != Some(v) { self.last = Some(v); self.value = None; } match self.value { Some(v) && => v, None => { let v = (self.calculation)(arg); self.value = Some(v); v }, } } } And it should be called like: let incrementor = Cacher::new(|a| a + 1); let x = incrementor.value(5); // x will be 6 {!docs/abbreviations.txt!}","title":"Caching using Struct"},{"location":"rust/solutions/cache/#caching-using-struct","text":"You create a struct that will hold the closure and the resulting value of calling the closure. The struct will execute the closure only if we need the resulting value, and it will cache the resulting value so the rest of our code doesn\u2019t have to be responsible for saving and reusing the result.","title":"Caching using Struct"},{"location":"rust/solutions/cache/#simple-implementation","text":"struct Cacher<T> where T: Fn(u32) -> u32 { calculation: T, last: Option<u32>, value: Option<u32>, } Now you add methods to initialize this cache and to retrieve the value: impl<T> Cacher<T> where T: Fn(u32) -> u32 { fn new(calculation: T) -> Cacher<T> { Cacher { calculation, last: 0, // value at start didn't matter value: None, } } fn value(&mut self, arg: u32) -> u32 { // check if argument changed and use only store if last call was the same if self.last != Some(v) { self.last = Some(v); self.value = None; } match self.value { Some(v) && => v, None => { let v = (self.calculation)(arg); self.value = Some(v); v }, } } } And it should be called like: let incrementor = Cacher::new(|a| a + 1); let x = incrementor.value(5); // x will be 6 {!docs/abbreviations.txt!}","title":"Simple implementation"},{"location":"solutions/","text":"Solutions What is presented here as solutions are concepts, ideas, modules and also complete working products. See it as the good things you can get out of this whole book. Most of the book was written while I worked on this solutions so they are the successor of this book. This site contains description about basic concepts but will only contain short overviews about each product or module with how it works and what's the purpose of it. But to really decide if it can help you solve your problems and needs you have to follow the links to further documentation to check if you may use it. It's not much at the moment. More solutions will be presented here as far as they are open source and at least partly working. Specific tools sometimes mentioned in my blog which are special to a client can not be integrated here. Also some of my older codes are not included here because they don't match my newer quality standards and may be integrated later, if they've been rewritten. {: .center} Only the major third party modules are shown here. DevOps : This will contain solutions from the operations area which can be used to make the tasks there more automatic. Applications : Real applications, which can be installed, configured and used by anybody. Modules : This contains base modules which help you to start developing your project. Have a look at them and check what you need and what not. Include them as needed. Playground : My example applications are working but not fully functional. They are used as a base to develop them further for your needs. You can fork them and continue the development with adding your own modules... Quality Standards : These are seen as goals, more like a vision, meaning not every goal may be reached or completely reached. But you should try to fulfill these as much as possible. Smaller language specific concepts can also be found under: Rust . {!docs/abbreviations.txt!}","title":"Overview"},{"location":"solutions/#solutions","text":"What is presented here as solutions are concepts, ideas, modules and also complete working products. See it as the good things you can get out of this whole book. Most of the book was written while I worked on this solutions so they are the successor of this book. This site contains description about basic concepts but will only contain short overviews about each product or module with how it works and what's the purpose of it. But to really decide if it can help you solve your problems and needs you have to follow the links to further documentation to check if you may use it. It's not much at the moment. More solutions will be presented here as far as they are open source and at least partly working. Specific tools sometimes mentioned in my blog which are special to a client can not be integrated here. Also some of my older codes are not included here because they don't match my newer quality standards and may be integrated later, if they've been rewritten. {: .center} Only the major third party modules are shown here. DevOps : This will contain solutions from the operations area which can be used to make the tasks there more automatic. Applications : Real applications, which can be installed, configured and used by anybody. Modules : This contains base modules which help you to start developing your project. Have a look at them and check what you need and what not. Include them as needed. Playground : My example applications are working but not fully functional. They are used as a base to develop them further for your needs. You can fork them and continue the development with adding your own modules... Quality Standards : These are seen as goals, more like a vision, meaning not every goal may be reached or completely reached. But you should try to fulfill these as much as possible. Smaller language specific concepts can also be found under: Rust . {!docs/abbreviations.txt!}","title":"Solutions"},{"location":"solutions/applications/","text":"Applications This page will collect applications which are ready to use. So install them, configure them and use them right away. {: .right .icon} Web and REST Server A pre-configured and opinionated but ready to use Web and REST Server which can easily be used as backend to other services or any frontend type. It's the successor of the Portal Server (see below). Source Documentation {: .right .icon} GUI Client for Server The Alinex GUI Client is a modern web application which can be also be used as application on nearly any platform. It is also written using TypeScript and uses a modular concept and a modern client. You may use it as SPA, PWA, SSR, Android or IOS App, Windows, Mac or Linux Application all from the same code base. GUI Client Documentation {: .right .icon} Small Tools based on NodeJS (older) dbreport tool to query database and send results as email attachments mailman management console based on email worktime command line application for logging work times {!docs/abbreviations.txt!}","title":"Applications"},{"location":"solutions/applications/#applications","text":"This page will collect applications which are ready to use. So install them, configure them and use them right away. {: .right .icon}","title":"Applications"},{"location":"solutions/applications/#web-and-rest-server","text":"A pre-configured and opinionated but ready to use Web and REST Server which can easily be used as backend to other services or any frontend type. It's the successor of the Portal Server (see below). Source Documentation {: .right .icon}","title":"Web and REST Server"},{"location":"solutions/applications/#gui-client-for-server","text":"The Alinex GUI Client is a modern web application which can be also be used as application on nearly any platform. It is also written using TypeScript and uses a modular concept and a modern client. You may use it as SPA, PWA, SSR, Android or IOS App, Windows, Mac or Linux Application all from the same code base. GUI Client Documentation {: .right .icon}","title":"GUI Client for Server"},{"location":"solutions/applications/#small-tools-based-on-nodejs-older","text":"dbreport tool to query database and send results as email attachments mailman management console based on email worktime command line application for logging work times {!docs/abbreviations.txt!}","title":"Small Tools based on NodeJS (older)"},{"location":"solutions/devops/","text":"DevOps Tools DevOps is a term for a group of concepts that, while not all new, have catalyzed into a movement and are rapidly spreading throughout the technical community. It is also characterized by operations staff making use many of the same techniques as developers for their systems work. The tools listed here are mostly for developers, which want to help automate operation tasks. That means you can't directly use them, you have to use them as a base and need to setup and specialize them for your individual needs. But they help you to do this. {: .right .icon} Bash Library This library can be used to easily build powerful bash scripts. It's use is very simple and can be integrated fast. I for myself use it within the ssh-control an application to manage hundreds of virtual machines in a standardized way and my admin-utils collection which controls server processes, backups and also for data conversion. This two tools are closed source because they are specific to my work environment. But you can make such things using the bash-lib easily yourself. Source Documentation {: .right .icon} DataStore The data store is a in memory store for small to medium sized data structures. It allows you to read, work and write in different formats and therefore also to transform between them. It can be used to display and extract specific data from various formats or transform between them. It supports protocols like file, sftp, http, https... with formats like JSON, BSON, XML, INI, properties and more also in compressed version. Source Documentation {: .right .icon} Older NodeJS Helpers monitor server application for monitoring IT systems scripter environment to make powerful scripts {!docs/abbreviations.txt!}","title":"DevOps"},{"location":"solutions/devops/#devops-tools","text":"DevOps is a term for a group of concepts that, while not all new, have catalyzed into a movement and are rapidly spreading throughout the technical community. It is also characterized by operations staff making use many of the same techniques as developers for their systems work. The tools listed here are mostly for developers, which want to help automate operation tasks. That means you can't directly use them, you have to use them as a base and need to setup and specialize them for your individual needs. But they help you to do this. {: .right .icon}","title":"DevOps Tools"},{"location":"solutions/devops/#bash-library","text":"This library can be used to easily build powerful bash scripts. It's use is very simple and can be integrated fast. I for myself use it within the ssh-control an application to manage hundreds of virtual machines in a standardized way and my admin-utils collection which controls server processes, backups and also for data conversion. This two tools are closed source because they are specific to my work environment. But you can make such things using the bash-lib easily yourself. Source Documentation {: .right .icon}","title":"Bash Library"},{"location":"solutions/devops/#datastore","text":"The data store is a in memory store for small to medium sized data structures. It allows you to read, work and write in different formats and therefore also to transform between them. It can be used to display and extract specific data from various formats or transform between them. It supports protocols like file, sftp, http, https... with formats like JSON, BSON, XML, INI, properties and more also in compressed version. Source Documentation {: .right .icon}","title":"DataStore"},{"location":"solutions/devops/#older-nodejs-helpers","text":"monitor server application for monitoring IT systems scripter environment to make powerful scripts {!docs/abbreviations.txt!}","title":"Older NodeJS Helpers"},{"location":"solutions/modules/","text":"Modules The modules will help you to start developing your projects. They can sometimes be also used as CLI tools in bash script. I try to make them as general as possible, so they met not only my first use case but also a lot others, too. Have a look at them and find out what you need and what not. Include them as needed. They are all written with TypeScript definitions, so if you can benefit of the TypeChecking and tooling help. {: .right .icon} Validator Like the alinex-datastore it allows to load and read data structures from different locations and formats. But the validator adds checking and optimizing the data structure. This is done using predefined schema definitions which are powerful and rule based. Using this module you may read configuration, describe structures, call REST services and be prevented against injection. Source Documentation {: .right .icon} DataStore The data store is a in memory store for small to medium sized data structures. It allows you to read, work and write in different formats and therefore also to transform between them. It can be used to display, access and modify specific data from various formats or transform between them. It supports protocols like file, sftp, http, https... with formats like JSON, BSON, XML, INI, properties and more also in compressed version. Source Documentation {: .right .icon} Data This is a collection of methods to work on JavaScript data structures which are a combination of objects, array and other data types. These methods are: clone to make a exact copy of the complete data structure, filter to access and transform specific elements and merge to bring two or more structures together using different merge methods. Source Documentation {: .right .icon} Async Multiple methods to make working with promises easier: delay , retry , each , map , filter and parallel Source Documentation {: .right .icon} Core The core module is a base for all of my alinex modules. It has some common methods which are necessary in nearly any of my packages. It includes error and exit handler and also displays the logo, mainly used for CLI. Source Documentation {: .right .icon} Older NodeJs Modules exec async execution control for other processes database wrapper module supporting mysql and more fs enhancement of the node filesystem library mail easy mail sending media media file analyzation and manipulation server basic webserver based on express handlebars helper collection for handlebars templates report easy report generation using markdown ssh ssh port tunneling made easy table working with table like data codedoc a tool to extract documentation out of code Outdated Validator replaced by new Validator Core replaced by new Core util major parts replaced by Data builder no longer necessary format replaced by DataStore {!docs/abbreviations.txt!}","title":"Modules"},{"location":"solutions/modules/#modules","text":"The modules will help you to start developing your projects. They can sometimes be also used as CLI tools in bash script. I try to make them as general as possible, so they met not only my first use case but also a lot others, too. Have a look at them and find out what you need and what not. Include them as needed. They are all written with TypeScript definitions, so if you can benefit of the TypeChecking and tooling help. {: .right .icon}","title":"Modules"},{"location":"solutions/modules/#validator","text":"Like the alinex-datastore it allows to load and read data structures from different locations and formats. But the validator adds checking and optimizing the data structure. This is done using predefined schema definitions which are powerful and rule based. Using this module you may read configuration, describe structures, call REST services and be prevented against injection. Source Documentation {: .right .icon}","title":"Validator"},{"location":"solutions/modules/#datastore","text":"The data store is a in memory store for small to medium sized data structures. It allows you to read, work and write in different formats and therefore also to transform between them. It can be used to display, access and modify specific data from various formats or transform between them. It supports protocols like file, sftp, http, https... with formats like JSON, BSON, XML, INI, properties and more also in compressed version. Source Documentation {: .right .icon}","title":"DataStore"},{"location":"solutions/modules/#data","text":"This is a collection of methods to work on JavaScript data structures which are a combination of objects, array and other data types. These methods are: clone to make a exact copy of the complete data structure, filter to access and transform specific elements and merge to bring two or more structures together using different merge methods. Source Documentation {: .right .icon}","title":"Data"},{"location":"solutions/modules/#async","text":"Multiple methods to make working with promises easier: delay , retry , each , map , filter and parallel Source Documentation {: .right .icon}","title":"Async"},{"location":"solutions/modules/#core","text":"The core module is a base for all of my alinex modules. It has some common methods which are necessary in nearly any of my packages. It includes error and exit handler and also displays the logo, mainly used for CLI. Source Documentation {: .right .icon}","title":"Core"},{"location":"solutions/modules/#older-nodejs-modules","text":"exec async execution control for other processes database wrapper module supporting mysql and more fs enhancement of the node filesystem library mail easy mail sending media media file analyzation and manipulation server basic webserver based on express handlebars helper collection for handlebars templates report easy report generation using markdown ssh ssh port tunneling made easy table working with table like data codedoc a tool to extract documentation out of code","title":"Older NodeJs Modules"},{"location":"solutions/modules/#outdated","text":"Validator replaced by new Validator Core replaced by new Core util major parts replaced by Data builder no longer necessary format replaced by DataStore {!docs/abbreviations.txt!}","title":"Outdated"},{"location":"solutions/playground/","text":"Playground !!! warning These are working but not fully functional. They are used as a base to develop them further for your needs. You can fork them and continue the development with adding your own modules... {: .right .icon} Application Server This is a start for a more robust and performing server part which may be used instead of the Portal Server described above. It should develop into a full fledged and production ready server which can be used together with a client application like the Portal Client. Source Documentation {!docs/abbreviations.txt!}","title":"Playground"},{"location":"solutions/playground/#playground","text":"!!! warning These are working but not fully functional. They are used as a base to develop them further for your needs. You can fork them and continue the development with adding your own modules... {: .right .icon}","title":"Playground"},{"location":"solutions/playground/#application-server","text":"This is a start for a more robust and performing server part which may be used instead of the Portal Server described above. It should develop into a full fledged and production ready server which can be used together with a client application like the Portal Client. Source Documentation {!docs/abbreviations.txt!}","title":"Application Server"},{"location":"solutions/quality/","text":"Quality Standards These are seen as goals, more like a vision, meaning not every goal may be reached or completely reached. But you should try to fulfill these as much as possible. Functionality !!! check \"Goal\" Everything which is possible in the web should be allowed. The range should be kept wide but implemented in a short range first. Each function have to fulfill the following points: all specific requirements have to be reached security issues have to be checked automatic tests should be integrated Universal usable !!! check \"Goal\" The system should run on most systems without problems. Special modules are always optional. No commercial code is used or integrated. General interfaces are used everywhere: easy to install works beside other software For the user it should also be accessible with any client on any system from everywhere with ease: easy to understand easy to learn use as known from other systems optimized for office (shortcuts...) Stability !!! check \"Goal\" Clear control structures and optimal error management helps to work on problems. Unit Tests give the ability to find those early in the development process. As bugs are always there this is reached specifically by: complete checking of all external data one problem should not bring down the whole system work with problems and broken connections and retry automatically Performance !!! check \"Goal\" The system's performance is optimized and it should be as fast as possible also under high frequency. But this is not the highest goal because it contradicts the fast prototyping. low resource usage support for different caching systems clustering with load balancing monitoring to detect problems before they occur Extensibility !!! check \"Goal\" Modularization is used to make extending the system an easy task. Loose coupling and dependency injection are the key concepts to allow easy extension. The concrete way differs based on the language possibilities. Security !!! check \"Goal\" The user's data is the valueablest thing, so always protect it. Nowadays security is one of the biggest point everywhere. So you should: connections and data transfer should always be secure passwords and sensitive data is only stored secured user identification are done through anonymous tokens statistics should never contain user specific data user data can also be deleted from the system on demand {!docs/abbreviations.txt!}","title":"Quality Standards"},{"location":"solutions/quality/#quality-standards","text":"These are seen as goals, more like a vision, meaning not every goal may be reached or completely reached. But you should try to fulfill these as much as possible.","title":"Quality Standards"},{"location":"solutions/quality/#functionality","text":"!!! check \"Goal\" Everything which is possible in the web should be allowed. The range should be kept wide but implemented in a short range first. Each function have to fulfill the following points: all specific requirements have to be reached security issues have to be checked automatic tests should be integrated","title":"Functionality"},{"location":"solutions/quality/#universal-usable","text":"!!! check \"Goal\" The system should run on most systems without problems. Special modules are always optional. No commercial code is used or integrated. General interfaces are used everywhere: easy to install works beside other software For the user it should also be accessible with any client on any system from everywhere with ease: easy to understand easy to learn use as known from other systems optimized for office (shortcuts...)","title":"Universal usable"},{"location":"solutions/quality/#stability","text":"!!! check \"Goal\" Clear control structures and optimal error management helps to work on problems. Unit Tests give the ability to find those early in the development process. As bugs are always there this is reached specifically by: complete checking of all external data one problem should not bring down the whole system work with problems and broken connections and retry automatically","title":"Stability"},{"location":"solutions/quality/#performance","text":"!!! check \"Goal\" The system's performance is optimized and it should be as fast as possible also under high frequency. But this is not the highest goal because it contradicts the fast prototyping. low resource usage support for different caching systems clustering with load balancing monitoring to detect problems before they occur","title":"Performance"},{"location":"solutions/quality/#extensibility","text":"!!! check \"Goal\" Modularization is used to make extending the system an easy task. Loose coupling and dependency injection are the key concepts to allow easy extension. The concrete way differs based on the language possibilities.","title":"Extensibility"},{"location":"solutions/quality/#security","text":"!!! check \"Goal\" The user's data is the valueablest thing, so always protect it. Nowadays security is one of the biggest point everywhere. So you should: connections and data transfer should always be secure passwords and sensitive data is only stored secured user identification are done through anonymous tokens statistics should never contain user specific data user data can also be deleted from the system on demand {!docs/abbreviations.txt!}","title":"Security"},{"location":"systems/","text":"Systems This chapter collects description of systems which are used like databases special storages or more. You will find short overviews and a quick help for them. {!docs/abbreviations.txt!}","title":"Overview"},{"location":"systems/#systems","text":"This chapter collects description of systems which are used like databases special storages or more. You will find short overviews and a quick help for them. {!docs/abbreviations.txt!}","title":"Systems"},{"location":"systems/apache/","text":"Apache Web Server I use apache for a long term as webserver, load balancer and more... This page should be a maintenance page for me with some helpful configuration snippets, which I often use. Virtual Host <VirtualHost *:80> ServerName website.de ServerAdmin webmaster@website.de .... DocumentRoot /var/www </VirtualHost> Logging LogLevel warn CustomLog /var/log/apache2/my_service_access.log combined ErrorLog /var/log/apache2/my_service_error.log Reverse Proxy The apache module mod_proxy_http has to be installed and enabled or some other mod_proxy_* modules if other protocols should be used. ProxyRequests Off # only needed for apache 2.2 (default since apache 2.4) ProxyPreserveHost On ProxyPass \"/\" \"http://internal-service:8080/\" ProxyReversePass \"/\" \"http://internal-service:8080/\" SSL SSLEngine on SSLCertificateFile /etc/apache2/ssl/website.de.crt SSLCertificateKeyFile /etc/apache2/ssl/website.de.key SSLCACertificateFile /etc/apache2/ssl/website.de.ca-bundle Forwarding HTTP The apache module mod_rewrite has to be installed and enabled. RewriteEngine On RewriteRule (.*) https://%{HTTP_HOST}%{REQUEST_URI} {!docs/abbreviations.txt!}","title":"Apache Web Server"},{"location":"systems/apache/#apache-web-server","text":"I use apache for a long term as webserver, load balancer and more... This page should be a maintenance page for me with some helpful configuration snippets, which I often use.","title":"Apache Web Server"},{"location":"systems/apache/#virtual-host","text":"<VirtualHost *:80> ServerName website.de ServerAdmin webmaster@website.de .... DocumentRoot /var/www </VirtualHost>","title":"Virtual Host"},{"location":"systems/apache/#logging","text":"LogLevel warn CustomLog /var/log/apache2/my_service_access.log combined ErrorLog /var/log/apache2/my_service_error.log","title":"Logging"},{"location":"systems/apache/#reverse-proxy","text":"The apache module mod_proxy_http has to be installed and enabled or some other mod_proxy_* modules if other protocols should be used. ProxyRequests Off # only needed for apache 2.2 (default since apache 2.4) ProxyPreserveHost On ProxyPass \"/\" \"http://internal-service:8080/\" ProxyReversePass \"/\" \"http://internal-service:8080/\"","title":"Reverse Proxy"},{"location":"systems/apache/#ssl","text":"SSLEngine on SSLCertificateFile /etc/apache2/ssl/website.de.crt SSLCertificateKeyFile /etc/apache2/ssl/website.de.key SSLCACertificateFile /etc/apache2/ssl/website.de.ca-bundle","title":"SSL"},{"location":"systems/apache/#forwarding-http","text":"The apache module mod_rewrite has to be installed and enabled. RewriteEngine On RewriteRule (.*) https://%{HTTP_HOST}%{REQUEST_URI} {!docs/abbreviations.txt!}","title":"Forwarding HTTP"},{"location":"systems/linux/","text":"Linux This is a small and incomplete collection of some Problems I had to fix, to keep them if I may need them again. Touch click events not working after suspend Create a script which is called on resume event under /etc/pm/sleep.d/0000trackpad : #!/bin/sh case \"$1\" in resume) DISPLAY=:0.0 su <USER> -c '/usr/bin/synclient TouchpadOff=0' ;; esac Tunnel Git through socks proxy # open tunnel ssh -D 1337 -q -C -N alinex@peacock.uberspace.de # Ctrl-C to stop # use tunnel for git git config --global http.proxy 'socks5://127.0.0.1:1337' # remove tunnel from git git config --global --unset http.proxy {!docs/abbreviations.txt!}","title":"Linux"},{"location":"systems/linux/#linux","text":"This is a small and incomplete collection of some Problems I had to fix, to keep them if I may need them again.","title":"Linux"},{"location":"systems/linux/#touch-click-events-not-working-after-suspend","text":"Create a script which is called on resume event under /etc/pm/sleep.d/0000trackpad : #!/bin/sh case \"$1\" in resume) DISPLAY=:0.0 su <USER> -c '/usr/bin/synclient TouchpadOff=0' ;; esac","title":"Touch click events not working after suspend"},{"location":"systems/linux/#tunnel-git-through-socks-proxy","text":"# open tunnel ssh -D 1337 -q -C -N alinex@peacock.uberspace.de # Ctrl-C to stop # use tunnel for git git config --global http.proxy 'socks5://127.0.0.1:1337' # remove tunnel from git git config --global --unset http.proxy {!docs/abbreviations.txt!}","title":"Tunnel Git through socks proxy"},{"location":"systems/mongo/","text":"MongoDB MongoDB is a NoSQL document storage as such it is different to Relational Databases (RDBs) like MySQL or postgreSQL. Installation Under debian it is in the default package repository: apt-get install -y mongodb This should also install the mongodb-server and mongodb-clients on your system. Documents The data within mongo is stored as documents in JSON format. They are organized in collections, which is the same as tables in a relational database. Shell To manage the documents you may use the contained shell: mongo [<dbname>] This will give you an interactive shell to administrate. It's a JavaScript based shell. Commands In the following table you find some common command examples. Command Usage db show current db name db.<collection>.find().pretty() list all documents and pretty print db.<collection>.insert(<json>) insert document Management Database To create a new database you only have to access it: > use mydb switched to db mydb And to get the name of the current database use the db command. Users Using db.createUser() new users can be added: > db.createUser({ ... user: \"alex\", ... pwd: \"test123\", ... roles: [\"readWrite\", \"dbAdmin\"] ... }); Successfully added user: { \"user\" : \"alex\", \"roles\" : [ \"readWrite\", \"dbAdmin\" ] } Collections To create a collection, use the db.createCollection() method. It takes one parameter: the name of the collection. > db.createCollection(\"persons\"); { \"ok\" : 1 } The success message will be ok with the count of affected items (or created collections, in this case). {!docs/abbreviations.txt!}","title":"MongoDB"},{"location":"systems/mongo/#mongodb","text":"MongoDB is a NoSQL document storage as such it is different to Relational Databases (RDBs) like MySQL or postgreSQL.","title":"MongoDB"},{"location":"systems/mongo/#installation","text":"Under debian it is in the default package repository: apt-get install -y mongodb This should also install the mongodb-server and mongodb-clients on your system.","title":"Installation"},{"location":"systems/mongo/#documents","text":"The data within mongo is stored as documents in JSON format. They are organized in collections, which is the same as tables in a relational database.","title":"Documents"},{"location":"systems/mongo/#shell","text":"To manage the documents you may use the contained shell: mongo [<dbname>] This will give you an interactive shell to administrate. It's a JavaScript based shell.","title":"Shell"},{"location":"systems/mongo/#commands","text":"In the following table you find some common command examples. Command Usage db show current db name db.<collection>.find().pretty() list all documents and pretty print db.<collection>.insert(<json>) insert document","title":"Commands"},{"location":"systems/mongo/#management","text":"","title":"Management"},{"location":"systems/mongo/#database","text":"To create a new database you only have to access it: > use mydb switched to db mydb And to get the name of the current database use the db command.","title":"Database"},{"location":"systems/mongo/#users","text":"Using db.createUser() new users can be added: > db.createUser({ ... user: \"alex\", ... pwd: \"test123\", ... roles: [\"readWrite\", \"dbAdmin\"] ... }); Successfully added user: { \"user\" : \"alex\", \"roles\" : [ \"readWrite\", \"dbAdmin\" ] }","title":"Users"},{"location":"systems/mongo/#collections","text":"To create a collection, use the db.createCollection() method. It takes one parameter: the name of the collection. > db.createCollection(\"persons\"); { \"ok\" : 1 } The success message will be ok with the count of affected items (or created collections, in this case). {!docs/abbreviations.txt!}","title":"Collections"},{"location":"systems/postgres/","text":"PostgreSQL Database As I use PostgreSQL databases a lot I will collect some operation tasks around them. Root access As defined in the default configuration of PostgreSQL on Linux, a user called postgres is made and only this user may connect without password and has super admin rights to the entire PostgreSQL instance. So to change the configuration or administrate it you have to be postgres user: sudo -u postgres psql For security reasons sudo -u may not generally be allowed but a setting like: # setup once as root echo \"su -s /bin/bash -l postgres\" >> /usr/local/sbin/go-user-postgres chmod 755 /usr/local/sbin/go-user-postgres echo \"<user> ALL=NOPASSWD: /usr/local/sbin/go-user-postgres\" >> /etc/sudoers.d/divibib This allows a specific user to switch to postgres user but not to other users: sudo /usr/local/sbin/go-user-postgres And to make it easier for you to call it, add it in your users path: echo \"export PATH=$PATH:/usr/local/sbin\" >> /home/divibib/.bashrc source /home/divibib/.bashrc # now you only need to call sudo go-user-postgres Configuration To generally calculate how to configure PGTune can be used. But this is only a start point you should always check that the values suggested here are working on the specific scenario. Setup Database Creating user bash sudo -u postgres createuser <username> Creating Database bash sudo -u postgres createdb <dbname> Giving the user a password bash sudo -u postgres psql -c \"alter user <username> with encrypted password '<password>';\" Granting privileges on database bash sudo -u postgres psql -c \"grant all privileges on database <dbname> to <username>;\" And yeah, that should be it! Or do it completely as SQL: CREATE DATABASE <your db name>; CREATE USER <your user> WITH ENCRYPTED PASSWORD '<password>'; GRANT ALL PRIVILEGES ON DATABASE <your db name> TO <your user>; Maintenance A quick overview of what is going on can be seen using pg_top which is like the system top, but for the database. Install it in debian using: apt-get install pgtop . Databases and Schemas Get a list of all databases using \\l in psql or the following SQL: SELECT datname AS db, datconnlimit AS connlimit FROM pg_database WHERE datistemplate = false; The schemas can be retrieved for each database with size (connect to the database first): SELECT schema_name, pg_size_pretty(sum(table_size)::bigint) AS size, round((sum(table_size) / pg_database_size(current_database())) * 100, 2) AS percent FROM ( SELECT pg_catalog.pg_namespace.nspname as schema_name, pg_relation_size(pg_catalog.pg_class.oid) as table_size FROM pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON relnamespace = pg_catalog.pg_namespace.oid ) t GROUP BY schema_name ORDER BY percent DESC; And the extensions installed in each database are shown using \\dx or: SELECT * FROM pg_extension; Connections To get the current connections for each database, use: select datname, numbackends from pg_stat_database; This will show a table like: datname | numbackends ------------+------------- template0 | 0 system1 | 10 template1 | 0 To get some more information over the whole database cluster the connections used on the whole database server use: select max_conn,used,res_for_super,max_conn-used-res_for_super res_for_normal from (select count(*) used from pg_stat_activity) t1, (select setting::int res_for_super from pg_settings where name=$$superuser_reserved_connections$$) t2, (select setting::int max_conn from pg_settings where name=$$max_connections$$) t3; This will give you something like: max_conn | used | res_for_super | res_for_normal ---------+------+---------------+---------------- 100 | 2 | 3 | 95 (1 row) The user limit can be displayed using: SELECT rolname, (SELECT count(*) FROM pg_stat_activity WHERE usename=rolname) AS used, rolconnlimit FROM pg_roles WHERE rolconnlimit <> -1; rolname | used | rolconnlimit --------------+------|-------------- my_user | 12 | 30 (1 row) And it can be changed using ALTER USER my_user CONNECTION LIMIT 50; Processes A short info what is running can be seen in htop as there is a process per connection running which also shows the database and type of SQL. More information what is running can be displayed using: SELECT * FROM pg_stat_activity WHERE state = 'active'; To stop one of this statements you can call: SELECT pg_cancel_backend(<pid of the process>) If the process cannot be killed, try: SELECT pg_terminate_backend(<pid of the process>) Blocking Using the following SQL you will get a concrete List of who is blocking who: -- run using postgres user SELECT blocked_locks.pid AS blocked_pid, blocked_activity.usename AS blocked_user, blocking_locks.pid AS blocking_pid, blocking_activity.usename AS blocking_user, blocked_activity.query AS blocked_statement, blocking_activity.query AS current_statement_in_blocking_process FROM pg_catalog.pg_locks AS blocked_locks JOIN pg_catalog.pg_stat_activity AS blocked_activity ON blocked_activity.pid = blocked_locks.pid JOIN pg_catalog.pg_locks AS blocking_locks ON blocking_locks.locktype = blocked_locks.locktype AND blocking_locks.DATABASE IS NOT DISTINCT FROM blocked_locks.DATABASE AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid AND blocking_locks.pid != blocked_locks.pid JOIN pg_catalog.pg_stat_activity AS blocking_activity ON blocking_activity.pid = blocking_locks.pid WHERE NOT blocked_locks.GRANTED; Deadlock Die folgende Abfrage zeigt Prozesse an, die sich blockieren. SELECT DISTINCT blocked_locks.pid AS blocked_pid, blocked_activity.usename AS blocked_user, blocking_locks.pid AS blocking_pid, blocking_activity.usename AS blocking_user, substring(blocked_activity.query, 0, 40) AS blocked_statement, substring(blocking_activity.query, 0, 40) AS current_statement_in_blocking_process FROM pg_catalog.pg_locks blocked_locks JOIN pg_catalog.pg_stat_activity blocked_activity ON blocked_activity.pid = blocked_locks.pid JOIN pg_catalog.pg_locks blocking_locks ON blocking_locks.locktype = blocked_locks.locktype AND blocking_locks.DATABASE IS NOT DISTINCT FROM blocked_locks.DATABASE AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid AND blocking_locks.pid != blocked_locks.pid JOIN pg_catalog.pg_stat_activity blocking_activity ON blocking_activity.pid = blocking_locks.pid WHERE NOT blocked_locks.GRANTED; Backup/Restore To backup and restore or to move a database to another server (maybe with newer postgres version): sudo -u postgres pg_dump --create mydb > mydb.dump sudo -u postgres pg_dumpall --globals-only > globals.dump And to later restore it check the following: create the tablespace directories if not there add locales to your system if missing ( locale -a , vi /etc/locale.gen , locale-gen ) and restart postgres Then you may import the data: sudo -u postgres psql < globals.dump sudo -u postgres psql < mydb.dump Upgrade Postgres After you have installed multiple versions of postgres on your server you can see them like: $ pg_lsclusters Ver Cluster Port Status Owner Data directory Log file 9.4 main 5432 online postgres /var/lib/postgresql/9.4/main /var/log/postgresql/postgresql-9.4-main.log 9.6 main 5433 online postgres /var/lib/postgresql/9.6/main /var/log/postgresql/postgresql-9.6-main.log To switch from the used 9.4 to the currently unused 9.6 do the following: pg_dropcluster 9.6 main --stop pg_upgradecluster 9.4 main This may take some time in which a new 9.6 database on port 5433 will be created and updated. After done it should look like: $ pg_lsclusters Ver Cluster Port Status Owner Data directory Log file 9.4 main 5433 down postgres /var/lib/postgresql/9.4/main /var/log/postgresql/postgresql-9.4-main.log 9.6 main 5432 online postgres /var/lib/postgresql/9.6/main /var/log/postgresql/postgresql-9.6-main.log Now the old database server can be dropped: pg_dropcluster 9.4 main apt-get --purge remove postgresql-client-9.4 postgresql-9.4 Allow access (pg_hba.conf) To allow somebody to access the server a matching entry in the file /etc/postgresql/11/main/pg_hba.conf is needed: # TYPE DATABASE USER ADDRESS METHOD # \"local\" is for Unix domain socket connections only local all all peer # local connections with credentials host all all 127.0.0.1/32 md5 host all all ::1/128 md5 # Allow replication connections from localhost, by a user with the # replication privilege. local replication all peer host replication all 127.0.0.1/32 md5 host replication all ::1/128 md5 # Users and systems which are allowed to use the database: host all all 192.168.1.0/24 md5 Your file should look something like the above and you can change or add the entries. After something was changed, you have to reload the configuration in the server to take effect: From the command line as postgres user: /usr/bin/pg_ctl reload Or using SQL: SELECT pg_reload_conf(); Rights management To show which rights are set use: SELECT grantee, privilege_type FROM information_schema.role_table_grants WHERE table_name='table' Here are some common statement to grant access to a PostgreSQL user or group: Grant CONNECT to the database: sql GRANT CONNECT ON DATABASE database_name TO username; Grant USAGE on schema: sql GRANT USAGE ON SCHEMA schema_name TO username; Grant on all tables for DML statements: SELECT, INSERT, UPDATE, DELETE: sql GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA schema_name TO username; Grant all privileges on all tables in the schema: sql GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA schema_name TO username; Grant all privileges on all sequences in the schema: sql GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA schema_name TO username; Grant all privileges on the database: sql GRANT ALL PRIVILEGES ON DATABASE database_name TO username; Grant permission to create database: sql ALTER USER username CREATEDB; Make a user superuser: sql ALTER USER myuser WITH SUPERUSER; Remove superuser status: sql ALTER USER username WITH NOSUPERUSER; Those statements above only affect the current existing tables. To apply to newly created tables, you need to use alter default. For example: ALTER DEFAULT PRIVILEGES FOR USER username IN SCHEMA schema_name GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO username; {!docs/abbreviations.txt!}","title":"PostgreSQL Database"},{"location":"systems/postgres/#postgresql-database","text":"As I use PostgreSQL databases a lot I will collect some operation tasks around them.","title":"PostgreSQL Database"},{"location":"systems/postgres/#root-access","text":"As defined in the default configuration of PostgreSQL on Linux, a user called postgres is made and only this user may connect without password and has super admin rights to the entire PostgreSQL instance. So to change the configuration or administrate it you have to be postgres user: sudo -u postgres psql For security reasons sudo -u may not generally be allowed but a setting like: # setup once as root echo \"su -s /bin/bash -l postgres\" >> /usr/local/sbin/go-user-postgres chmod 755 /usr/local/sbin/go-user-postgres echo \"<user> ALL=NOPASSWD: /usr/local/sbin/go-user-postgres\" >> /etc/sudoers.d/divibib This allows a specific user to switch to postgres user but not to other users: sudo /usr/local/sbin/go-user-postgres And to make it easier for you to call it, add it in your users path: echo \"export PATH=$PATH:/usr/local/sbin\" >> /home/divibib/.bashrc source /home/divibib/.bashrc # now you only need to call sudo go-user-postgres","title":"Root access"},{"location":"systems/postgres/#configuration","text":"To generally calculate how to configure PGTune can be used. But this is only a start point you should always check that the values suggested here are working on the specific scenario.","title":"Configuration"},{"location":"systems/postgres/#setup-database","text":"Creating user bash sudo -u postgres createuser <username> Creating Database bash sudo -u postgres createdb <dbname> Giving the user a password bash sudo -u postgres psql -c \"alter user <username> with encrypted password '<password>';\" Granting privileges on database bash sudo -u postgres psql -c \"grant all privileges on database <dbname> to <username>;\" And yeah, that should be it! Or do it completely as SQL: CREATE DATABASE <your db name>; CREATE USER <your user> WITH ENCRYPTED PASSWORD '<password>'; GRANT ALL PRIVILEGES ON DATABASE <your db name> TO <your user>;","title":"Setup Database"},{"location":"systems/postgres/#maintenance","text":"A quick overview of what is going on can be seen using pg_top which is like the system top, but for the database. Install it in debian using: apt-get install pgtop .","title":"Maintenance"},{"location":"systems/postgres/#databases-and-schemas","text":"Get a list of all databases using \\l in psql or the following SQL: SELECT datname AS db, datconnlimit AS connlimit FROM pg_database WHERE datistemplate = false; The schemas can be retrieved for each database with size (connect to the database first): SELECT schema_name, pg_size_pretty(sum(table_size)::bigint) AS size, round((sum(table_size) / pg_database_size(current_database())) * 100, 2) AS percent FROM ( SELECT pg_catalog.pg_namespace.nspname as schema_name, pg_relation_size(pg_catalog.pg_class.oid) as table_size FROM pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON relnamespace = pg_catalog.pg_namespace.oid ) t GROUP BY schema_name ORDER BY percent DESC; And the extensions installed in each database are shown using \\dx or: SELECT * FROM pg_extension;","title":"Databases and Schemas"},{"location":"systems/postgres/#connections","text":"To get the current connections for each database, use: select datname, numbackends from pg_stat_database; This will show a table like: datname | numbackends ------------+------------- template0 | 0 system1 | 10 template1 | 0 To get some more information over the whole database cluster the connections used on the whole database server use: select max_conn,used,res_for_super,max_conn-used-res_for_super res_for_normal from (select count(*) used from pg_stat_activity) t1, (select setting::int res_for_super from pg_settings where name=$$superuser_reserved_connections$$) t2, (select setting::int max_conn from pg_settings where name=$$max_connections$$) t3; This will give you something like: max_conn | used | res_for_super | res_for_normal ---------+------+---------------+---------------- 100 | 2 | 3 | 95 (1 row) The user limit can be displayed using: SELECT rolname, (SELECT count(*) FROM pg_stat_activity WHERE usename=rolname) AS used, rolconnlimit FROM pg_roles WHERE rolconnlimit <> -1; rolname | used | rolconnlimit --------------+------|-------------- my_user | 12 | 30 (1 row) And it can be changed using ALTER USER my_user CONNECTION LIMIT 50;","title":"Connections"},{"location":"systems/postgres/#processes","text":"A short info what is running can be seen in htop as there is a process per connection running which also shows the database and type of SQL. More information what is running can be displayed using: SELECT * FROM pg_stat_activity WHERE state = 'active'; To stop one of this statements you can call: SELECT pg_cancel_backend(<pid of the process>) If the process cannot be killed, try: SELECT pg_terminate_backend(<pid of the process>)","title":"Processes"},{"location":"systems/postgres/#blocking","text":"Using the following SQL you will get a concrete List of who is blocking who: -- run using postgres user SELECT blocked_locks.pid AS blocked_pid, blocked_activity.usename AS blocked_user, blocking_locks.pid AS blocking_pid, blocking_activity.usename AS blocking_user, blocked_activity.query AS blocked_statement, blocking_activity.query AS current_statement_in_blocking_process FROM pg_catalog.pg_locks AS blocked_locks JOIN pg_catalog.pg_stat_activity AS blocked_activity ON blocked_activity.pid = blocked_locks.pid JOIN pg_catalog.pg_locks AS blocking_locks ON blocking_locks.locktype = blocked_locks.locktype AND blocking_locks.DATABASE IS NOT DISTINCT FROM blocked_locks.DATABASE AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid AND blocking_locks.pid != blocked_locks.pid JOIN pg_catalog.pg_stat_activity AS blocking_activity ON blocking_activity.pid = blocking_locks.pid WHERE NOT blocked_locks.GRANTED;","title":"Blocking"},{"location":"systems/postgres/#deadlock","text":"Die folgende Abfrage zeigt Prozesse an, die sich blockieren. SELECT DISTINCT blocked_locks.pid AS blocked_pid, blocked_activity.usename AS blocked_user, blocking_locks.pid AS blocking_pid, blocking_activity.usename AS blocking_user, substring(blocked_activity.query, 0, 40) AS blocked_statement, substring(blocking_activity.query, 0, 40) AS current_statement_in_blocking_process FROM pg_catalog.pg_locks blocked_locks JOIN pg_catalog.pg_stat_activity blocked_activity ON blocked_activity.pid = blocked_locks.pid JOIN pg_catalog.pg_locks blocking_locks ON blocking_locks.locktype = blocked_locks.locktype AND blocking_locks.DATABASE IS NOT DISTINCT FROM blocked_locks.DATABASE AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid AND blocking_locks.pid != blocked_locks.pid JOIN pg_catalog.pg_stat_activity blocking_activity ON blocking_activity.pid = blocking_locks.pid WHERE NOT blocked_locks.GRANTED;","title":"Deadlock"},{"location":"systems/postgres/#backuprestore","text":"To backup and restore or to move a database to another server (maybe with newer postgres version): sudo -u postgres pg_dump --create mydb > mydb.dump sudo -u postgres pg_dumpall --globals-only > globals.dump And to later restore it check the following: create the tablespace directories if not there add locales to your system if missing ( locale -a , vi /etc/locale.gen , locale-gen ) and restart postgres Then you may import the data: sudo -u postgres psql < globals.dump sudo -u postgres psql < mydb.dump","title":"Backup/Restore"},{"location":"systems/postgres/#upgrade-postgres","text":"After you have installed multiple versions of postgres on your server you can see them like: $ pg_lsclusters Ver Cluster Port Status Owner Data directory Log file 9.4 main 5432 online postgres /var/lib/postgresql/9.4/main /var/log/postgresql/postgresql-9.4-main.log 9.6 main 5433 online postgres /var/lib/postgresql/9.6/main /var/log/postgresql/postgresql-9.6-main.log To switch from the used 9.4 to the currently unused 9.6 do the following: pg_dropcluster 9.6 main --stop pg_upgradecluster 9.4 main This may take some time in which a new 9.6 database on port 5433 will be created and updated. After done it should look like: $ pg_lsclusters Ver Cluster Port Status Owner Data directory Log file 9.4 main 5433 down postgres /var/lib/postgresql/9.4/main /var/log/postgresql/postgresql-9.4-main.log 9.6 main 5432 online postgres /var/lib/postgresql/9.6/main /var/log/postgresql/postgresql-9.6-main.log Now the old database server can be dropped: pg_dropcluster 9.4 main apt-get --purge remove postgresql-client-9.4 postgresql-9.4","title":"Upgrade Postgres"},{"location":"systems/postgres/#allow-access-pg_hbaconf","text":"To allow somebody to access the server a matching entry in the file /etc/postgresql/11/main/pg_hba.conf is needed: # TYPE DATABASE USER ADDRESS METHOD # \"local\" is for Unix domain socket connections only local all all peer # local connections with credentials host all all 127.0.0.1/32 md5 host all all ::1/128 md5 # Allow replication connections from localhost, by a user with the # replication privilege. local replication all peer host replication all 127.0.0.1/32 md5 host replication all ::1/128 md5 # Users and systems which are allowed to use the database: host all all 192.168.1.0/24 md5 Your file should look something like the above and you can change or add the entries. After something was changed, you have to reload the configuration in the server to take effect: From the command line as postgres user: /usr/bin/pg_ctl reload Or using SQL: SELECT pg_reload_conf();","title":"Allow access (pg_hba.conf)"},{"location":"systems/postgres/#rights-management","text":"To show which rights are set use: SELECT grantee, privilege_type FROM information_schema.role_table_grants WHERE table_name='table' Here are some common statement to grant access to a PostgreSQL user or group: Grant CONNECT to the database: sql GRANT CONNECT ON DATABASE database_name TO username; Grant USAGE on schema: sql GRANT USAGE ON SCHEMA schema_name TO username; Grant on all tables for DML statements: SELECT, INSERT, UPDATE, DELETE: sql GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA schema_name TO username; Grant all privileges on all tables in the schema: sql GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA schema_name TO username; Grant all privileges on all sequences in the schema: sql GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA schema_name TO username; Grant all privileges on the database: sql GRANT ALL PRIVILEGES ON DATABASE database_name TO username; Grant permission to create database: sql ALTER USER username CREATEDB; Make a user superuser: sql ALTER USER myuser WITH SUPERUSER; Remove superuser status: sql ALTER USER username WITH NOSUPERUSER; Those statements above only affect the current existing tables. To apply to newly created tables, you need to use alter default. For example: ALTER DEFAULT PRIVILEGES FOR USER username IN SCHEMA schema_name GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO username; {!docs/abbreviations.txt!}","title":"Rights management"}]}